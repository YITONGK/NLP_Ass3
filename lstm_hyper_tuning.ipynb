{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaa2f05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/felikskong/anaconda3/envs/nlp/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/felikskong/anaconda3/envs/nlp/lib/python3.11/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <CFED5F8E-EC3F-36FD-AAA3-2C6C7F8D3DD9> /Users/felikskong/anaconda3/envs/nlp/lib/python3.11/site-packages/torchvision/image.so\n",
      "  Expected in:     <1FF6C703-3F50-3698-A578-F618DE160E0B> /Users/felikskong/anaconda3/envs/nlp/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from pathlib import Path\n",
    "import json\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ----------- Static config -----------\n",
    "DATA_DIR       = Path(\"data\")\n",
    "TRAIN_JSON     = DATA_DIR / \"train-claims.json\"\n",
    "DEV_JSON       = DATA_DIR / \"dev-claims.json\"\n",
    "EVID_JSON      = DATA_DIR / \"evidence.json\"\n",
    "BERT_MODEL     = \"bert-base-uncased\"\n",
    "MAX_LEN        = 256\n",
    "NUM_CLASSES    = 4\n",
    "BATCH_SIZE     = 16\n",
    "EPOCHS         = 5\n",
    "LR             = 2e-4\n",
    "DEVICE = \"cpu\"\n",
    "if torch.cuda.is_available(): \n",
    "    DEVICE = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "\n",
    "label2idx = {\n",
    "    \"SUPPORTS\": 0,\n",
    "    \"REFUTES\": 1,\n",
    "    \"NOT_ENOUGH_INFO\": 2,\n",
    "    \"DISPUTED\": 3,\n",
    "}\n",
    "\n",
    "# ----------- Load data -----------\n",
    "with open(TRAIN_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    train_claims = json.load(f)\n",
    "with open(DEV_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    dev_claims = json.load(f)\n",
    "with open(EVID_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    evidence_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ada3a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL)\n",
    "\n",
    "class ClaimEvidenceDataset(Dataset):\n",
    "    def __init__(self, claims, evidences, tokenizer, max_len):\n",
    "        self.items = []\n",
    "        for cid, obj in claims.items():\n",
    "            claim_text = obj[\"claim_text\"]\n",
    "            ev_ids = obj.get(\"evidences\", [])\n",
    "            ev_texts = [evidences[e] for e in ev_ids if e in evidences]\n",
    "            full_input = claim_text + \" [SEP] \" + \" \".join(ev_texts)\n",
    "            label = label2idx[obj[\"claim_label\"]]\n",
    "            self.items.append((full_input, label))\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text, label = self.items[idx]\n",
    "        enc = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return (\n",
    "            enc[\"input_ids\"].squeeze(0),\n",
    "            enc[\"attention_mask\"].squeeze(0),\n",
    "            torch.tensor(label, dtype=torch.long),\n",
    "        )\n",
    "\n",
    "def collate_batch(batch):\n",
    "    ids, masks, labs = zip(*batch)\n",
    "    return torch.stack(ids), torch.stack(masks), torch.stack(labs)\n",
    "\n",
    "train_ds = ClaimEvidenceDataset(train_claims, evidence_dict, tokenizer, MAX_LEN)\n",
    "dev_ds = ClaimEvidenceDataset(dev_claims, evidence_dict, tokenizer, MAX_LEN)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "dev_dl = DataLoader(dev_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "# ----------- Model definition -----------\n",
    "class BiLSTMWithBertEncoder(nn.Module):\n",
    "    def __init__(self, bert_name, lstm_hid, num_classes, dropout_prob, lstm_layers):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_name)\n",
    "        for p in self.bert.parameters():\n",
    "            p.requires_grad = False\n",
    "        bert_dim = self.bert.config.hidden_size\n",
    "        self.dropout_bert = nn.Dropout(dropout_prob)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=bert_dim,\n",
    "            hidden_size=lstm_hid,\n",
    "            num_layers=lstm_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout_prob if lstm_layers > 1 else 0.0\n",
    "        )\n",
    "        self.attn_fc = nn.Linear(2 * lstm_hid, 1)\n",
    "        self.dropout_pool = nn.Dropout(dropout_prob)\n",
    "        self.classifier = nn.Linear(2 * lstm_hid, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        seq_emb = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        seq_emb = self.dropout_bert(seq_emb)\n",
    "        lstm_out, _ = self.lstm(seq_emb)\n",
    "        scores = self.attn_fc(lstm_out).squeeze(-1)\n",
    "        scores = scores.masked_fill(attention_mask == 0, -1e9)\n",
    "        alphas = torch.softmax(scores, dim=1)\n",
    "        pooled = torch.sum(lstm_out * alphas.unsqueeze(-1), dim=1)\n",
    "        pooled = self.dropout_pool(pooled)\n",
    "        return self.classifier(pooled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c358b8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Training with LSTM_HID_DIM=256, DROPOUT_PROB=0.1, NUM_LAYERS=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:29<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1 ‚Üí Val Accuracy: 44.1558%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:28<00:00,  2.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 2 ‚Üí Val Accuracy: 44.1558%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:28<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 3 ‚Üí Val Accuracy: 44.8052%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:28<00:00,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 4 ‚Üí Val Accuracy: 46.1039%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:28<00:00,  2.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 5 ‚Üí Val Accuracy: 47.4026%\n",
      "‚úÖ Best val acc for this config: 47.4026%\n",
      "üî• New global best model saved (acc 47.4026%)\n",
      "\n",
      "üîç Training with LSTM_HID_DIM=256, DROPOUT_PROB=0.1, NUM_LAYERS=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:31<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1 ‚Üí Val Accuracy: 44.8052%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:31<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 2 ‚Üí Val Accuracy: 44.1558%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:31<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 3 ‚Üí Val Accuracy: 46.7532%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:31<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 4 ‚Üí Val Accuracy: 44.8052%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:31<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 5 ‚Üí Val Accuracy: 44.8052%\n",
      "‚úÖ Best val acc for this config: 46.7532%\n",
      "\n",
      "üîç Training with LSTM_HID_DIM=256, DROPOUT_PROB=0.2, NUM_LAYERS=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:28<00:00,  2.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1 ‚Üí Val Accuracy: 44.1558%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:28<00:00,  2.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 2 ‚Üí Val Accuracy: 44.1558%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:28<00:00,  2.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 3 ‚Üí Val Accuracy: 44.1558%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:28<00:00,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 4 ‚Üí Val Accuracy: 44.8052%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:28<00:00,  2.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 5 ‚Üí Val Accuracy: 44.8052%\n",
      "‚úÖ Best val acc for this config: 44.8052%\n",
      "\n",
      "üîç Training with LSTM_HID_DIM=256, DROPOUT_PROB=0.2, NUM_LAYERS=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:31<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1 ‚Üí Val Accuracy: 44.1558%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:31<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 2 ‚Üí Val Accuracy: 44.1558%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:31<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 3 ‚Üí Val Accuracy: 44.1558%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:31<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 4 ‚Üí Val Accuracy: 44.1558%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:31<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 5 ‚Üí Val Accuracy: 43.5065%\n",
      "‚úÖ Best val acc for this config: 44.1558%\n",
      "\n",
      "üîç Training with LSTM_HID_DIM=256, DROPOUT_PROB=0.3, NUM_LAYERS=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:28<00:00,  2.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1 ‚Üí Val Accuracy: 44.1558%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:28<00:00,  2.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 2 ‚Üí Val Accuracy: 44.1558%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:28<00:00,  2.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 3 ‚Üí Val Accuracy: 44.1558%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:28<00:00,  2.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 4 ‚Üí Val Accuracy: 44.1558%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:28<00:00,  2.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 5 ‚Üí Val Accuracy: 44.1558%\n",
      "‚úÖ Best val acc for this config: 44.1558%\n",
      "\n",
      "üîç Training with LSTM_HID_DIM=256, DROPOUT_PROB=0.3, NUM_LAYERS=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:31<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1 ‚Üí Val Accuracy: 44.1558%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:31<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 2 ‚Üí Val Accuracy: 44.1558%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:31<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 3 ‚Üí Val Accuracy: 44.1558%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:31<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 4 ‚Üí Val Accuracy: 44.1558%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:31<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 5 ‚Üí Val Accuracy: 44.1558%\n",
      "‚úÖ Best val acc for this config: 44.1558%\n",
      "\n",
      "üîç Training with LSTM_HID_DIM=512, DROPOUT_PROB=0.1, NUM_LAYERS=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:34<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1 ‚Üí Val Accuracy: 44.8052%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:34<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 2 ‚Üí Val Accuracy: 44.8052%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:34<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 3 ‚Üí Val Accuracy: 47.4026%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:34<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 4 ‚Üí Val Accuracy: 50.6494%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:34<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 5 ‚Üí Val Accuracy: 51.2987%\n",
      "‚úÖ Best val acc for this config: 51.2987%\n",
      "üî• New global best model saved (acc 51.2987%)\n",
      "\n",
      "üîç Training with LSTM_HID_DIM=512, DROPOUT_PROB=0.1, NUM_LAYERS=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:40<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1 ‚Üí Val Accuracy: 46.1039%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:40<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 2 ‚Üí Val Accuracy: 48.0519%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:40<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 3 ‚Üí Val Accuracy: 49.3506%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:40<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 4 ‚Üí Val Accuracy: 51.9481%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:40<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 5 ‚Üí Val Accuracy: 51.2987%\n",
      "‚úÖ Best val acc for this config: 51.9481%\n",
      "üî• New global best model saved (acc 51.9481%)\n",
      "\n",
      "üîç Training with LSTM_HID_DIM=512, DROPOUT_PROB=0.2, NUM_LAYERS=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:34<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1 ‚Üí Val Accuracy: 44.1558%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:34<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 2 ‚Üí Val Accuracy: 44.8052%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:34<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 3 ‚Üí Val Accuracy: 45.4545%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:34<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 4 ‚Üí Val Accuracy: 44.8052%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:34<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 5 ‚Üí Val Accuracy: 46.1039%\n",
      "‚úÖ Best val acc for this config: 46.1039%\n",
      "\n",
      "üîç Training with LSTM_HID_DIM=512, DROPOUT_PROB=0.2, NUM_LAYERS=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:40<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1 ‚Üí Val Accuracy: 44.1558%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:40<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 2 ‚Üí Val Accuracy: 45.4545%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:40<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 3 ‚Üí Val Accuracy: 46.7532%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:40<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 4 ‚Üí Val Accuracy: 46.7532%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:40<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 5 ‚Üí Val Accuracy: 48.0519%\n",
      "‚úÖ Best val acc for this config: 48.0519%\n",
      "\n",
      "üîç Training with LSTM_HID_DIM=512, DROPOUT_PROB=0.3, NUM_LAYERS=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:34<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1 ‚Üí Val Accuracy: 44.1558%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:34<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 2 ‚Üí Val Accuracy: 44.1558%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:34<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 3 ‚Üí Val Accuracy: 44.1558%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:34<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 4 ‚Üí Val Accuracy: 44.1558%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:34<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 5 ‚Üí Val Accuracy: 44.8052%\n",
      "‚úÖ Best val acc for this config: 44.8052%\n",
      "\n",
      "üîç Training with LSTM_HID_DIM=512, DROPOUT_PROB=0.3, NUM_LAYERS=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:40<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1 ‚Üí Val Accuracy: 44.1558%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:41<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 2 ‚Üí Val Accuracy: 44.1558%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:40<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 3 ‚Üí Val Accuracy: 44.1558%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:40<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 4 ‚Üí Val Accuracy: 44.1558%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:41<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 5 ‚Üí Val Accuracy: 44.1558%\n",
      "‚úÖ Best val acc for this config: 44.1558%\n",
      "\n",
      "üèÜ Grid Search Done. Best Config: LSTM_HID_DIM=512, DROPOUT=0.1, LAYERS=3\n",
      "‚úÖ Highest Validation Accuracy: 51.9481%\n"
     ]
    }
   ],
   "source": [
    "# ----------- Grid search -----------\n",
    "param_grid = {\n",
    "    \"LSTM_HID_DIM\": [256, 512],\n",
    "    \"DROPOUT_PROB\": [0.1, 0.2, 0.3],\n",
    "    \"NUM_LAYERS\": [2, 3],\n",
    "}\n",
    "\n",
    "best_global_acc = 0.0\n",
    "best_global_path = \"task2_best_model_grid.pt\"\n",
    "best_config = None\n",
    "\n",
    "for hid_dim, dropout, layers in product(param_grid[\"LSTM_HID_DIM\"], param_grid[\"DROPOUT_PROB\"], param_grid[\"NUM_LAYERS\"]):\n",
    "    print(f\"\\nüîç Training with LSTM_HID_DIM={hid_dim}, DROPOUT_PROB={dropout}, NUM_LAYERS={layers}\")\n",
    "    model = BiLSTMWithBertEncoder(BERT_MODEL, hid_dim, NUM_CLASSES, dropout, layers).to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.classifier.parameters(), lr=LR)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        model.train()\n",
    "        for input_ids, attn_mask, labels in tqdm(train_dl, desc=f\"Train Epoch {epoch}\", dynamic_ncols=True):\n",
    "            input_ids = input_ids.to(DEVICE)\n",
    "            attn_mask = attn_mask.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            logits = model(input_ids, attn_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for input_ids, attn_mask, labels in dev_dl:\n",
    "                input_ids = input_ids.to(DEVICE)\n",
    "                attn_mask = attn_mask.to(DEVICE)\n",
    "                labels = labels.to(DEVICE)\n",
    "                preds = model(input_ids, attn_mask).argmax(dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        acc = correct / total\n",
    "        print(f\"  Epoch {epoch} ‚Üí Val Accuracy: {acc:.4%}\")\n",
    "\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "\n",
    "    print(f\"‚úÖ Best val acc for this config: {best_acc:.4%}\")\n",
    "    if best_acc > best_global_acc:\n",
    "        best_global_acc = best_acc\n",
    "        best_config = (hid_dim, dropout, layers)\n",
    "        torch.save(model.state_dict(), best_global_path)\n",
    "        print(f\"üî• New global best model saved (acc {best_acc:.4%})\")\n",
    "\n",
    "print(f\"\\nüèÜ Grid Search Done. Best Config: LSTM_HID_DIM={best_config[0]}, DROPOUT={best_config[1]}, LAYERS={best_config[2]}\")\n",
    "print(f\"‚úÖ Highest Validation Accuracy: {best_global_acc:.4%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b365af79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def build_vocab(claims, evidences, min_freq=2):\n",
    "    counter = Counter()\n",
    "    for obj in claims.values():\n",
    "        counter.update(word_tokenize(obj[\"claim_text\"].lower()))\n",
    "    for text in evidences.values():\n",
    "        counter.update(word_tokenize(text.lower()))\n",
    "    vocab = {word: i+2 for i, (word, freq) in enumerate(counter.items()) if freq >= min_freq}\n",
    "    vocab[\"<PAD>\"] = 0\n",
    "    vocab[\"<UNK>\"] = 1\n",
    "    return vocab\n",
    "\n",
    "vocab = build_vocab(train_claims, evidence_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1242e0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenDataset(Dataset):\n",
    "    def __init__(self, claims, evidences, vocab, max_len):\n",
    "        self.items = []\n",
    "        for obj in claims.values():\n",
    "            claim_text = obj[\"claim_text\"]\n",
    "            ev_ids = obj.get(\"evidences\", [])\n",
    "            ev_texts = [evidences[e] for e in ev_ids if e in evidences]\n",
    "            text = claim_text + \" \" + \" \".join(ev_texts)\n",
    "            tokens = word_tokenize(text.lower())\n",
    "            token_ids = [vocab.get(w, vocab[\"<UNK>\"]) for w in tokens]\n",
    "            if len(token_ids) > max_len:\n",
    "                token_ids = token_ids[:max_len]\n",
    "            else:\n",
    "                token_ids += [vocab[\"<PAD>\"]] * (max_len - len(token_ids))\n",
    "            label = label2idx[obj[\"claim_label\"]]\n",
    "            self.items.append((torch.tensor(token_ids), torch.tensor(label)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.items[idx]\n",
    "\n",
    "def collate_tokens(batch):\n",
    "    tokens, labels = zip(*batch)\n",
    "    return torch.stack(tokens), torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32dda650",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hid_dim, num_classes, dropout):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hid_dim, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hid_dim, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, _):\n",
    "        x = self.embed(input_ids)\n",
    "        _, (h, _) = self.lstm(x)\n",
    "        out = self.dropout(h[-1])\n",
    "        return self.fc(out)\n",
    "\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hid_dim, num_classes, dropout):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hid_dim, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(2 * hid_dim, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, _):\n",
    "        x = self.embed(input_ids)\n",
    "        _, (h, _) = self.lstm(x)\n",
    "        h = torch.cat((h[-2], h[-1]), dim=1)\n",
    "        out = self.dropout(h)\n",
    "        return self.fc(out)\n",
    "    \n",
    "class BiLSTMWithBert(nn.Module):\n",
    "    def __init__(self, bert_name, lstm_hid, num_classes, dropout_prob):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_name)\n",
    "        for p in self.bert.parameters():\n",
    "            p.requires_grad = False\n",
    "        bert_dim = self.bert.config.hidden_size\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=bert_dim,\n",
    "            hidden_size=lstm_hid,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout_prob\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.classifier = nn.Linear(2 * lstm_hid, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        with torch.no_grad():\n",
    "            bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        lstm_out, _ = self.lstm(bert_output)\n",
    "        pooled = torch.mean(lstm_out, dim=1)\n",
    "        return self.classifier(self.dropout(pooled))\n",
    "    \n",
    "class BiLSTMWithBertEncoder(nn.Module):\n",
    "    def __init__(self, bert_name, lstm_hid, num_classes, dropout_prob, lstm_layers):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_name)\n",
    "        for p in self.bert.parameters():\n",
    "            p.requires_grad = False\n",
    "        bert_dim = self.bert.config.hidden_size\n",
    "\n",
    "        self.dropout_bert = nn.Dropout(dropout_prob)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=bert_dim,\n",
    "            hidden_size=lstm_hid,\n",
    "            num_layers=lstm_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout_prob if lstm_layers > 1 else 0.0\n",
    "        )\n",
    "        self.attn_fc = nn.Linear(2 * lstm_hid, 1)\n",
    "        self.dropout_pool = nn.Dropout(dropout_prob)\n",
    "        self.classifier = nn.Linear(2 * lstm_hid, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        with torch.no_grad():\n",
    "            seq_emb = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        seq_emb = self.dropout_bert(seq_emb)\n",
    "        lstm_out, _ = self.lstm(seq_emb)\n",
    "        scores = self.attn_fc(lstm_out).squeeze(-1)\n",
    "        scores = scores.masked_fill(attention_mask == 0, -1e9)\n",
    "        alphas = torch.softmax(scores, dim=1)\n",
    "        pooled = torch.sum(lstm_out * alphas.unsqueeze(-1), dim=1)\n",
    "        pooled = self.dropout_pool(pooled)\n",
    "        return self.classifier(pooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6221324d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(model, train_dl, dev_dl):\n",
    "    model = model.to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        loop = tqdm(train_dl, desc=f\"Train Epoch {epoch}\", leave=False, dynamic_ncols=True)\n",
    "        for input_ids, *rest in loop:\n",
    "            labels = rest[-1].to(DEVICE)\n",
    "            input_ids = input_ids.to(DEVICE)\n",
    "            attention_mask = rest[0].to(DEVICE) if len(rest) == 2 else None\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        # ----- Evaluation after each epoch -----\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for input_ids, *rest in dev_dl:\n",
    "                labels = rest[-1].to(DEVICE)\n",
    "                input_ids = input_ids.to(DEVICE)\n",
    "                attention_mask = rest[0].to(DEVICE) if len(rest) == 2 else None\n",
    "\n",
    "                preds = model(input_ids, attention_mask).argmax(dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        acc = correct / total\n",
    "        best_acc = max(best_acc, acc)\n",
    "\n",
    "    return best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee0897c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LSTM] Dev Accuracy: 47.4026%\n",
      "[BiLSTM] Dev Accuracy: 48.0519%  \n",
      "[BiLSTM + BERT] Dev Accuracy: 51.6883%\n",
      "[BiLSTM + BERT + Attn] Dev Accuracy: 52.6364% \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare token-level data\n",
    "train_token_ds = TokenDataset(train_claims, evidence_dict, vocab, MAX_LEN)\n",
    "dev_token_ds = TokenDataset(dev_claims, evidence_dict, vocab, MAX_LEN)\n",
    "train_token_dl = DataLoader(train_token_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_tokens)\n",
    "dev_token_dl = DataLoader(dev_token_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_tokens)\n",
    "\n",
    "# 1. LSTM\n",
    "lstm_model = LSTMClassifier(len(vocab), 300, 512, NUM_CLASSES, 0.1)\n",
    "acc_lstm = train_and_eval(lstm_model, train_token_dl, dev_token_dl)\n",
    "print(f\"[LSTM] Dev Accuracy: {acc_lstm:.4%}\")\n",
    "\n",
    "# 2. BiLSTM\n",
    "bilstm_model = BiLSTMClassifier(len(vocab), 300, 512, NUM_CLASSES, 0.1)\n",
    "acc_bilstm = train_and_eval(bilstm_model, train_token_dl, dev_token_dl)\n",
    "print(f\"[BiLSTM] Dev Accuracy: {acc_bilstm:.4%}\")\n",
    "\n",
    "# 3. BiLSTM + BERT\n",
    "bert_model = BiLSTMWithBert(BERT_MODEL, 512, NUM_CLASSES, 0.1)\n",
    "acc_bert = train_and_eval(bert_model, train_dl, dev_dl)\n",
    "print(f\"[BiLSTM + BERT] Dev Accuracy: {acc_bert:.4%}\")\n",
    "\n",
    "# 4. BiLSTM + BERT + Attention\n",
    "bert_attn_model = BiLSTMWithBertEncoder(BERT_MODEL, 512, NUM_CLASSES, 0.1, 3)\n",
    "acc_bert_attn = train_and_eval(bert_attn_model, train_dl, dev_dl)\n",
    "print(f\"[BiLSTM + BERT + Attn] Dev Accuracy: {acc_bert_attn:.4%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
