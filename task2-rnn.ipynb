{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72f72b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/sophia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/sophia/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sophia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 0) Config & Paths\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "TRAIN_JSON = DATA_DIR / \"train-claims.json\"\n",
    "DEV_JSON = DATA_DIR / \"dev-claims.json\"\n",
    "EVID_JSON = DATA_DIR / \"evidence.json\"\n",
    "\n",
    "MAX_LEN = 50\n",
    "EMBED_DIM = 100\n",
    "HIDDEN_DIM = 64\n",
    "NUM_CLASSES = 4\n",
    "DROPOUT_PROB = 0.4\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "LR = 1e-4\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 1) Preprocessing utils\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "stopwords = set(nltk_stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word, 'v')\n",
    "    return lemmatizer.lemmatize(lemma, 'n')\n",
    "\n",
    "def preprocess(text, remove_stopwords=True, lemma=True, stem=False):\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    tokens = [t for t in tokens if re.match('^[a-zA-Z0-9-]+$', t)]\n",
    "    if remove_stopwords:\n",
    "        tokens = [t for t in tokens if t not in stopwords]\n",
    "    if lemma:\n",
    "        tokens = [lemmatize(t) for t in tokens]\n",
    "    if stem:\n",
    "        tokens = [stemmer.stem(t) for t in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 2) Load JSON data → DataFrame\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def load_data(claims_file, evidence_file):\n",
    "    with open(claims_file, 'r', encoding='utf-8') as f:\n",
    "        claims_data = json.load(f)\n",
    "    with open(evidence_file, 'r', encoding='utf-8') as f:\n",
    "        evid_data = json.load(f)\n",
    "    \n",
    "    claim_texts, evid_texts, labels = [], [], []\n",
    "    for cid, cdata in claims_data.items():\n",
    "        claim = preprocess(cdata['claim_text'])\n",
    "        evid_ids = cdata['evidences']\n",
    "        evids = ' '.join([evid_data.get(eid, '') for eid in evid_ids])\n",
    "        evid = preprocess(evids)\n",
    "        claim_texts.append(claim)\n",
    "        evid_texts.append(evid)\n",
    "        labels.append(cdata['claim_label'])\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'claim': claim_texts,\n",
    "        'evidence': evid_texts,\n",
    "        'label': labels\n",
    "    })\n",
    "    return df\n",
    "\n",
    "train_df = load_data(TRAIN_JSON, EVID_JSON)\n",
    "dev_df = load_data(DEV_JSON, EVID_JSON)\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 3) Prepare vocab & sequences\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "all_text = train_df['claim'].tolist() + train_df['evidence'].tolist()\n",
    "token_counts = Counter(w for text in all_text for w in text.split())\n",
    "vocab = {w: idx+1 for idx, (w, _) in enumerate(token_counts.items())}\n",
    "vocab_size = len(vocab) + 1\n",
    "\n",
    "def text_to_seq(text):\n",
    "    seq = [vocab.get(w, 0) for w in text.split()]\n",
    "    return seq + [0]*(MAX_LEN - len(seq)) if len(seq) < MAX_LEN else seq[:MAX_LEN]\n",
    "\n",
    "train_claims = [text_to_seq(t) for t in train_df['claim']]\n",
    "train_evids = [text_to_seq(t) for t in train_df['evidence']]\n",
    "dev_claims = [text_to_seq(t) for t in dev_df['claim']]\n",
    "dev_evids = [text_to_seq(t) for t in dev_df['evidence']]\n",
    "\n",
    "label_enc = LabelEncoder()\n",
    "train_labels = label_enc.fit_transform(train_df['label'])\n",
    "dev_labels = label_enc.transform(dev_df['label'])\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 4) Dataset + DataLoader\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "class ClaimDataset(Dataset):\n",
    "    def __init__(self, claims, evidences, labels):\n",
    "        self.claims = torch.tensor(claims, dtype=torch.long)\n",
    "        self.evidences = torch.tensor(evidences, dtype=torch.long)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.claims[idx], self.evidences[idx], self.labels[idx]\n",
    "\n",
    "train_ds = ClaimDataset(train_claims, train_evids, train_labels)\n",
    "dev_ds = ClaimDataset(dev_claims, dev_evids, dev_labels)\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dev_dl = DataLoader(dev_ds, batch_size=BATCH_SIZE)\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 5) Self-Attention Pooling\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "class SelfAttentionPooling(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq_len, hidden_dim]\n",
    "        weights = torch.softmax(self.attention(x), dim=1)  # [batch, seq_len, 1]\n",
    "        pooled = torch.sum(weights * x, dim=1)  # [batch, hidden_dim]\n",
    "        return pooled\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 6) Model\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.embed_dropout = nn.Dropout(DROPOUT_PROB)\n",
    "        self.rnn_claim = nn.RNN(embed_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "        self.rnn_evid = nn.RNN(embed_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "        self.rnn_dropout = nn.Dropout(DROPOUT_PROB)\n",
    "        self.attention_claim = SelfAttentionPooling(hidden_dim * 2)\n",
    "        self.attention_evid = SelfAttentionPooling(hidden_dim * 2)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 4, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT_PROB),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, claim, evidence):\n",
    "        claim_emb = self.embed_dropout(self.embedding(claim))\n",
    "        evid_emb = self.embed_dropout(self.embedding(evidence))\n",
    "        \n",
    "        claim_out, _ = self.rnn_claim(claim_emb)\n",
    "        evid_out, _ = self.rnn_evid(evid_emb)\n",
    "        \n",
    "        claim_out = self.rnn_dropout(claim_out)\n",
    "        evid_out = self.rnn_dropout(evid_out)\n",
    "        \n",
    "        claim_pool = self.attention_claim(claim_out)\n",
    "        evid_pool = self.attention_evid(evid_out)\n",
    "        \n",
    "        combined = torch.cat([claim_pool, evid_pool], dim=1)\n",
    "        return self.classifier(combined)\n",
    "\n",
    "model = RNNModel(vocab_size, EMBED_DIM, HIDDEN_DIM, NUM_CLASSES).to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "97f17914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b75301c416a42ce8316f98ede9c37ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7460, Train Acc: 0.6270\n",
      "Val Loss: 1.3373, Val Acc: 0.4870\n",
      "✅ New best model saved (epoch 1, acc 48.7013%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f88e2424711348cbaed9b6404fddd0f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7405, Train Acc: 0.6319\n",
      "Val Loss: 1.3324, Val Acc: 0.4935\n",
      "✅ New best model saved (epoch 2, acc 49.3506%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63deee70a9ac414787c4d08960d4691b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7411, Train Acc: 0.6254\n",
      "Val Loss: 1.3429, Val Acc: 0.5000\n",
      "✅ New best model saved (epoch 3, acc 50.0000%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f43e3d69d97a42cc94655cd654b21c34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7289, Train Acc: 0.6401\n",
      "Val Loss: 1.3448, Val Acc: 0.4935\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee3b63789ae8495eb9d3cc5654fef914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7468, Train Acc: 0.6156\n",
      "Val Loss: 1.3502, Val Acc: 0.4870\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e2afc19799c43b098ef8d2fcccfcd50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7425, Train Acc: 0.6303\n",
      "Val Loss: 1.3551, Val Acc: 0.5000\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b849b87c1a54b0e92c557bf0520f4fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7311, Train Acc: 0.6230\n",
      "Val Loss: 1.3534, Val Acc: 0.4870\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aff949bab87c487ca4d30628447ec238",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7323, Train Acc: 0.6295\n",
      "Val Loss: 1.3534, Val Acc: 0.4935\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbf1e925336948b1805e9b0d35f972fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7329, Train Acc: 0.6327\n",
      "Val Loss: 1.3564, Val Acc: 0.5000\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cff95dc84795487bad210e776f89d172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7254, Train Acc: 0.6344\n",
      "Val Loss: 1.3606, Val Acc: 0.5000\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3224140d2b24255a72be6743e25a820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7267, Train Acc: 0.6384\n",
      "Val Loss: 1.3709, Val Acc: 0.5000\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a562ec30ab6e4a679b956578335b9f20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7067, Train Acc: 0.6409\n",
      "Val Loss: 1.3828, Val Acc: 0.4935\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e9b75e0b9684665b97cd19dba126ef6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7311, Train Acc: 0.6295\n",
      "Val Loss: 1.3945, Val Acc: 0.4935\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eb0dde93afe434eace66a552e39391c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7191, Train Acc: 0.6531\n",
      "Val Loss: 1.3835, Val Acc: 0.5000\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52b57494f81941be897b618d1af3753f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7233, Train Acc: 0.6425\n",
      "Val Loss: 1.3992, Val Acc: 0.4870\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c1a95d1907d479796d73729909d48c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7312, Train Acc: 0.6336\n",
      "Val Loss: 1.4030, Val Acc: 0.4935\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc5e053718214675ba9104c227a9c78e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 17:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7174, Train Acc: 0.6490\n",
      "Val Loss: 1.3985, Val Acc: 0.4935\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c41bbeb6b5f43a8850e9671b7c0af97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 18:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7197, Train Acc: 0.6507\n",
      "Val Loss: 1.4037, Val Acc: 0.4935\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b6280fa2c7444fd8761c77d98539da8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 19:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7292, Train Acc: 0.6303\n",
      "Val Loss: 1.3882, Val Acc: 0.4870\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f0b39f3cecb4edf9d7becd84f59a1b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 20:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7486, Train Acc: 0.6401\n",
      "Val Loss: 1.3558, Val Acc: 0.4870\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Weighted Cross Entropy Loss\n",
    "label2idx = {\n",
    "    \"SUPPORTS\":         0,\n",
    "    \"NOT_ENOUGH_INFO\":  1,\n",
    "    \"REFUTES\":          2,\n",
    "    \"DISPUTED\":         3,\n",
    "}\n",
    "\n",
    "with open(TRAIN_JSON, 'r', encoding='utf-8') as f:\n",
    "    train_claim = json.load(f)\n",
    "\n",
    "label_counts = Counter([label2idx[obj[\"claim_label\"]] for obj in train_claim.values()])\n",
    "total = sum(label_counts.values())\n",
    "\n",
    "# 权重越大越重要，可以用 total / class_count 作为反比权重\n",
    "class_weights = [total / label_counts[i] for i in range(len(label2idx))]\n",
    "\n",
    "# 转成 tensor 并放到 DEVICE 上\n",
    "weights = torch.tensor(class_weights, dtype=torch.float).to(DEVICE)\n",
    "\n",
    "# 创建 loss 函数\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 7) Training loop\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "best_acc = 0.0\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    total_loss, total_correct = 0, 0\n",
    "    for claim, evid, label in tqdm(train_dl, desc=f\"Epoch {epoch}\"):\n",
    "        claim, evid, label = claim.to(DEVICE), evid.to(DEVICE), label.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(claim, evid)\n",
    "        loss = criterion(out, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        total_correct += (out.argmax(1) == label).sum().item()\n",
    "    acc = total_correct / len(train_ds)\n",
    "    print(f\"Train Loss: {total_loss/len(train_dl):.4f}, Train Acc: {acc:.4f}\")\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss, val_correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for claim, evid, label in dev_dl:\n",
    "            claim, evid, label = claim.to(DEVICE), evid.to(DEVICE), label.to(DEVICE)\n",
    "            out = model(claim, evid)\n",
    "            loss = criterion(out, label)\n",
    "            val_loss += loss.item()\n",
    "            val_correct += (out.argmax(1) == label).sum().item()\n",
    "    val_acc = val_correct / len(dev_ds)\n",
    "    print(f\"Val Loss: {val_loss/len(dev_dl):.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"rnn_model.pth\")\n",
    "        print(f\"✅ New best model saved (epoch {epoch}, acc {val_acc:.4%})\\n\")\n",
    "    else:\n",
    "        print()\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 8) Save model + label encoder\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "\n",
    "import pickle\n",
    "with open('label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(label_enc, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "39ed9206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_file, evidence_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        claims_data = json.load(f)\n",
    "    with open(evidence_file, 'r', encoding='utf-8') as f:\n",
    "        evid_data = json.load(f)\n",
    "    \n",
    "    claim_ids, claim_texts, evid_texts = [], [], []\n",
    "    for cid, cdata in claims_data.items():\n",
    "        claim_ids.append(cid)\n",
    "        claim = preprocess(cdata['claim_text'])\n",
    "        evid_ids = cdata['evidences']\n",
    "        evids = ' '.join([evid_data.get(eid, '') for eid in evid_ids])\n",
    "        evid = preprocess(evids)\n",
    "        claim_texts.append(claim)\n",
    "        evid_texts.append(evid)\n",
    "    \n",
    "    test_claims = torch.tensor([text_to_seq(t) for t in claim_texts], dtype=torch.long).to(DEVICE)\n",
    "    test_evid = torch.tensor([text_to_seq(t) for t in evid_texts], dtype=torch.long).to(DEVICE)\n",
    "    \n",
    "    model.load_state_dict(torch.load('rnn_model.pth'))\n",
    "    model.eval()\n",
    "    with open('label_encoder.pkl', 'rb') as f:\n",
    "        label_enc = pickle.load(f)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(test_claims, test_evid)\n",
    "        preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "    pred_labels = label_enc.inverse_transform(preds)\n",
    "    \n",
    "    output_data = {cid: {'claim_text': ctext, 'claim_label': plabel, 'evidences': claims_data[cid]['evidences']} \n",
    "                   for cid, plabel, ctext in zip(claim_ids, pred_labels, claim_texts)}\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output_data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "97037bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict('test-claims-predictions.json', 'data/evidence.json', 'predicted_results.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
