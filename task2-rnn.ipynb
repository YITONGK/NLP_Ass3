{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72f72b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/sophia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/sophia/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sophia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 0) Config & Paths\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "TRAIN_JSON = DATA_DIR / \"train-claims.json\"\n",
    "DEV_JSON = DATA_DIR / \"dev-claims.json\"\n",
    "EVID_JSON = DATA_DIR / \"evidence.json\"\n",
    "\n",
    "MAX_LEN = 50\n",
    "EMBED_DIM = 100\n",
    "HIDDEN_DIM = 64\n",
    "NUM_CLASSES = 4\n",
    "DROPOUT_PROB = 0.4\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "LR = 1e-4\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 1) Preprocessing utils\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "stopwords = set(nltk_stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word, 'v')\n",
    "    return lemmatizer.lemmatize(lemma, 'n')\n",
    "\n",
    "def preprocess(text, remove_stopwords=True, lemma=True, stem=False):\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    tokens = [t for t in tokens if re.match('^[a-zA-Z0-9-]+$', t)]\n",
    "    if remove_stopwords:\n",
    "        tokens = [t for t in tokens if t not in stopwords]\n",
    "    if lemma:\n",
    "        tokens = [lemmatize(t) for t in tokens]\n",
    "    if stem:\n",
    "        tokens = [stemmer.stem(t) for t in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 2) Load JSON data → DataFrame\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def load_data(claims_file, evidence_file):\n",
    "    with open(claims_file, 'r', encoding='utf-8') as f:\n",
    "        claims_data = json.load(f)\n",
    "    with open(evidence_file, 'r', encoding='utf-8') as f:\n",
    "        evid_data = json.load(f)\n",
    "    \n",
    "    claim_texts, evid_texts, labels = [], [], []\n",
    "    for cid, cdata in claims_data.items():\n",
    "        claim = preprocess(cdata['claim_text'])\n",
    "        evid_ids = cdata['evidences']\n",
    "        evids = ' '.join([evid_data.get(eid, '') for eid in evid_ids])\n",
    "        evid = preprocess(evids)\n",
    "        claim_texts.append(claim)\n",
    "        evid_texts.append(evid)\n",
    "        labels.append(cdata['claim_label'])\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'claim': claim_texts,\n",
    "        'evidence': evid_texts,\n",
    "        'label': labels\n",
    "    })\n",
    "    return df\n",
    "\n",
    "train_df = load_data(TRAIN_JSON, EVID_JSON)\n",
    "dev_df = load_data(DEV_JSON, EVID_JSON)\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 3) Prepare vocab & sequences\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "all_text = train_df['claim'].tolist() + train_df['evidence'].tolist()\n",
    "token_counts = Counter(w for text in all_text for w in text.split())\n",
    "vocab = {w: idx+1 for idx, (w, _) in enumerate(token_counts.items())}\n",
    "vocab_size = len(vocab) + 1\n",
    "\n",
    "def text_to_seq(text):\n",
    "    seq = [vocab.get(w, 0) for w in text.split()]\n",
    "    return seq + [0]*(MAX_LEN - len(seq)) if len(seq) < MAX_LEN else seq[:MAX_LEN]\n",
    "\n",
    "train_claims = [text_to_seq(t) for t in train_df['claim']]\n",
    "train_evids = [text_to_seq(t) for t in train_df['evidence']]\n",
    "dev_claims = [text_to_seq(t) for t in dev_df['claim']]\n",
    "dev_evids = [text_to_seq(t) for t in dev_df['evidence']]\n",
    "\n",
    "label_enc = LabelEncoder()\n",
    "train_labels = label_enc.fit_transform(train_df['label'])\n",
    "dev_labels = label_enc.transform(dev_df['label'])\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 4) Dataset + DataLoader\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "class ClaimDataset(Dataset):\n",
    "    def __init__(self, claims, evidences, labels):\n",
    "        self.claims = torch.tensor(claims, dtype=torch.long)\n",
    "        self.evidences = torch.tensor(evidences, dtype=torch.long)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.claims[idx], self.evidences[idx], self.labels[idx]\n",
    "\n",
    "train_ds = ClaimDataset(train_claims, train_evids, train_labels)\n",
    "dev_ds = ClaimDataset(dev_claims, dev_evids, dev_labels)\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dev_dl = DataLoader(dev_ds, batch_size=BATCH_SIZE)\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 5) Self-Attention Pooling\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "class SelfAttentionPooling(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq_len, hidden_dim]\n",
    "        weights = torch.softmax(self.attention(x), dim=1)  # [batch, seq_len, 1]\n",
    "        pooled = torch.sum(weights * x, dim=1)  # [batch, hidden_dim]\n",
    "        return pooled\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 6) Model\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.embed_dropout = nn.Dropout(DROPOUT_PROB)\n",
    "        self.rnn_claim = nn.RNN(embed_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "        self.rnn_evid = nn.RNN(embed_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "        self.rnn_dropout = nn.Dropout(DROPOUT_PROB)\n",
    "        self.attention_claim = SelfAttentionPooling(hidden_dim * 2)\n",
    "        self.attention_evid = SelfAttentionPooling(hidden_dim * 2)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 4, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT_PROB),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, claim, evidence):\n",
    "        claim_emb = self.embed_dropout(self.embedding(claim))\n",
    "        evid_emb = self.embed_dropout(self.embedding(evidence))\n",
    "        \n",
    "        claim_out, _ = self.rnn_claim(claim_emb)\n",
    "        evid_out, _ = self.rnn_evid(evid_emb)\n",
    "        \n",
    "        claim_out = self.rnn_dropout(claim_out)\n",
    "        evid_out = self.rnn_dropout(evid_out)\n",
    "        \n",
    "        claim_pool = self.attention_claim(claim_out)\n",
    "        evid_pool = self.attention_evid(evid_out)\n",
    "        \n",
    "        combined = torch.cat([claim_pool, evid_pool], dim=1)\n",
    "        return self.classifier(combined)\n",
    "\n",
    "model = RNNModel(vocab_size, EMBED_DIM, HIDDEN_DIM, NUM_CLASSES).to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97f17914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e22176e30ae4481a470467b06b811b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0477, Train Acc: 0.5725\n",
      "Val Loss: 1.3388, Val Acc: 0.4481\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e81b82b4de404b49985e94062681a9e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0412, Train Acc: 0.5733\n",
      "Val Loss: 1.3230, Val Acc: 0.4675\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "344c2f83b1b24425afa5cbe2eb854a73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0350, Train Acc: 0.5757\n",
      "Val Loss: 1.3461, Val Acc: 0.4351\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fa3a54da0cd4fb2a187708092c663c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0188, Train Acc: 0.5741\n",
      "Val Loss: 1.3484, Val Acc: 0.4351\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0199c4a912f4907b91553081302b8b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0294, Train Acc: 0.5757\n",
      "Val Loss: 1.3646, Val Acc: 0.4351\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e635462dfa744cd8b8ddf4bedc87dd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0251, Train Acc: 0.5790\n",
      "Val Loss: 1.3629, Val Acc: 0.4351\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ed28bbfdeb4dcabd5cef6e416f4401",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0304, Train Acc: 0.5863\n",
      "Val Loss: 1.3566, Val Acc: 0.4351\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57c8ffa9e62e4bee8e6c328c14d96070",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0217, Train Acc: 0.5863\n",
      "Val Loss: 1.3563, Val Acc: 0.4416\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "def1c43c939b47648f030ebe07d604cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0229, Train Acc: 0.5855\n",
      "Val Loss: 1.3522, Val Acc: 0.4481\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7707114a0e6640a78820133cb84fbc48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0228, Train Acc: 0.5855\n",
      "Val Loss: 1.3749, Val Acc: 0.4351\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dd9d699c0354e06b4c801904febd58d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0063, Train Acc: 0.5855\n",
      "Val Loss: 1.3677, Val Acc: 0.4416\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "466b0d5b2a154e4c8f5b0b38331f7f70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0176, Train Acc: 0.5855\n",
      "Val Loss: 1.3847, Val Acc: 0.4481\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be72203853a34533b6046358fcfbcff6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9973, Train Acc: 0.5920\n",
      "Val Loss: 1.3841, Val Acc: 0.4675\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d084e79761514fe3a3db7bae9cdd58c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9931, Train Acc: 0.5961\n",
      "Val Loss: 1.3961, Val Acc: 0.4545\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b46fbe69f12f4a53b77eeee2cc08418f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9954, Train Acc: 0.5945\n",
      "Val Loss: 1.4149, Val Acc: 0.4545\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d04875f0db24c7798fefc1d95b94b21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9988, Train Acc: 0.5879\n",
      "Val Loss: 1.4091, Val Acc: 0.4545\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a7c73314ed142079908327f98934443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 17:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9821, Train Acc: 0.5961\n",
      "Val Loss: 1.4261, Val Acc: 0.4545\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c8ca071d82846ef87235d256da8b1d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 18:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9638, Train Acc: 0.5904\n",
      "Val Loss: 1.4283, Val Acc: 0.4610\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aecf7553873d41a18dddab53d93be1de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 19:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9876, Train Acc: 0.5912\n",
      "Val Loss: 1.4337, Val Acc: 0.4610\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ab4b7624a2e4fdd9e7d49811be3311b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 20:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0045, Train Acc: 0.5945\n",
      "Val Loss: 1.4259, Val Acc: 0.4610\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 7) Training loop\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    total_loss, total_correct = 0, 0\n",
    "    for claim, evid, label in tqdm(train_dl, desc=f\"Epoch {epoch}\"):\n",
    "        claim, evid, label = claim.to(DEVICE), evid.to(DEVICE), label.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(claim, evid)\n",
    "        loss = criterion(out, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        total_correct += (out.argmax(1) == label).sum().item()\n",
    "    acc = total_correct / len(train_ds)\n",
    "    print(f\"Train Loss: {total_loss/len(train_dl):.4f}, Train Acc: {acc:.4f}\")\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss, val_correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for claim, evid, label in dev_dl:\n",
    "            claim, evid, label = claim.to(DEVICE), evid.to(DEVICE), label.to(DEVICE)\n",
    "            out = model(claim, evid)\n",
    "            loss = criterion(out, label)\n",
    "            val_loss += loss.item()\n",
    "            val_correct += (out.argmax(1) == label).sum().item()\n",
    "    val_acc = val_correct / len(dev_ds)\n",
    "    print(f\"Val Loss: {val_loss/len(dev_dl):.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 8) Save model + label encoder\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "torch.save(model.state_dict(), \"rnn_model.pth\")\n",
    "import pickle\n",
    "with open('label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(label_enc, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39ed9206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_file, evidence_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        claims_data = json.load(f)\n",
    "    with open(evidence_file, 'r', encoding='utf-8') as f:\n",
    "        evid_data = json.load(f)\n",
    "    \n",
    "    claim_ids, claim_texts, evid_texts = [], [], []\n",
    "    for cid, cdata in claims_data.items():\n",
    "        claim_ids.append(cid)\n",
    "        claim = preprocess(cdata['claim_text'])\n",
    "        evid_ids = cdata['evidences']\n",
    "        evids = ' '.join([evid_data.get(eid, '') for eid in evid_ids])\n",
    "        evid = preprocess(evids)\n",
    "        claim_texts.append(claim)\n",
    "        evid_texts.append(evid)\n",
    "    \n",
    "    test_claims = torch.tensor([text_to_seq(t) for t in claim_texts], dtype=torch.long).to(DEVICE)\n",
    "    test_evid = torch.tensor([text_to_seq(t) for t in evid_texts], dtype=torch.long).to(DEVICE)\n",
    "    \n",
    "    model.load_state_dict(torch.load('rnn_model.pth'))\n",
    "    model.eval()\n",
    "    with open('label_encoder.pkl', 'rb') as f:\n",
    "        label_enc = pickle.load(f)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(test_claims, test_evid)\n",
    "        preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "    pred_labels = label_enc.inverse_transform(preds)\n",
    "    \n",
    "    output_data = {cid: {'claim_text': ctext, 'claim_label': plabel, 'evidences': claims_data[cid]['evidences']} \n",
    "                   for cid, plabel, ctext in zip(claim_ids, pred_labels, claim_texts)}\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output_data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97037bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict('test-claims-predictions.json', 'data/evidence.json', 'predicted_results.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
