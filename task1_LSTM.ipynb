{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f215ada8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid cache – preprocessing will start …\n",
      "Tokenising 1,208,827 evidence passages with 9 CPU process(es)…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stemming evidence: 100%|██████████| 1208827/1208827 [07:24<00:00, 2717.06doc/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidence saved → /Users/felikskong/Desktop/NLP/NLP_Ass3/preprocessed/evidence_stemmed.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stemming train-claims.json: 100%|██████████| 1228/1228 [00:44<00:00, 27.88doc/s]\n",
      "Stemming dev-claims.json: 100%|██████████| 154/154 [00:42<00:00,  3.59doc/s]\n",
      "Stemming test-claims-unlabelled.json: 100%|██████████| 153/153 [00:42<00:00,  3.60doc/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claims saved → /Users/felikskong/Desktop/NLP/NLP_Ass3/preprocessed/claims_stemmed.json\n",
      "\n",
      "=== Evidence after stemming ===\n",
      "Total passages        : 1,207,920\n",
      "Stem length (min/max) : 1 / 304\n",
      "Stem length (mean)    : 11.3\n",
      "Vocabulary size       : 510,195\n",
      "Top-20 stems          : [('also', 66963), ('state', 58250), ('bear', 56376), ('first', 53537), ('one', 49589), ('new', 44100), ('year', 42117), ('play', 39752), ('american', 39704), ('includ', 39608), ('use', 39337), ('unit', 38930), ('nation', 37995), ('name', 37335), ('know', 37286), ('district', 34882), ('two', 34481), ('film', 33964), ('counti', 32636), ('footbal', 31480)]\n",
      "\n",
      "Finished in 583.2 s – results cached for future runs.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Stemming-based preprocessing for the retrieval task.\n",
    "✓ Multi-process spaCy\n",
    "✓ Porter stemming\n",
    "✓ On-disk cache (evidence_stemmed.json / claims_stemmed.json)\n",
    "\"\"\"\n",
    "\n",
    "import json, statistics, collections, time, multiprocessing as mp\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import spacy\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ------------------------------------------------------------------\n",
    "DATA_DIR      = Path(\"data\")\n",
    "OUT_EVID      = Path(\"preprocessed/evidence_stemmed.json\")\n",
    "OUT_CLAIM     = Path(\"preprocessed/claims_stemmed.json\")\n",
    "FORCE_REBUILD = True                 # True → ignore cache, rebuild\n",
    "BATCH_SIZE    = 1_000                 # spaCy batch size\n",
    "NUM_PROC      = max(mp.cpu_count() - 1, 1)   # use all but 1 core\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# INITIALISE SPACY & STEMMER\n",
    "# ------------------------------------------------------------------\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
    "stemmer = PorterStemmer()\n",
    "stop_set = set(stopwords.words(\"english\"))\n",
    "\n",
    "def stem_doc(doc):\n",
    "    out = []\n",
    "    for tok in doc:\n",
    "        lemma = tok.lemma_.lower()\n",
    "        if lemma.isalpha() and lemma not in stop_set:\n",
    "            out.append(stemmer.stem(lemma))\n",
    "    return out\n",
    "\n",
    "def jload(path: Path):\n",
    "    with path.open(encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def jdump(obj, path: Path):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 0) LOAD CACHE (IF PRESENT)\n",
    "# ------------------------------------------------------------------\n",
    "if OUT_EVID.exists() and OUT_CLAIM.exists() and not FORCE_REBUILD:\n",
    "    t0 = time.time()\n",
    "    evidence_proc  = jload(OUT_EVID)\n",
    "    claim_proc_all = jload(OUT_CLAIM)\n",
    "    print(f\"Cached data loaded in {time.time() - t0:.2f} s – ready to use.\")\n",
    "    exit(0)\n",
    "\n",
    "print(\"No valid cache – preprocessing will start …\")\n",
    "t_start = time.time()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1) PRE-PROCESS EVIDENCE (PARALLEL)\n",
    "# ------------------------------------------------------------------\n",
    "evidence_raw = jload(DATA_DIR / \"evidence.json\")\n",
    "evid_ids     = list(evidence_raw.keys())\n",
    "evid_texts   = list(evidence_raw.values())\n",
    "\n",
    "evidence_proc = {}\n",
    "lengths = []\n",
    "\n",
    "print(f\"Tokenising {len(evid_ids):,} evidence passages \"\n",
    "      f\"with {NUM_PROC} CPU process(es)…\")\n",
    "\n",
    "for evid_id, doc in tqdm(\n",
    "        zip(evid_ids,\n",
    "            nlp.pipe(evid_texts,\n",
    "                     batch_size=BATCH_SIZE,\n",
    "                     n_process=NUM_PROC)),\n",
    "        total=len(evid_ids),\n",
    "        desc=\"Stemming evidence\",\n",
    "        unit=\"doc\"\n",
    "):\n",
    "    stems = stem_doc(doc)\n",
    "    if stems:\n",
    "        evidence_proc[evid_id] = stems\n",
    "        lengths.append(len(stems))\n",
    "\n",
    "jdump(evidence_proc, OUT_EVID)\n",
    "print(f\"Evidence saved → {OUT_EVID.resolve()}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2) PRE-PROCESS CLAIMS (PARALLEL, PER SPLIT)\n",
    "# ------------------------------------------------------------------\n",
    "claim_files = [\n",
    "    \"train-claims.json\",\n",
    "    \"dev-claims.json\",\n",
    "    \"test-claims-unlabelled.json\",\n",
    "]\n",
    "claim_proc_all = {}\n",
    "\n",
    "for fname in claim_files:\n",
    "    raw_claims = jload(DATA_DIR / fname)\n",
    "    cids  = list(raw_claims.keys())\n",
    "    texts = [raw_claims[cid][\"claim_text\"] for cid in cids]\n",
    "\n",
    "    for cid, doc in tqdm(\n",
    "            zip(cids,\n",
    "                nlp.pipe(texts,\n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         n_process=NUM_PROC)),\n",
    "            total=len(cids),\n",
    "            desc=f\"Stemming {fname}\",\n",
    "            unit=\"doc\"\n",
    "    ):\n",
    "        stems = stem_doc(doc)\n",
    "        if stems:\n",
    "            claim_proc_all[cid] = stems\n",
    "\n",
    "jdump(claim_proc_all, OUT_CLAIM)\n",
    "print(f\"Claims saved → {OUT_CLAIM.resolve()}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3) QUICK CORPUS STATISTICS\n",
    "# ------------------------------------------------------------------\n",
    "print(\"\\n=== Evidence after stemming ===\")\n",
    "print(f\"Total passages        : {len(evidence_proc):,}\")\n",
    "print(f\"Stem length (min/max) : {min(lengths)} / {max(lengths)}\")\n",
    "print(f\"Stem length (mean)    : {statistics.mean(lengths):.1f}\")\n",
    "\n",
    "vocab = {s for toks in evidence_proc.values() for s in toks}\n",
    "print(f\"Vocabulary size       : {len(vocab):,}\")\n",
    "\n",
    "counter = collections.Counter(s for toks in evidence_proc.values() for s in toks)\n",
    "print(\"Top-20 stems          :\", counter.most_common(20))\n",
    "\n",
    "print(f\"\\nFinished in {time.time() - t_start:.1f} s – \"\n",
    "      f\"results cached for future runs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46c25060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0) Imports & Config\n",
    "import os, json, random, itertools, collections\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Hyperparameters & Paths\n",
    "DATA_DIR      = Path(\"data\")\n",
    "PRE_DIR       = Path(\"preprocessed\")\n",
    "EVID_J        = PRE_DIR/\"evidence_stemmed.json\"\n",
    "CLAIM_J       = PRE_DIR/\"claims_stemmed.json\"\n",
    "TRAIN_J       = DATA_DIR/\"train-claims.json\"\n",
    "DEV_J         = DATA_DIR/\"dev-claims.json\"\n",
    "EVID_CORPUS_J = DATA_DIR/\"evidence.json\"\n",
    "\n",
    "EMB_DIM   = 100\n",
    "HID_DIM   = 128\n",
    "BATCH     = 128\n",
    "EPOCHS    = 5\n",
    "LR        = 3e-4\n",
    "MARGIN    = 0.3\n",
    "MIN_FREQ  = 3\n",
    "TOP_K     = 5\n",
    "DEVICE    = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 1) Load pre-stemmed data\n",
    "with open(EVID_J, \"r\", encoding=\"utf-8\") as f:\n",
    "    evidence_proc = json.load(f)\n",
    "with open(CLAIM_J, \"r\", encoding=\"utf-8\") as f:\n",
    "    claim_proc_all = json.load(f)\n",
    "\n",
    "# 2) Build vocab\n",
    "freq = collections.Counter(\n",
    "    t for toks in itertools.chain(evidence_proc.values(),\n",
    "                                  claim_proc_all.values())\n",
    "    for t in toks\n",
    ")\n",
    "PAD, UNK = \"<PAD>\", \"<UNK>\"\n",
    "itos = [PAD, UNK] + [t for t,c in freq.items() if c>=MIN_FREQ]\n",
    "stoi = {t:i for i,t in enumerate(itos)}\n",
    "def numerise(tokens):\n",
    "    return [stoi.get(t, stoi[UNK]) for t in tokens]\n",
    "\n",
    "# 3) Load labels\n",
    "train_lbl = json.loads(TRAIN_J.read_text())\n",
    "dev_lbl   = json.loads(DEV_J.read_text())\n",
    "\n",
    "# 4) Triplet Dataset & DataLoader\n",
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, labeled, evid_dict):\n",
    "        items, evid_ids = [], list(evid_dict.keys())\n",
    "        for cid, obj in labeled.items():\n",
    "            pos = [e for e in obj[\"evidences\"] if e in evid_dict]\n",
    "            for p in pos:\n",
    "                n = random.choice(evid_ids)\n",
    "                while n==p: n = random.choice(evid_ids)\n",
    "                items.append((cid, p, n))\n",
    "        self.items = items\n",
    "        self.evid  = evid_dict\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "    def __getitem__(self, idx):\n",
    "        cid,p,n = self.items[idx]\n",
    "        return (\n",
    "          torch.tensor(numerise(claim_proc_all[cid]), dtype=torch.long),\n",
    "          torch.tensor(numerise(self.evid[p]), dtype=torch.long),\n",
    "          torch.tensor(numerise(self.evid[n]), dtype=torch.long),\n",
    "        )\n",
    "\n",
    "def collate_fn(batch):\n",
    "    def pad(seqs):\n",
    "        m = max(len(s) for s in seqs)\n",
    "        return torch.tensor([s.tolist()+[0]*(m-len(s)) for s in seqs])\n",
    "    c,p,n = zip(*batch)\n",
    "    return pad(c), pad(p), pad(n)\n",
    "\n",
    "train_ds = TripletDataset(train_lbl, evidence_proc)\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH, shuffle=True,\n",
    "                      collate_fn=collate_fn)\n",
    "\n",
    "# 5) BiLSTM Sentence Encoder\n",
    "class BiLSTMSentenceEncoder(nn.Module):\n",
    "    def __init__(self, vocab_sz, emb_dim=EMB_DIM, hid_dim=HID_DIM):\n",
    "        super().__init__()\n",
    "        self.emb  = nn.Embedding(vocab_sz, emb_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, batch_first=True,\n",
    "                            bidirectional=True)\n",
    "    def forward(self, x):\n",
    "        mask = (x!=0).float().unsqueeze(-1)\n",
    "        out, _ = self.lstm(self.emb(x))\n",
    "        # mean‐pool over the length dim\n",
    "        out = (out * mask).sum(1) / mask.sum(1)\n",
    "        return nn.functional.normalize(out, p=2, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd514b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 33/33 [01:29<00:00,  2.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1 avg loss = 0.1006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 33/33 [01:21<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 2 avg loss = 0.0620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 33/33 [01:26<00:00,  2.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 3 avg loss = 0.0442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 33/33 [01:42<00:00,  3.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 4 avg loss = 0.0326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 33/33 [01:24<00:00,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 5 avg loss = 0.0243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 6) Train the retriever\n",
    "model   = BiLSTMSentenceEncoder(len(itos)).to(DEVICE)\n",
    "optim   = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "loss_fn = nn.MarginRankingLoss(margin=MARGIN)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total = 0\n",
    "    for c, p, n in tqdm(train_dl, desc=f\"Epoch {epoch+1}\"):\n",
    "        c,p,n = [t.to(DEVICE) for t in (c,p,n)]\n",
    "        vc, vp, vn = model(c), model(p), model(n)\n",
    "        pos_sim = (vc * vp).sum(1)\n",
    "        neg_sim = (vc * vn).sum(1)\n",
    "        loss    = loss_fn(pos_sim, neg_sim,\n",
    "                          torch.ones_like(pos_sim, device=DEVICE))\n",
    "        optim.zero_grad(); loss.backward(); optim.step()\n",
    "        total += loss.item()\n",
    "    print(f\"  Epoch {epoch+1} avg loss = {total/len(train_dl):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0d1949e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding evidence: 100%|██████████| 2360/2360 [52:21<00:00,  1.33s/it] \n"
     ]
    }
   ],
   "source": [
    "# 7) Encode all evidence\n",
    "# single‐worker to avoid multiprocessing issues in notebook\n",
    "evidence_vecs = {}\n",
    "loader = DataLoader(\n",
    "    [(eid, torch.tensor(numerise(evidence_proc[eid]),\n",
    "                        dtype=torch.long))\n",
    "     for eid in evidence_proc],\n",
    "    batch_size=512, shuffle=False,\n",
    "    collate_fn=lambda batch: (\n",
    "        [e[0] for e in batch],\n",
    "        pad_sequence([e[1] for e in batch],\n",
    "                     batch_first=True, padding_value=0)\n",
    "    ),\n",
    "    num_workers=0,\n",
    ")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for eids, seqs in tqdm(loader, desc=\"Encoding evidence\"):\n",
    "        vecs = model(seqs.to(DEVICE)).cpu()\n",
    "        for eid, v in zip(eids, vecs):\n",
    "            evidence_vecs[eid] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8739abb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 154/154 [05:03<00:00,  1.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recall@3:    5.94%\n",
      "Precision@3: 4.76%\n",
      "F1@3:        4.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 154/154 [04:53<00:00,  1.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recall@4:    6.20%\n",
      "Precision@4: 3.90%\n",
      "F1@4:        4.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 154/154 [04:54<00:00,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recall@5:    7.11%\n",
      "Precision@5: 3.77%\n",
      "F1@5:        4.61%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 8) Ranking & Evaluation on dev\n",
    "def rank_evidence(stems, top_k):\n",
    "    idxs = numerise(stems)\n",
    "    x = torch.tensor([idxs], dtype=torch.long, device=DEVICE)\n",
    "    with torch.no_grad():\n",
    "        vc = model(x).cpu().squeeze(0)\n",
    "    sims = {eid: float(torch.dot(vc, v_e))\n",
    "            for eid, v_e in evidence_vecs.items()}\n",
    "    return sorted(sims, key=sims.get, reverse=True)[:top_k]\n",
    "\n",
    "def evaluate(top_k):\n",
    "    recalls, precisions, f1s = [], [], []\n",
    "    for cid, obj in tqdm(dev_lbl.items(), desc=\"Evaluating\"):\n",
    "        gold      = set(obj[\"evidences\"])\n",
    "        retrieved = rank_evidence(claim_proc_all[cid], top_k)\n",
    "        hits      = len(gold & set(retrieved))\n",
    "        r = hits/len(gold) if gold else 0.0\n",
    "        p = hits/top_k\n",
    "        f = (2*r*p/(r+p)) if (r+p)>0 else 0.0\n",
    "        recalls.append(r); precisions.append(p); f1s.append(f)\n",
    "\n",
    "    print(f\"\\nRecall@{top_k}:    {np.mean(recalls):.2%}\")\n",
    "    print(f\"Precision@{top_k}: {np.mean(precisions):.2%}\")\n",
    "    print(f\"F1@{top_k}:        {np.mean(f1s):.2%}\")\n",
    "\n",
    "for k in [3, 4, 5]:\n",
    "    evaluate(k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
