{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6e69f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/felikskong/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/felikskong/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/felikskong/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import logging as hf_logging\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import re\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 0) 配置与准备\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MAX_LEN = 256\n",
    "id2label = {0: \"SUPPORTS\", 1: \"NOT_ENOUGH_INFO\", 2: \"REFUTES\", 3: \"DISPUTED\"}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 1) 加载数据\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "with open(\"test-claims-predictions.json\", \"r\") as f:\n",
    "    test_claims = json.load(f)\n",
    "\n",
    "with open(\"data/evidence.json\", \"r\") as f:\n",
    "    evidence_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bee3c272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 2) 加载 BERT 模型\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "bert_model = BertForSequenceClassification.from_pretrained(\"my_bert_classifier\").to(DEVICE)\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"my_bert_classifier\")\n",
    "bert_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e8846ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xv/d8fp_fgs1fx30rm4fs63qv1c0000gn/T/ipykernel_82566/3862677291.py:44: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  bilstm_model.load_state_dict(torch.load(\"task2_best_model_6.pt\", map_location=DEVICE))\n"
     ]
    }
   ],
   "source": [
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 3) 加载 BiLSTM 模型\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "class BiLSTMWithBertEncoder(torch.nn.Module):\n",
    "    def __init__(self, bert_name, lstm_hid, num_classes, dropout_prob, lstm_layers):\n",
    "        super().__init__()\n",
    "        from transformers import AutoModel\n",
    "        self.bert = AutoModel.from_pretrained(bert_name)\n",
    "        for p in self.bert.parameters():\n",
    "            p.requires_grad = False\n",
    "        bert_dim = self.bert.config.hidden_size\n",
    "        self.dropout_bert = torch.nn.Dropout(dropout_prob)\n",
    "        self.lstm = torch.nn.LSTM(\n",
    "            input_size=bert_dim,\n",
    "            hidden_size=lstm_hid,\n",
    "            num_layers=lstm_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout_prob\n",
    "        )\n",
    "        self.attn_fc = torch.nn.Linear(2 * lstm_hid, 1)\n",
    "        self.dropout_pool = torch.nn.Dropout(dropout_prob)\n",
    "        self.classifier = torch.nn.Linear(2 * lstm_hid, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        bert_out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        seq_emb = self.dropout_bert(bert_out.last_hidden_state)\n",
    "        lstm_out, _ = self.lstm(seq_emb)\n",
    "        scores = self.attn_fc(lstm_out).squeeze(-1)\n",
    "        scores = scores.masked_fill(attention_mask == 0, -1e9)\n",
    "        alphas = torch.softmax(scores, dim=1)\n",
    "        pooled = torch.sum(lstm_out * alphas.unsqueeze(-1), dim=1)\n",
    "        pooled = self.dropout_pool(pooled)\n",
    "        logits = self.classifier(pooled)\n",
    "        return logits\n",
    "\n",
    "BERT_MODEL = \"bert-base-uncased\"\n",
    "LSTM_HID_DIM = 512\n",
    "NUM_CLASSES = 4\n",
    "DROPOUT_PROB = 0.2\n",
    "NUM_LAYERS = 3\n",
    "\n",
    "bilstm_model = BiLSTMWithBertEncoder(BERT_MODEL, LSTM_HID_DIM, NUM_CLASSES, DROPOUT_PROB, NUM_LAYERS).to(DEVICE)\n",
    "bilstm_model.load_state_dict(torch.load(\"task2_best_model_6.pt\", map_location=DEVICE))\n",
    "bilstm_model.eval()\n",
    "bilstm_tokenizer = bert_tokenizer  # 可共用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c8146cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xv/d8fp_fgs1fx30rm4fs63qv1c0000gn/T/ipykernel_82566/3133519141.py:97: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  rnn_model.load_state_dict(torch.load(\"rnn_model.pth\", map_location=DEVICE))\n"
     ]
    }
   ],
   "source": [
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 4) 加载 RNN 模型\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MAX_LEN = 50\n",
    "EMBED_DIM = 100\n",
    "HIDDEN_DIM = 64\n",
    "NUM_CLASSES = 4\n",
    "DROPOUT_PROB = 0.4\n",
    "vocab_size = 1  # placeholder, will be overwritten\n",
    "\n",
    "stopwords = set(nltk_stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word, 'v')\n",
    "    return lemmatizer.lemmatize(lemma, 'n')\n",
    "\n",
    "def preprocess(text, remove_stopwords=True, lemma=True, stem=False):\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    tokens = [t for t in tokens if re.match('^[a-zA-Z0-9-]+$', t)]\n",
    "    if remove_stopwords:\n",
    "        tokens = [t for t in tokens if t not in stopwords]\n",
    "    if lemma:\n",
    "        tokens = [lemmatize(t) for t in tokens]\n",
    "    if stem:\n",
    "        tokens = [stemmer.stem(t) for t in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def text_to_seq(text):\n",
    "    tokens = text.split()\n",
    "    seq = [vocab.get(t, 0) for t in tokens]\n",
    "    return seq + [0] * (MAX_LEN - len(seq)) if len(seq) < MAX_LEN else seq[:MAX_LEN]\n",
    "\n",
    "class SelfAttentionPooling(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        weights = torch.softmax(self.attention(x), dim=1)\n",
    "        pooled = torch.sum(weights * x, dim=1)\n",
    "        return pooled\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.embed_dropout = nn.Dropout(DROPOUT_PROB)\n",
    "        self.rnn_claim = nn.RNN(embed_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "        self.rnn_evid = nn.RNN(embed_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "        self.rnn_dropout = nn.Dropout(DROPOUT_PROB)\n",
    "        self.attention_claim = SelfAttentionPooling(hidden_dim * 2)\n",
    "        self.attention_evid = SelfAttentionPooling(hidden_dim * 2)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 4, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT_PROB),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, claim, evidence):\n",
    "        claim_emb = self.embed_dropout(self.embedding(claim))\n",
    "        evid_emb = self.embed_dropout(self.embedding(evidence))\n",
    "\n",
    "        claim_out, _ = self.rnn_claim(claim_emb)\n",
    "        evid_out, _ = self.rnn_evid(evid_emb)\n",
    "\n",
    "        claim_out = self.rnn_dropout(claim_out)\n",
    "        evid_out = self.rnn_dropout(evid_out)\n",
    "\n",
    "        claim_pool = self.attention_claim(claim_out)\n",
    "        evid_pool = self.attention_evid(evid_out)\n",
    "\n",
    "        combined = torch.cat([claim_pool, evid_pool], dim=1)\n",
    "        return self.classifier(combined)\n",
    "\n",
    "with open(\"data/train-claims.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    train_data = json.load(f)\n",
    "with open(\"data/evidence.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    evidence_dict = json.load(f)\n",
    "\n",
    "all_text = []\n",
    "for item in train_data.values():\n",
    "    claim = preprocess(item[\"claim_text\"])\n",
    "    evids = ' '.join([evidence_dict.get(eid, '') for eid in item[\"evidences\"]])\n",
    "    ev_text = preprocess(evids)\n",
    "    all_text.extend(claim.split() + ev_text.split())\n",
    "\n",
    "token_counts = Counter(all_text)\n",
    "vocab = {w: idx + 1 for idx, (w, _) in enumerate(token_counts.items())}\n",
    "vocab_size = len(vocab) + 1\n",
    "\n",
    "# 加载模型权重\n",
    "rnn_model = RNNModel(vocab_size, EMBED_DIM, HIDDEN_DIM, NUM_CLASSES).to(DEVICE)\n",
    "rnn_model.load_state_dict(torch.load(\"rnn_model.pth\", map_location=DEVICE))\n",
    "rnn_model.eval()\n",
    "\n",
    "with open(\"label_encoder.pkl\", \"rb\") as f:\n",
    "    label_enc = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d5f24df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 5) 推理函数\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "def get_bert_probs(claim, evid_ids):\n",
    "    evids = \" \".join([evidence_dict.get(eid, \"\") for eid in evid_ids])\n",
    "    inputs = bert_tokenizer(claim, evids, truncation=True, padding=\"max_length\", max_length=MAX_LEN, return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        logits = bert_model(**inputs).logits\n",
    "        probs = F.softmax(logits, dim=-1).cpu().numpy()[0]\n",
    "    reordered = [probs[0], probs[2], probs[1], probs[3]]\n",
    "    return np.array(reordered)\n",
    "\n",
    "def get_bilstm_probs(claim, evid_ids):\n",
    "    evids = \" \".join([evidence_dict.get(eid, \"\") for eid in evid_ids])\n",
    "    inputs = bilstm_tokenizer(claim, evids, truncation=True, padding=\"max_length\", max_length=MAX_LEN, return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        logits = bilstm_model(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
    "    return F.softmax(logits, dim=-1).cpu().numpy()[0]\n",
    "\n",
    "\n",
    "def get_rnn_probs(claim, evid_ids):\n",
    "    claim_text = preprocess(claim)\n",
    "    evid_text = \" \".join([evidence_dict.get(eid, \"\") for eid in evid_ids])\n",
    "    evid_text = preprocess(evid_text)\n",
    "    claim_seq = text_to_seq(claim_text)\n",
    "    evid_seq = text_to_seq(evid_text)\n",
    "    claim_tensor = torch.tensor([claim_seq], dtype=torch.long).to(DEVICE)\n",
    "    evid_tensor = torch.tensor([evid_seq], dtype=torch.long).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        logits = rnn_model(claim_tensor, evid_tensor)\n",
    "        probs = F.softmax(logits, dim=-1).cpu().numpy()[0]\n",
    "    reordered = [probs[3], probs[1], probs[2], probs[0]]\n",
    "    return np.array(reordered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bb1f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ensemble Predicting: 100%|██████████| 154/154 [02:47<00:00,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Ensemble Accuracy on Dev Set: 0.5519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ✅ 关闭 transformers 中关于 token 溢出的警告\n",
    "hf_logging.set_verbosity_error()\n",
    "\n",
    "with open(\"data/dev-claims.json\", \"r\") as f:\n",
    "    dev_claims = json.load(f)\n",
    "\n",
    "# soft voting\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "for cid, entry in tqdm(dev_claims.items(), desc=\"Ensemble Predicting\"):\n",
    "    claim_text = entry[\"claim_text\"]\n",
    "    evidence_ids = entry.get(\"evidences\", [])\n",
    "    true_label = label2id[entry[\"claim_label\"]]\n",
    "\n",
    "    p1 = get_bert_probs(claim_text, evidence_ids)\n",
    "    p2 = get_bilstm_probs(claim_text, evidence_ids)\n",
    "    p3 = get_rnn_probs(claim_text, evidence_ids)\n",
    "\n",
    "    avg_probs = (p1 + p2 + p3) / 3\n",
    "    pred_idx = int(np.argmax(avg_probs))\n",
    "\n",
    "    true_labels.append(true_label)\n",
    "    pred_labels.append(pred_idx)\n",
    "\n",
    "acc = accuracy_score(true_labels, pred_labels)\n",
    "print(f\"✅ Ensemble Accuracy on Dev Set: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51eac59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 6) Soft Voting Ensemble 推理\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "final_preds = {}\n",
    "\n",
    "for cid, entry in tqdm(test_claims.items(), desc=\"Ensemble Predicting\"):\n",
    "    claim = entry[\"claim_text\"]\n",
    "    evid_ids = entry.get(\"evidences\", [])\n",
    "\n",
    "    p1 = get_bert_probs(claim, evid_ids)\n",
    "    p2 = get_bilstm_probs(claim, evid_ids)\n",
    "    p3 = get_rnn_probs(claim, evid_ids)\n",
    "\n",
    "    avg_probs = (p1 + p2 + p3) / 3\n",
    "    pred_idx = int(np.argmax(avg_probs))\n",
    "    pred_label = id2label[pred_idx]\n",
    "\n",
    "    final_preds[cid] = {\n",
    "        \"claim_text\": claim,\n",
    "        \"claim_label\": pred_label,\n",
    "        \"evidences\": evid_ids\n",
    "    }\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 7) 保存输出\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "with open(\"softvote-predictions.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(final_preds, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"✅ Ensemble prediction saved to softvote-predictions.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "03943417",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting stacking features: 100%|██████████| 154/154 [02:31<00:00,  1.02it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "X_dev = []\n",
    "y_dev = []\n",
    "\n",
    "for cid, entry in tqdm(dev_claims.items(), desc=\"Extracting stacking features\"):\n",
    "    claim_text = entry[\"claim_text\"]\n",
    "    evidence_ids = entry.get(\"evidences\", [])\n",
    "    label = label2id[entry[\"claim_label\"]]\n",
    "\n",
    "    p1 = get_bert_probs(claim_text, evidence_ids)\n",
    "    p2 = get_bilstm_probs(claim_text, evidence_ids)\n",
    "    p3 = get_rnn_probs(claim_text, evidence_ids)\n",
    "\n",
    "    features = np.concatenate([p1, p2, p3])\n",
    "    X_dev.append(features)\n",
    "    y_dev.append(label)\n",
    "\n",
    "X_dev = np.array(X_dev)\n",
    "y_dev = np.array(y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2374e6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting stacking features: 100%|██████████| 154/154 [03:12<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Meta-model accuracy on dev set: 0.6169\n"
     ]
    }
   ],
   "source": [
    "meta_clf_lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "meta_clf_lr.fit(X_dev, y_dev)\n",
    "\n",
    "dev_preds = meta_clf_lr.predict(X_dev)\n",
    "acc = accuracy_score(y_dev, dev_preds)\n",
    "print(f\"✅ LR accuracy on dev set: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c179e810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MLP accuracy on dev set: 0.7403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/felikskong/anaconda3/envs/nlp/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "meta_clf_mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(64, 32),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "meta_clf_mlp.fit(X_dev, y_dev)\n",
    "\n",
    "dev_preds_mlp = meta_clf_mlp.predict(X_dev)\n",
    "acc_mlp = accuracy_score(y_dev, dev_preds_mlp)\n",
    "print(f\"✅ MLP accuracy on dev set: {acc_mlp:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8e122c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RandomForest accuracy on dev set: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "meta_clf_rf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "meta_clf_rf.fit(X_dev, y_dev)\n",
    "\n",
    "dev_preds_rf = meta_clf_rf.predict(X_dev)\n",
    "acc_rf = accuracy_score(y_dev, dev_preds_rf)\n",
    "print(f\"✅ RandomForest accuracy on dev set: {acc_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28277725",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stacking Predicting: 100%|██████████| 153/153 [02:55<00:00,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved stacking predictions to stacking-predictions.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "final_preds = {}\n",
    "\n",
    "for cid, entry in tqdm(test_claims.items(), desc=\"Stacking Predicting\"):\n",
    "    claim = entry[\"claim_text\"]\n",
    "    evidence_ids = entry.get(\"evidences\", [])\n",
    "\n",
    "    p1 = get_bert_probs(claim, evidence_ids)\n",
    "    p2 = get_bilstm_probs(claim, evidence_ids)\n",
    "    p3 = get_rnn_probs(claim, evidence_ids)\n",
    "\n",
    "    features = np.concatenate([p1, p2, p3]).reshape(1, -1)\n",
    "    pred_idx = meta_clf_mlp.predict(features)[0]\n",
    "    pred_label = id2label[pred_idx]\n",
    "\n",
    "    final_preds[cid] = {\n",
    "        \"claim_text\": claim,\n",
    "        \"claim_label\": pred_label,\n",
    "        \"evidences\": evidence_ids\n",
    "    }\n",
    "\n",
    "# 保存为 JSON 以便上传\n",
    "with open(\"stacking-predictions.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(final_preds, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"✅ Saved stacking predictions to stacking-predictions.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
