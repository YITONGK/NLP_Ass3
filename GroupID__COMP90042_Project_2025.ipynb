{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32yCsRUo8H33"
      },
      "source": [
        "# 2025 COMP90042 Project\n",
        "*Make sure you change the file name with your group id.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCybYoGz8YWQ"
      },
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the marker, please mention here.*\n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style, please put those the bottom of this ipynb file*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6po98qVA8bJD"
      },
      "source": [
        "# 1.DataSet Processing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.1 Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qvff21Hv8zjk"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/felikskong/anaconda3/envs/nlp_final/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "/Users/felikskong/anaconda3/envs/nlp_final/lib/python3.10/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/Users/felikskong/anaconda3/envs/nlp_final/lib/python3.10/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/felikskong/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/felikskong/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /Users/felikskong/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/felikskong/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Built-in\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import re\n",
        "from collections import Counter, defaultdict\n",
        "from pathlib import Path\n",
        "\n",
        "# NLP\n",
        "import faiss\n",
        "import nltk\n",
        "import spacy\n",
        "from lemminflect import getAllInflections\n",
        "from nltk.corpus import stopwords as nltk_stopwords, wordnet\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Transformers & Sentence Transformers\n",
        "from datasets import Dataset\n",
        "from sentence_transformers import CrossEncoder, InputExample, SentenceTransformer, losses\n",
        "from transformers import (\n",
        "    BertForSequenceClassification,\n",
        "    BertModel,\n",
        "    BertTokenizer,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    logging as hf_logging\n",
        ")\n",
        "\n",
        "# sklearn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Utility\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Resource downloads\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Ignore Warning Output\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2 Define file path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_claims_path = './data/train-claims.json'\n",
        "dev_claims_path = './data/dev-claims.json'\n",
        "evidence_path = './data/evidence.json'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.3 Task 1 - Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step1: English keep 1207838/1208827\n",
            "Step2: Climate-related keep 385471/1207838\n"
          ]
        }
      ],
      "source": [
        "# ✅\n",
        "# load data\n",
        "with open(train_claims_path, 'r') as f:\n",
        "    train_claims = json.load(f)\n",
        "\n",
        "with open(evidence_path, 'r') as f:\n",
        "    evidence_dict = json.load(f)\n",
        "\n",
        "# Extract all nouns from claim_text and count their frequencies \n",
        "all_nouns = []\n",
        "for claim_obj in train_claims.values():\n",
        "    doc = nlp(claim_obj[\"claim_text\"])\n",
        "    nouns = [token.lemma_.lower() for token in doc if token.pos_ == \"NOUN\"]\n",
        "    all_nouns.extend(nouns)\n",
        "\n",
        "# Select the top 100 most frequent nouns as keywords\n",
        "top_keywords = set(word for word, _ in Counter(all_nouns).most_common(100))\n",
        "\n",
        "all_forms = set()\n",
        "for lemma in top_keywords:\n",
        "    all_forms.add(lemma)\n",
        "    # Get all possible noun forms\n",
        "    infl_map = getAllInflections(lemma, upos=\"NOUN\")\n",
        "    # infl_map is a dict: { 'NNS': ['cats'], 'NNPS': ['children'], ... }\n",
        "    for forms in infl_map.values():\n",
        "        all_forms.update(forms)\n",
        "def contains_climate_keywords(text: str, all_forms: set) -> bool:\n",
        "    # Lowercase the text and split into words then check the set\n",
        "    words = re.findall(r\"\\b[a-z']+\\b\", text.lower())\n",
        "    return any(word in all_forms for word in words)\n",
        "\n",
        "\n",
        "def is_english(text: str, threshold: float = 0.5) -> bool:\n",
        "    # Clean the text, only keep letters and spaces\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    if len(text) == 0:  # If the text is empty after cleaning, return False\n",
        "        return False\n",
        "    # Calculate the proportion of English characters\n",
        "    english_char_count = sum(1 for char in text if char.isalpha())\n",
        "    return (english_char_count / len(text)) >= threshold\n",
        "\n",
        "def clean_and_split(eid, text):\n",
        "    result_ids = []\n",
        "    result_texts = []\n",
        "    sentences = sent_tokenize(text)\n",
        "    for i, sent in enumerate(sentences):\n",
        "        sent = sent.lower()\n",
        "        sent = re.sub(r'[^a-z0-9\\s.,!?]', '', sent)  # Remove punctuation\n",
        "        sent = re.sub(r'\\s+', ' ', sent).strip()\n",
        "        if len(sent.split()) >= 5:  # Optional: Filter out too short texts\n",
        "            result_ids.append(f\"{eid}_s{i}\")  # Use the original eID plus sentence index\n",
        "            result_texts.append(sent)\n",
        "    return result_ids, result_texts\n",
        "# Load the evidence embeddings\n",
        "word_embedding_path = './word_embedding/evidence_embeddings.npy'\n",
        "word_embedding_meta_path = \"./word_embedding/evidence_meta.pkl\"\n",
        "\n",
        "\n",
        "with open(evidence_path, 'r') as f:\n",
        "    evidence_dict = json.load(f)\n",
        "# 1. Remove non-English\n",
        "eids  = list(evidence_dict.keys())\n",
        "texts = list(evidence_dict.values())\n",
        "english_pairs = [\n",
        "    (eid, txt)\n",
        "    for eid, txt in zip(eids, texts)\n",
        "    if is_english(txt)\n",
        "]\n",
        "print(f\"Step1: English keep {len(english_pairs)}/{len(texts)}\")\n",
        "\n",
        "# 2. Remove non-climate-related\n",
        "climate_pairs = [\n",
        "    (eid, txt)\n",
        "    for eid, txt in english_pairs\n",
        "    if contains_climate_keywords(txt, all_forms)\n",
        "]\n",
        "print(f\"Step2: Climate-related keep {len(climate_pairs)}/{len(english_pairs)}\")\n",
        "\n",
        "\n",
        "# The results of cleaning and splitting\n",
        "cleaned_evidence_ids = []\n",
        "cleaned_evidence_texts = []\n",
        "original_evidence_ids = []  # Record the original evidence_id of each sentence\n",
        "\n",
        "# Iterate through the evidence data and clean and split\n",
        "for eid, text in climate_pairs:\n",
        "    cleaned_ids, cleaned_texts = clean_and_split(eid, text)\n",
        "    cleaned_evidence_ids.extend(cleaned_ids)\n",
        "    cleaned_evidence_texts.extend(cleaned_texts)\n",
        "    original_evidence_ids.extend([eid] * len(cleaned_ids))  # Each sentence records the original eID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.4 Task 2 - Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.4.1 Data Processing for BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# label2id = {\n",
        "#     \"SUPPORTS\": 0,\n",
        "#     \"REFUTES\": 1,\n",
        "#     \"NOT_ENOUGH_INFO\": 2,\n",
        "#     \"DISPUTED\": 3\n",
        "# }\n",
        "\n",
        "# class ClaimEvidenceDataset(Dataset):\n",
        "#     def __init__(self, claims, evidence_dict, tokenizer, max_length=512):\n",
        "#         self.encodings = []\n",
        "#         self.labels = []\n",
        "#         for claim_data in claims.values():\n",
        "#             claim_text = claim_data[\"claim_text\"]\n",
        "#             label_str = claim_data[\"claim_label\"]\n",
        "#             for eid in claim_data.get(\"evidences\", []):\n",
        "#                 if eid in evidence_dict:\n",
        "#                     evidence_text = evidence_dict[eid]\n",
        "#                     encoded = tokenizer(\n",
        "#                         claim_text,\n",
        "#                         evidence_text,\n",
        "#                         padding=\"max_length\",\n",
        "#                         truncation=True,\n",
        "#                         max_length=max_length,\n",
        "#                         return_tensors=\"pt\"\n",
        "#                     )\n",
        "#                     self.encodings.append({k: v.squeeze() for k, v in encoded.items()})\n",
        "#                     self.labels.append(label2id[label_str])\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         item = self.encodings[idx]\n",
        "#         item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "#         return item\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.labels)\n",
        "\n",
        "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "# PAD, MASK = tokenizer.pad_token_id, tokenizer.mask_token_id\n",
        "\n",
        "# label2id = {\"SUPPORTS\": 0, \"REFUTES\": 1, \"NOT_ENOUGH_INFO\": 2, \"DISPUTED\": 3}\n",
        "# id2label = {v: k for k, v in label2id.items()}\n",
        "\n",
        "\n",
        "# # Dataset class for claim and evidence\n",
        "# class ClaimEvidenceDataset(Dataset):\n",
        "#     \"\"\"\n",
        "#     - balance=True  : Balance all labels to max_count * target_ratio\n",
        "#     - augmenters    : Optional {'dropout','swap','pad','cutmix'}\n",
        "#     - aug_params    : Hyperparameters for each method\n",
        "#     \"\"\"\n",
        "#     def __init__(self,\n",
        "#                  claims: dict,\n",
        "#                  evidence_dict: dict,\n",
        "#                  tokenizer,\n",
        "#                  max_length: int = 512,\n",
        "#                  balance: bool = True,\n",
        "#                  target_ratio: float = 1.0,\n",
        "#                  augmenters=None,\n",
        "#                  aug_params=None,\n",
        "#                  seed: int = 42):\n",
        "\n",
        "#         random.seed(seed)\n",
        "#         self.tokenizer, self.max_length = tokenizer, max_length\n",
        "#         self.encodings, self.labels = [], []\n",
        "#         self.augmenters = set(augmenters or ['dropout', 'swap', 'pad', 'cutmix'])\n",
        "\n",
        "#         # Default hyperparameters\n",
        "#         _default = dict(dropout_prob=0.15,\n",
        "#                         swap_prob=0.10,\n",
        "#                         pad_prob=0.05,\n",
        "#                         cutmix_min=0.3,\n",
        "#                         cutmix_max=0.7)\n",
        "#         self.aug_params = {**_default, **(aug_params or {})}\n",
        "\n",
        "#         # --------- Original sample encoding --------- #\n",
        "#         for cdict in claims.values():\n",
        "#             claim_text = cdict[\"claim_text\"]\n",
        "#             lab = label2id[cdict[\"claim_label\"]]\n",
        "#             for eid in cdict.get(\"evidences\", []):\n",
        "#                 if eid in evidence_dict:\n",
        "#                     evi = evidence_dict[eid]\n",
        "#                     toks = tokenizer(claim_text, evi,\n",
        "#                                      truncation=True,\n",
        "#                                      padding=\"max_length\",\n",
        "#                                      max_length=max_length,\n",
        "#                                      return_tensors=\"pt\")\n",
        "#                     self.encodings.append({k: v.squeeze(0) for k, v in toks.items()})\n",
        "#                     self.labels.append(lab)\n",
        "\n",
        "#         # --------- Oversample with online augmentation --------- #\n",
        "#         if balance:\n",
        "#             self._balance_dataset(target_ratio)\n",
        "\n",
        "#     # ======= Dataset API ======= #\n",
        "#     def __len__(self):\n",
        "#         return len(self.labels)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         item = {k: v.clone() for k, v in self.encodings[idx].items()}\n",
        "#         item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "#         return item\n",
        "\n",
        "#     # ======= Balance samples by different labels ======= #\n",
        "#     def _balance_dataset(self, target_ratio: float):\n",
        "#         by_label = defaultdict(list)\n",
        "#         for i, y in enumerate(self.labels):\n",
        "#             by_label[y].append(i)\n",
        "\n",
        "#         max_count = int(max(len(v) for v in by_label.values()) * target_ratio)\n",
        "\n",
        "#         for lab, idx_list in by_label.items():\n",
        "#             need = max_count - len(idx_list)\n",
        "#             for _ in range(max(0, need)):\n",
        "#                 base_idx = random.choice(idx_list)\n",
        "#                 base_enc = self.encodings[base_idx]\n",
        "#                 aug_enc = self._augment_encoding(base_enc, lab)\n",
        "#                 self.encodings.append(aug_enc)\n",
        "#                 self.labels.append(lab)\n",
        "\n",
        "#     # ======= Augment a single sample ======= #\n",
        "#     def _augment_encoding(self, enc, lab):\n",
        "#         enc = {k: v.clone() for k, v in enc.items()} \n",
        "#         choice = random.choice(list(self.augmenters))\n",
        "#         if choice == 'dropout':\n",
        "#             self._token_dropout(enc)\n",
        "#         elif choice == 'swap':\n",
        "#             self._swap_neighbor(enc)\n",
        "#         elif choice == 'pad':\n",
        "#             self._random_pad(enc)\n",
        "#         elif choice == 'cutmix':\n",
        "#             self._cutmix(enc, lab)\n",
        "#         return enc\n",
        "\n",
        "#     # ----- 1. Random token dropout -----\n",
        "#     def _token_dropout(self, enc):\n",
        "#         ids = enc['input_ids']\n",
        "#         mask = torch.rand_like(ids.float()) < self.aug_params['dropout_prob']\n",
        "#         ids[mask & (ids != PAD)] = MASK\n",
        "#         enc['input_ids'] = ids\n",
        "\n",
        "#     # ----- 2. Swap neighbouring tokens -----\n",
        "#     def _swap_neighbor(self, enc):\n",
        "#         ids = enc['input_ids']\n",
        "#         for i in range(1, len(ids) - 1):\n",
        "#             if random.random() < self.aug_params['swap_prob'] and ids[i] not in (PAD, MASK):\n",
        "#                 ids[i], ids[i + 1] = ids[i + 1].clone(), ids[i].clone()\n",
        "#         enc['input_ids'] = ids\n",
        "\n",
        "#     # ----- 3. Random inner padding -----\n",
        "#     def _random_pad(self, enc):\n",
        "#         ids, mask = enc['input_ids'], enc['attention_mask']\n",
        "#         pad_prob = self.aug_params['pad_prob']\n",
        "#         new_ids, new_mask = [], []\n",
        "#         for tok, m in zip(ids, mask):\n",
        "#             if m.item() == 0:  \n",
        "#                 break\n",
        "#             new_ids.append(tok.item())\n",
        "#             new_mask.append(1)\n",
        "#             if random.random() < pad_prob and len(new_ids) < self.max_length - 1:\n",
        "#                 new_ids.append(PAD)\n",
        "#                 new_mask.append(0)\n",
        "#         # Truncate / pad with PAD at the end\n",
        "#         new_ids = (new_ids + [PAD] * self.max_length)[:self.max_length]\n",
        "#         new_mask = (new_mask + [0] * self.max_length)[:self.max_length]\n",
        "#         enc['input_ids'] = torch.tensor(new_ids, dtype=torch.long)\n",
        "#         enc['attention_mask'] = torch.tensor(new_mask, dtype=torch.long)\n",
        "\n",
        "#     # ----- 4. CutMix (same label) -----\n",
        "#     def _cutmix(self, enc, lab):\n",
        "#         # Randomly select another sample with the same label\n",
        "#         same_idxs = [i for i, y in enumerate(self.labels) if y == lab]\n",
        "#         other = {k: v.clone() for k, v in self.encodings[random.choice(same_idxs)].items()}\n",
        "#         lam = random.uniform(self.aug_params['cutmix_min'],\n",
        "#                              self.aug_params['cutmix_max'])\n",
        "#         cut_point = int(lam * self.max_length)\n",
        "#         # Take the current half and the other half\n",
        "#         enc['input_ids'][cut_point:] = other['input_ids'][cut_point:]\n",
        "#         enc['attention_mask'][cut_point:] = other['attention_mask'][cut_point:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.4.2 Data Processing for RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # ───────────────────────────────────────────────────────────────────────────────\n",
        "# # 1) Preprocessing utils\n",
        "# # ───────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "# stopwords = set(nltk_stopwords.words('english'))\n",
        "# lemmatizer = WordNetLemmatizer()\n",
        "# stemmer = SnowballStemmer('english')\n",
        "\n",
        "# MAX_LEN = 50\n",
        "# BATCH_SIZE = 64\n",
        "\n",
        "# def lemmatize(word):\n",
        "#     lemma = lemmatizer.lemmatize(word, 'v')\n",
        "#     return lemmatizer.lemmatize(lemma, 'n')\n",
        "\n",
        "# def preprocess(text, remove_stopwords=True, lemma=True, stem=False):\n",
        "#     tokens = nltk.word_tokenize(text.lower())\n",
        "#     tokens = [t for t in tokens if re.match('^[a-zA-Z0-9-]+$', t)]\n",
        "#     if remove_stopwords:\n",
        "#         tokens = [t for t in tokens if t not in stopwords]\n",
        "#     if lemma:\n",
        "#         tokens = [lemmatize(t) for t in tokens]\n",
        "#     if stem:\n",
        "#         tokens = [stemmer.stem(t) for t in tokens]\n",
        "#     return ' '.join(tokens)\n",
        "\n",
        "# # ───────────────────────────────────────────────────────────────────────────────\n",
        "# # 2) Load JSON data → DataFrame\n",
        "# # ───────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "# def load_data(claims_file, evidence_file):\n",
        "#     with open(claims_file, 'r', encoding='utf-8') as f:\n",
        "#         claims_data = json.load(f)\n",
        "#     with open(evidence_file, 'r', encoding='utf-8') as f:\n",
        "#         evid_data = json.load(f)\n",
        "    \n",
        "#     claim_texts, evid_texts, labels = [], [], []\n",
        "#     for cid, cdata in claims_data.items():\n",
        "#         claim = preprocess(cdata['claim_text'])\n",
        "#         evid_ids = cdata['evidences']\n",
        "#         evids = ' '.join([evid_data.get(eid, '') for eid in evid_ids])\n",
        "#         evid = preprocess(evids)\n",
        "#         claim_texts.append(claim)\n",
        "#         evid_texts.append(evid)\n",
        "#         labels.append(cdata['claim_label'])\n",
        "    \n",
        "#     df = pd.DataFrame({\n",
        "#         'claim': claim_texts,\n",
        "#         'evidence': evid_texts,\n",
        "#         'label': labels\n",
        "#     })\n",
        "#     return df\n",
        "\n",
        "# train_df = load_data(train_claims_path, evidence_path)\n",
        "# dev_df = load_data(dev_claims_path, evidence_path)\n",
        "\n",
        "# # ───────────────────────────────────────────────────────────────────────────────\n",
        "# # 3) Prepare vocab & sequences\n",
        "# # ───────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "# all_text = train_df['claim'].tolist() + train_df['evidence'].tolist()\n",
        "# token_counts = Counter(w for text in all_text for w in text.split())\n",
        "# vocab = {w: idx+1 for idx, (w, _) in enumerate(token_counts.items())}\n",
        "# vocab_size = len(vocab) + 1\n",
        "\n",
        "# def text_to_seq(text):\n",
        "#     seq = [vocab.get(w, 0) for w in text.split()]\n",
        "#     return seq + [0]*(MAX_LEN - len(seq)) if len(seq) < MAX_LEN else seq[:MAX_LEN]\n",
        "\n",
        "# train_claims = [text_to_seq(t) for t in train_df['claim']]\n",
        "# train_evids = [text_to_seq(t) for t in train_df['evidence']]\n",
        "# dev_claims = [text_to_seq(t) for t in dev_df['claim']]\n",
        "# dev_evids = [text_to_seq(t) for t in dev_df['evidence']]\n",
        "\n",
        "# label_enc = LabelEncoder()\n",
        "# train_labels = label_enc.fit_transform(train_df['label'])\n",
        "# dev_labels = label_enc.transform(dev_df['label'])\n",
        "\n",
        "# # ───────────────────────────────────────────────────────────────────────────────\n",
        "# # 4) Dataset + DataLoader\n",
        "# # ───────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "# class ClaimDataset(Dataset):\n",
        "#     def __init__(self, claims, evidences, labels):\n",
        "#         self.claims = torch.tensor(claims, dtype=torch.long)\n",
        "#         self.evidences = torch.tensor(evidences, dtype=torch.long)\n",
        "#         self.labels = torch.tensor(labels, dtype=torch.long)\n",
        "#     def __len__(self):\n",
        "#         return len(self.labels)\n",
        "#     def __getitem__(self, idx):\n",
        "#         return self.claims[idx], self.evidences[idx], self.labels[idx]\n",
        "\n",
        "# train_ds = ClaimDataset(train_claims, train_evids, train_labels)\n",
        "# dev_ds = ClaimDataset(dev_claims, dev_evids, dev_labels)\n",
        "# train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "# dev_dl = DataLoader(dev_ds, batch_size=BATCH_SIZE)\n",
        "\n",
        "# # ───────────────────────────────────────────────────────────────────────────────\n",
        "# # 5) Self-Attention Pooling\n",
        "# # ───────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "# class SelfAttentionPooling(nn.Module):\n",
        "#     def __init__(self, input_dim):\n",
        "#         super().__init__()\n",
        "#         self.attention = nn.Linear(input_dim, 1)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # x: [batch, seq_len, hidden_dim]\n",
        "#         weights = torch.softmax(self.attention(x), dim=1)  # [batch, seq_len, 1]\n",
        "#         pooled = torch.sum(weights * x, dim=1)  # [batch, hidden_dim]\n",
        "#         return pooled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.4.3 Data Processing for LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train_claims_augmented_path = \"data/train-claims-augmented.json\"\n",
        "\n",
        "# with open(train_claims_path, \"r\", encoding=\"utf-8\") as f:\n",
        "#     train = json.load(f)\n",
        "# with open(dev_claims_path, \"r\", encoding=\"utf-8\") as f:\n",
        "#     dev = json.load(f)\n",
        "\n",
        "# def get_synonyms(word):\n",
        "#     syns = set()\n",
        "#     for syn in wordnet.synsets(word):\n",
        "#         for lemma in syn.lemmas():\n",
        "#             name = lemma.name().replace(\"_\", \" \")\n",
        "#             if name.lower() != word.lower():\n",
        "#                 syns.add(name)\n",
        "#     return list(syns)\n",
        "\n",
        "# def synonym_replacement(words, n):\n",
        "#     new_words = words.copy()\n",
        "#     candidates = [i for i,w in enumerate(words) if get_synonyms(w)]\n",
        "#     random.shuffle(candidates)\n",
        "#     for i in candidates[:n]:\n",
        "#         new_words[i] = random.choice(get_synonyms(words[i]))\n",
        "#     return new_words\n",
        "\n",
        "# def random_insertion(words, n):\n",
        "#     new_words = words.copy()\n",
        "#     for _ in range(n):\n",
        "#         candidates = [w for w in words if get_synonyms(w)]\n",
        "#         if not candidates:\n",
        "#             break\n",
        "#         word = random.choice(candidates)\n",
        "#         new_words.insert(random.randrange(len(new_words)), random.choice(get_synonyms(word)))\n",
        "#     return new_words\n",
        "\n",
        "# def random_swap(words, n):\n",
        "#     new_words = words.copy()\n",
        "#     length = len(new_words)\n",
        "#     for _ in range(n):\n",
        "#         i, j = random.sample(range(length), 2)\n",
        "#         new_words[i], new_words[j] = new_words[j], new_words[i]\n",
        "#     return new_words\n",
        "\n",
        "# def random_deletion(words, p):\n",
        "#     if len(words) == 1:\n",
        "#         return words\n",
        "#     return [w for w in words if random.random() > p] or [random.choice(words)]\n",
        "\n",
        "# def eda(sentence, alpha_sr=0.1, alpha_ri=0.1, alpha_rs=0.1, p_rd=0.1, num_aug=4):\n",
        "#     words = nltk.word_tokenize(sentence)\n",
        "#     num_words = len(words)\n",
        "#     n_sr = max(1, int(alpha_sr * num_words))\n",
        "#     n_ri = max(1, int(alpha_ri * num_words))\n",
        "#     n_rs = max(1, int(alpha_rs * num_words))\n",
        "#     out = []\n",
        "#     out.append(\" \".join(synonym_replacement(words, n_sr)))\n",
        "#     out.append(\" \".join(random_insertion(words, n_ri)))\n",
        "#     out.append(\" \".join(random_swap(words, n_rs)))\n",
        "#     out.append(\" \".join(random_deletion(words, p_rd)))\n",
        "#     return out[:num_aug]\n",
        "\n",
        "# counts = Counter(obj[\"claim_label\"] for obj in train.values())\n",
        "# max_count = counts.most_common(1)[0][1]\n",
        "\n",
        "# print(\"Before augmentation:\", counts)\n",
        "\n",
        "# augmented = {}\n",
        "# augmented.update(train)\n",
        "\n",
        "# ids_by_label = {}\n",
        "# for cid, obj in train.items():\n",
        "#     lbl = obj[\"claim_label\"]\n",
        "#     ids_by_label.setdefault(lbl, []).append(cid)\n",
        "\n",
        "# for lbl, id_list in ids_by_label.items():\n",
        "#     n_needed = max_count - len(id_list)\n",
        "#     if n_needed <= 0:\n",
        "#         continue\n",
        "#     for i in range(n_needed):\n",
        "#         orig_id = random.choice(id_list)\n",
        "#         orig = train[orig_id]\n",
        "#         aug_text = eda(orig[\"claim_text\"])[i % 4]\n",
        "#         new_id = f\"{orig_id}_aug{i}\"\n",
        "#         augmented[new_id] = {\n",
        "#             \"claim_text\": aug_text,\n",
        "#             \"claim_label\": lbl,\n",
        "#             \"evidences\": orig[\"evidences\"],\n",
        "#         }\n",
        "\n",
        "# new_counts = Counter(obj[\"claim_label\"] for obj in augmented.values())\n",
        "# print(\"After augmentation :\", new_counts)\n",
        "\n",
        "# with open(train_claims_augmented_path, \"w\", encoding=\"utf-8\") as f:\n",
        "#     json.dump(augmented, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# print(f\"Saved augmented training set ({len(augmented)} examples) to {train_claims_augmented_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FA2ao2l8hOg"
      },
      "source": [
        "# 2. Model Implementation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1 Task 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1.1 Train MiniLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total training pairs: 4122\n",
            "Missing evidence ids: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Iteration: 100%|██████████| 129/129 [01:06<00:00,  1.93it/s]\n",
            "Iteration: 100%|██████████| 129/129 [00:43<00:00,  2.98it/s]\n",
            "Iteration: 100%|██████████| 129/129 [00:39<00:00,  3.27it/s]\n",
            "Iteration: 100%|██████████| 129/129 [00:41<00:00,  3.13it/s]\n",
            "Iteration: 100%|██████████| 129/129 [00:42<00:00,  3.05it/s]\n",
            "Epoch: 100%|██████████| 5/5 [03:53<00:00, 46.64s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finetuned model saved.\n"
          ]
        }
      ],
      "source": [
        "# ✅\n",
        "with open(train_claims_path, 'r') as f:\n",
        "    train_claims = json.load(f)\n",
        "with open(evidence_path, 'r') as f:\n",
        "    evidence_dict = json.load(f)\n",
        "\n",
        "\n",
        "# Construct the training sample list (claim, evidence_text) -> label defaults to 1.0\n",
        "train_samples = []\n",
        "missed = 0\n",
        "\n",
        "for claim in train_claims.values():\n",
        "    claim_text = claim['claim_text']\n",
        "    evidence_ids = claim.get('evidences', [])\n",
        "    for eid in evidence_ids:\n",
        "        if eid in evidence_dict:\n",
        "            ev_text = evidence_dict[eid]\n",
        "            train_samples.append(InputExample(texts=[claim_text, ev_text], label=1.0))\n",
        "        else:\n",
        "            missed += 1\n",
        "\n",
        "\n",
        "print(f\"Total training pairs: {len(train_samples)}\")\n",
        "print(f\"Missing evidence ids: {missed}\")\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "# Build the DataLoader\n",
        "train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=32)\n",
        "\n",
        "# Define the loss function\n",
        "train_loss = losses.MultipleNegativesRankingLoss(model)\n",
        "\n",
        "# Start training\n",
        "model.fit(\n",
        "    train_objectives=[(train_dataloader, train_loss)],\n",
        "    epochs=5,\n",
        "    warmup_steps=100,\n",
        "    show_progress_bar=True\n",
        ")\n",
        "\n",
        "# Save the model\n",
        "model.save('./model/my_finetuned_minilm_retriever')\n",
        "print(\"Finetuned model saved.\")\n",
        "\n",
        "print(\"\"\"\n",
        "Total training pairs: 4122\n",
        "Missing evidence ids: 0\n",
        "Iteration: 100%|██████████| 129/129 [01:06<00:00,  1.93it/s]\n",
        "Iteration: 100%|██████████| 129/129 [00:43<00:00,  2.98it/s]\n",
        "Iteration: 100%|██████████| 129/129 [00:39<00:00,  3.27it/s]\n",
        "Iteration: 100%|██████████| 129/129 [00:41<00:00,  3.13it/s]\n",
        "Iteration: 100%|██████████| 129/129 [00:42<00:00,  3.05it/s]\n",
        "Epoch: 100%|██████████| 5/5 [03:53<00:00, 46.64s/it]\n",
        "Finetuned model saved.\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1.2 Train Msmarco Reranker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total training samples: 8244\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Iteration: 100%|██████████| 516/516 [01:09<00:00,  7.41it/s]\n",
            "Iteration: 100%|██████████| 516/516 [01:05<00:00,  7.89it/s]\n",
            "Iteration: 100%|██████████| 516/516 [01:12<00:00,  7.11it/s]\n",
            "Iteration: 100%|██████████| 516/516 [01:11<00:00,  7.27it/s]\n",
            "Iteration: 100%|██████████| 516/516 [01:11<00:00,  7.20it/s]\n",
            "Epoch: 100%|██████████| 5/5 [05:50<00:00, 70.05s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finetuned model saved.\n"
          ]
        }
      ],
      "source": [
        "# ✅\n",
        "with open(train_claims_path, 'r') as f:\n",
        "    train_claims = json.load(f)\n",
        "with open(evidence_path, 'r') as f:\n",
        "    evidence_dict = json.load(f)\n",
        "\n",
        "\n",
        "# Construct positive and negative samples\n",
        "train_samples = []\n",
        "\n",
        "def generate_samples(claims_data):\n",
        "    samples = []\n",
        "    for claim in claims_data.values():\n",
        "        claim_text = claim[\"claim_text\"]\n",
        "        evidence_ids = claim.get(\"evidences\", [])\n",
        "        pos_evidence_texts = [evidence_dict[eid] for eid in evidence_ids if eid in evidence_dict]\n",
        "\n",
        "        # Positive samples\n",
        "        for ev in pos_evidence_texts:\n",
        "            samples.append(InputExample(texts=[claim_text, ev], label=1.0))\n",
        "\n",
        "        # Negative samples\n",
        "        neg_pool = [e for eid, e in evidence_dict.items() if eid not in evidence_ids]\n",
        "        for _ in range(len(pos_evidence_texts)):\n",
        "            neg_ev = random.choice(neg_pool)\n",
        "            samples.append(InputExample(texts=[claim_text, neg_ev], label=0.0))\n",
        "\n",
        "    return samples\n",
        "\n",
        "train_samples.extend(generate_samples(train_claims))\n",
        "\n",
        "print(f\"Total training samples: {len(train_samples)}\")\n",
        "\n",
        "# Build the DataLoader (InputExample is a valid format)\n",
        "train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=16)\n",
        "\n",
        "# Load MS MARCO CrossEncoder\n",
        "model = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\", num_labels=1)\n",
        "\n",
        "# Train the model\n",
        "model.fit(\n",
        "    train_dataloader=train_dataloader,\n",
        "    epochs=5,\n",
        "    warmup_steps=100,\n",
        "    show_progress_bar=True\n",
        ")\n",
        "\n",
        "# Save the model\n",
        "model.save('./model/my_finetuned_msmarco_reranker')\n",
        "print(\"Finetuned model saved.\")\n",
        "\n",
        "print(\"\"\"\n",
        "Total training samples: 8244\n",
        "Iteration: 100%|██████████| 516/516 [01:09<00:00,  7.41it/s]\n",
        "Iteration: 100%|██████████| 516/516 [01:05<00:00,  7.89it/s]\n",
        "Iteration: 100%|██████████| 516/516 [01:12<00:00,  7.11it/s]\n",
        "Iteration: 100%|██████████| 516/516 [01:11<00:00,  7.27it/s]\n",
        "Iteration: 100%|██████████| 516/516 [01:11<00:00,  7.20it/s]\n",
        "Epoch: 100%|██████████| 5/5 [05:50<00:00, 70.05s/it]\n",
        "Finetuned model saved.\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1.3 Load Finetuned models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ✅\n",
        "model = SentenceTransformer('./model/my_finetuned_minilm_retriever')\n",
        "reranker =  CrossEncoder('./model/my_finetuned_msmarco_reranker')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1.4 Encode Evidence Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Batches: 100%|██████████| 12133/12133 [05:59<00:00, 33.76it/s]\n"
          ]
        }
      ],
      "source": [
        "# ✅\n",
        "# Encode the cleaned sentences using Sentence-BERT\n",
        "evidence_embeddings = model.encode(\n",
        "    cleaned_evidence_texts,\n",
        "    convert_to_numpy=True,\n",
        "    normalize_embeddings= True,\n",
        "    show_progress_bar=True\n",
        ")\n",
        "\n",
        "os.makedirs(\"./word_embedding\", exist_ok=True)\n",
        "# Save the encoded embeddings (embeddings)\n",
        "np.save(word_embedding_path, evidence_embeddings)\n",
        "\n",
        "# Save the evidence_ids and texts after splitting and the corresponding original evidence_id mapping\n",
        "with open(word_embedding_meta_path, \"wb\") as f:\n",
        "    pickle.dump((cleaned_evidence_ids, cleaned_evidence_texts, original_evidence_ids), f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 Task 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2.1 BERT Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model_path = \"./model/my_bert_classifier\"\n",
        "\n",
        "# # load data\n",
        "# with open('./data/train-claims.json', 'r') as f:\n",
        "#     train_claims = json.load(f)\n",
        "# with open('./data/evidence.json', 'r') as f:\n",
        "#     evidence_dict = json.load(f)\n",
        "\n",
        "\n",
        "# # 3. Construct the dataset\n",
        "# train_dataset = train_dataset = ClaimEvidenceDataset(\n",
        "#     claims=train_claims,\n",
        "#     evidence_dict=evidence_dict,\n",
        "#     tokenizer=tokenizer,\n",
        "#     max_length=512,\n",
        "#     balance=True,             \n",
        "#     target_ratio=1.0,\n",
        "# )\n",
        "# # Load BERT model\n",
        "# model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=4)\n",
        "\n",
        "# # Set training parameters\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir=\"./results\",\n",
        "#     per_device_train_batch_size=16,\n",
        "#     num_train_epochs=3,\n",
        "#     eval_strategy=\"no\",\n",
        "#     save_strategy=\"no\",\n",
        "#     logging_strategy=\"no\",   \n",
        "#     logging_steps=50,\n",
        "# )\n",
        "\n",
        "# # Initialize Trainer\n",
        "# trainer = Trainer(\n",
        "#     model=model,\n",
        "#     args=training_args,\n",
        "#     train_dataset=train_dataset,\n",
        "#     tokenizer=tokenizer\n",
        "# )\n",
        "\n",
        "# # Start training\n",
        "# trainer.train()\n",
        "\n",
        "# # Save model\n",
        "\n",
        "# model.save_pretrained(model_path)\n",
        "# tokenizer.save_pretrained(model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2.2 Train RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # ───────────────────────────────────────────────────────────────────────────────\n",
        "# # 6) Model\n",
        "# # ───────────────────────────────────────────────────────────────────────────────\n",
        "# EMBED_DIM = 100\n",
        "# HIDDEN_DIM = 64\n",
        "# NUM_CLASSES = 4\n",
        "# DROPOUT_PROB = 0.4\n",
        "# EPOCHS = 5\n",
        "# LR = 1e-4\n",
        "# DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "# class RNNModel(nn.Module):\n",
        "#     def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):\n",
        "#         super().__init__()\n",
        "#         self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "#         self.embed_dropout = nn.Dropout(DROPOUT_PROB)\n",
        "#         self.rnn_claim = nn.RNN(embed_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
        "#         self.rnn_evid = nn.RNN(embed_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
        "#         self.rnn_dropout = nn.Dropout(DROPOUT_PROB)\n",
        "#         self.attention_claim = SelfAttentionPooling(hidden_dim * 2)\n",
        "#         self.attention_evid = SelfAttentionPooling(hidden_dim * 2)\n",
        "#         self.classifier = nn.Sequential(\n",
        "#             nn.Linear(hidden_dim * 4, 128),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Dropout(DROPOUT_PROB),\n",
        "#             nn.Linear(128, num_classes)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, claim, evidence):\n",
        "#         claim_emb = self.embed_dropout(self.embedding(claim))\n",
        "#         evid_emb = self.embed_dropout(self.embedding(evidence))\n",
        "        \n",
        "#         claim_out, _ = self.rnn_claim(claim_emb)\n",
        "#         evid_out, _ = self.rnn_evid(evid_emb)\n",
        "        \n",
        "#         claim_out = self.rnn_dropout(claim_out)\n",
        "#         evid_out = self.rnn_dropout(evid_out)\n",
        "        \n",
        "#         claim_pool = self.attention_claim(claim_out)\n",
        "#         evid_pool = self.attention_evid(evid_out)\n",
        "        \n",
        "#         combined = torch.cat([claim_pool, evid_pool], dim=1)\n",
        "#         return self.classifier(combined)\n",
        "\n",
        "# model = RNNModel(vocab_size, EMBED_DIM, HIDDEN_DIM, NUM_CLASSES).to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Weighted Cross Entropy Loss\n",
        "# label2idx = {\n",
        "#     \"SUPPORTS\": 0,\n",
        "#     \"REFUTES\": 1,\n",
        "#     \"NOT_ENOUGH_INFO\": 2,\n",
        "#     \"DISPUTED\": 3\n",
        "# }\n",
        "\n",
        "# with open(train_claims_path, 'r', encoding='utf-8') as f:\n",
        "#     train_claim = json.load(f)\n",
        "\n",
        "# label_counts = Counter([label2idx[obj[\"claim_label\"]] for obj in train_claim.values()])\n",
        "# total = sum(label_counts.values())\n",
        "\n",
        "# # 权重越大越重要，可以用 total / class_count 作为反比权重\n",
        "# class_weights = [total / label_counts[i] for i in range(len(label2idx))]\n",
        "\n",
        "# # 转成 tensor 并放到 DEVICE 上\n",
        "# weights = torch.tensor(class_weights, dtype=torch.float).to(DEVICE)\n",
        "\n",
        "# # 创建 loss 函数\n",
        "# criterion = nn.CrossEntropyLoss(weight=weights)\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "# # ───────────────────────────────────────────────────────────────────────────────\n",
        "# # 7) Training loop\n",
        "# # ───────────────────────────────────────────────────────────────────────────────\n",
        "# best_acc = 0.0\n",
        "# for epoch in range(1, EPOCHS + 1):\n",
        "#     model.train()\n",
        "#     total_loss, total_correct = 0, 0\n",
        "#     for claim, evid, label in tqdm(train_dl, desc=f\"Epoch {epoch}\"):\n",
        "#         claim, evid, label = claim.to(DEVICE), evid.to(DEVICE), label.to(DEVICE)\n",
        "#         optimizer.zero_grad()\n",
        "#         out = model(claim, evid)\n",
        "#         loss = criterion(out, label)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         total_loss += loss.item()\n",
        "#         total_correct += (out.argmax(1) == label).sum().item()\n",
        "#     acc = total_correct / len(train_ds)\n",
        "#     print(f\"Train Loss: {total_loss/len(train_dl):.4f}, Train Acc: {acc:.4f}\")\n",
        "    \n",
        "#     model.eval()\n",
        "#     val_loss, val_correct = 0, 0\n",
        "#     with torch.no_grad():\n",
        "#         for claim, evid, label in dev_dl:\n",
        "#             claim, evid, label = claim.to(DEVICE), evid.to(DEVICE), label.to(DEVICE)\n",
        "#             out = model(claim, evid)\n",
        "#             loss = criterion(out, label)\n",
        "#             val_loss += loss.item()\n",
        "#             val_correct += (out.argmax(1) == label).sum().item()\n",
        "#     val_acc = val_correct / len(dev_ds)\n",
        "#     print(f\"Val Loss: {val_loss/len(dev_dl):.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "#     if val_acc > best_acc:\n",
        "#         best_acc = val_acc\n",
        "#         torch.save(model.state_dict(), \"rnn_model.pth\")\n",
        "#         print(f\"✅ New best model saved (epoch {epoch}, acc {val_acc:.4%})\\n\")\n",
        "#     else:\n",
        "#         print()\n",
        "\n",
        "# # ───────────────────────────────────────────────────────────────────────────────\n",
        "# # 8) Save model + label encoder\n",
        "# # ───────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "\n",
        "# import pickle\n",
        "# with open('label_encoder.pkl', 'wb') as f:\n",
        "#     pickle.dump(label_enc, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2.3 Train LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# BERT_MODEL     = \"bert-base-uncased\"\n",
        "# MAX_LEN        = 256\n",
        "# LSTM_HID_DIM   = 512\n",
        "# NUM_CLASSES    = 4\n",
        "# DROPOUT_PROB   = 0.2\n",
        "# NUM_LAYERS     = 3\n",
        "# BATCH_SIZE     = 16\n",
        "# EPOCHS         = 5\n",
        "# LR             = 2e-4\n",
        "\n",
        "# DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# # Label ↔ index map\n",
        "# label2idx = {\n",
        "#     \"SUPPORTS\":         0,\n",
        "#     \"NOT_ENOUGH_INFO\":  1,\n",
        "#     \"REFUTES\":          2,\n",
        "#     \"DISPUTED\":         3,\n",
        "# }\n",
        "\n",
        "# # ───────────────────────────────────────────────────────────────────────────────\n",
        "# # 1) Load JSON data\n",
        "# # ───────────────────────────────────────────────────────────────────────────────\n",
        "# with open(train_claims_augmented_path, \"r\", encoding=\"utf-8\") as f:\n",
        "#     train_claims = json.load(f)\n",
        "# with open(dev_claims_path, \"r\", encoding=\"utf-8\") as f:\n",
        "#     dev_claims = json.load(f)\n",
        "# with open(evidence_path, \"r\", encoding=\"utf-8\") as f:\n",
        "#     evidence_dict = json.load(f)\n",
        "\n",
        "# # ───────────────────────────────────────────────────────────────────────────────\n",
        "# # 2) Dataset + DataLoader (num_workers=0 to avoid pickling errors)\n",
        "# # ───────────────────────────────────────────────────────────────────────────────\n",
        "# tokenizer = BertTokenizer.from_pretrained(BERT_MODEL)\n",
        "\n",
        "# class ClaimEvidenceDataset(Dataset):\n",
        "#     def __init__(self, claims, evidences, tokenizer, max_len):\n",
        "#         self.items = []\n",
        "#         for cid, obj in claims.items():\n",
        "#             claim_text = obj[\"claim_text\"]\n",
        "#             ev_ids     = obj.get(\"evidences\", [])\n",
        "#             ev_texts   = [evidences[e] for e in ev_ids if e in evidences]\n",
        "#             # full sequence: claim [SEP] evidence1 evidence2 ...\n",
        "#             full_input = claim_text + \" [SEP] \" + \" \".join(ev_texts)\n",
        "#             label = label2idx[obj[\"claim_label\"]]\n",
        "#             self.items.append((full_input, label))\n",
        "#         self.tokenizer = tokenizer\n",
        "#         self.max_len   = max_len\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.items)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         text, label = self.items[idx]\n",
        "#         enc = self.tokenizer(\n",
        "#             text,\n",
        "#             truncation=True,\n",
        "#             padding=\"max_length\",\n",
        "#             max_length=self.max_len,\n",
        "#             return_tensors=\"pt\"\n",
        "#         )\n",
        "#         return (\n",
        "#             enc[\"input_ids\"].squeeze(0),\n",
        "#             enc[\"attention_mask\"].squeeze(0),\n",
        "#             torch.tensor(label, dtype=torch.long),\n",
        "#         )\n",
        "\n",
        "# def collate_batch(batch):\n",
        "#     ids, masks, labs = zip(*batch)\n",
        "#     return torch.stack(ids), torch.stack(masks), torch.stack(labs)\n",
        "\n",
        "# # create datasets and loaders\n",
        "# train_ds = ClaimEvidenceDataset(train_claims, evidence_dict, tokenizer, MAX_LEN)\n",
        "# dev_ds   = ClaimEvidenceDataset(dev_claims,   evidence_dict, tokenizer, MAX_LEN)\n",
        "\n",
        "# train_dl = DataLoader(\n",
        "#     train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
        "#     collate_fn=collate_batch, num_workers=0, pin_memory=True\n",
        "# )\n",
        "# dev_dl   = DataLoader(\n",
        "#     dev_ds,   batch_size=BATCH_SIZE, shuffle=False,\n",
        "#     collate_fn=collate_batch, num_workers=0, pin_memory=True\n",
        "# )\n",
        "\n",
        "# # ───────────────────────────────────────────────────────────────────────────────\n",
        "# # 3) Model Definition\n",
        "# # ───────────────────────────────────────────────────────────────────────────────\n",
        "# class BiLSTMWithBertEncoder(nn.Module):\n",
        "#     def __init__(self, bert_name, lstm_hid, num_classes, \n",
        "#                  dropout_prob, lstm_layers):\n",
        "#         super().__init__()\n",
        "#         # 1) Frozen BERT\n",
        "#         self.bert = BertModel.from_pretrained(bert_name)\n",
        "#         for p in self.bert.parameters():\n",
        "#             p.requires_grad = False\n",
        "\n",
        "#         bert_dim = self.bert.config.hidden_size\n",
        "\n",
        "#         # 2) Dropout on BERT outputs\n",
        "#         self.dropout_bert = nn.Dropout(dropout_prob)\n",
        "\n",
        "#         # 3) 2-layer BiLSTM with inter-layer dropout\n",
        "#         self.lstm = nn.LSTM(\n",
        "#             input_size    = bert_dim,\n",
        "#             hidden_size   = lstm_hid,\n",
        "#             num_layers    = lstm_layers,\n",
        "#             batch_first   = True,\n",
        "#             bidirectional = True,\n",
        "#             dropout       = dropout_prob  # only applied between layers\n",
        "#         )\n",
        "\n",
        "#         # 4) Attention scoring layer\n",
        "#         self.attn_fc = nn.Linear(2 * lstm_hid, 1)\n",
        "\n",
        "#         # 5) Dropout before classifier\n",
        "#         self.dropout_pool = nn.Dropout(dropout_prob)\n",
        "\n",
        "#         # 6) Final classification head\n",
        "#         self.classifier = nn.Linear(2 * lstm_hid, num_classes)\n",
        "\n",
        "#     def forward(self, input_ids, attention_mask):\n",
        "#         # a) BERT encoding (frozen)\n",
        "#         bert_out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "#         seq_emb  = bert_out.last_hidden_state            # (B, L, D)\n",
        "#         seq_emb  = self.dropout_bert(seq_emb)\n",
        "\n",
        "#         # b) BiLSTM\n",
        "#         lstm_out, _ = self.lstm(seq_emb)                 # (B, L, 2H)\n",
        "\n",
        "#         # c) Self-attention pooling\n",
        "#         scores = self.attn_fc(lstm_out).squeeze(-1)      # (B, L)\n",
        "#         scores = scores.masked_fill(attention_mask == 0, -1e9)\n",
        "#         alphas = torch.softmax(scores, dim=1)            # (B, L)\n",
        "#         pooled = torch.sum(lstm_out * alphas.unsqueeze(-1), dim=1)  # (B, 2H)\n",
        "\n",
        "#         # d) Dropout + classification\n",
        "#         pooled = self.dropout_pool(pooled)\n",
        "#         logits = self.classifier(pooled)                 # (B, num_classes)\n",
        "#         return logits\n",
        "    \n",
        "# model     = BiLSTMWithBertEncoder(BERT_MODEL, LSTM_HID_DIM, NUM_CLASSES, DROPOUT_PROB, NUM_LAYERS)\n",
        "# model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # ───────────────────────────────────────────────────────────────────────────────\n",
        "# # 4) Training Loop (with best-model saving)\n",
        "# # ───────────────────────────────────────────────────────────────────────────────\n",
        "# optimizer = torch.optim.Adam(model.classifier.parameters(), lr=LR)\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# best_acc = 0.0\n",
        "# BEST_MODEL_PATH = \"task2_best_lstm.pt\"\n",
        "\n",
        "# for epoch in range(1, EPOCHS + 1):\n",
        "#     # -- train --\n",
        "#     model.train()\n",
        "#     total_loss = 0.0\n",
        "#     for input_ids, attn_mask, labels in tqdm(train_dl, desc=f\"Train Epoch {epoch}\"):\n",
        "#         input_ids = input_ids.to(DEVICE)\n",
        "#         attn_mask = attn_mask.to(DEVICE)\n",
        "#         labels    = labels.to(DEVICE)\n",
        "\n",
        "#         logits = model(input_ids, attn_mask)\n",
        "#         loss   = criterion(logits, labels)\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         total_loss += loss.item()\n",
        "\n",
        "#     avg_loss = total_loss / len(train_dl)\n",
        "#     print(f\"→ Epoch {epoch} Avg Loss: {avg_loss:.4f}\")\n",
        "\n",
        "#     # -- eval on dev --\n",
        "#     model.eval()\n",
        "#     correct = 0\n",
        "#     total   = 0\n",
        "#     with torch.no_grad():\n",
        "#         for input_ids, attn_mask, labels in tqdm(dev_dl, desc=\" Eval\"):\n",
        "#             input_ids = input_ids.to(DEVICE)\n",
        "#             attn_mask = attn_mask.to(DEVICE)\n",
        "#             labels    = labels.to(DEVICE)\n",
        "\n",
        "#             preds = model(input_ids, attn_mask).argmax(dim=1)\n",
        "#             correct += (preds == labels).sum().item()\n",
        "#             total   += labels.size(0)\n",
        "\n",
        "#     acc = correct / total\n",
        "#     print(f\"→ Dev Accuracy: {acc:.4%}\")\n",
        "\n",
        "#     # -- save best model --\n",
        "#     if acc > best_acc:\n",
        "#         best_acc = acc\n",
        "#         torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
        "#         print(f\"✅ New best model saved (epoch {epoch}, acc {acc:.4%})\\n\")\n",
        "#     else:\n",
        "#         print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2.4 Ensembled Model Using Soft Vote"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzGuzHPE87Ya"
      },
      "source": [
        "# 3.Testing and Evaluation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Predict Task 1 on Dev Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZVeNYIH9IaL"
      },
      "outputs": [],
      "source": [
        "# ✅\n",
        "# Load the evidence embeddings\n",
        "word_embedding_path = './word_embedding/evidence_embeddings.npy'\n",
        "word_embedding_meta_path = \"./word_embedding/evidence_meta.pkl\"\n",
        "\n",
        "# Load numpy embeddings\n",
        "evidence_embeddings = np.load(word_embedding_path)\n",
        "\n",
        "# Load evidence_ids, evidence_texts, and original_evidence_ids\n",
        "with open(word_embedding_meta_path, \"rb\") as f:\n",
        "    evidence_ids, evidence_texts, original_evidence_ids = pickle.load(f)\n",
        "\n",
        "arr = np.array(evidence_embeddings, dtype='float32', order='C')\n",
        "\n",
        "dimension = evidence_embeddings.shape[1]\n",
        "index = faiss.IndexFlatIP(dimension)\n",
        "index.add(np.array(evidence_embeddings, dtype='float32', order='C'))\n",
        "\n",
        "\n",
        "def clean_claim(claim: str) -> str:\n",
        "    # Lowercase\n",
        "    claim = claim.lower()\n",
        "    # Remove punctuation\n",
        "    claim = re.sub(r'[^a-z0-9\\s]', '', claim)\n",
        "    # Remove extra spaces\n",
        "    claim = re.sub(r'\\s+', ' ', claim).strip()\n",
        "    return claim\n",
        "\n",
        "# Create mapping\n",
        "evidence_dict = dict(zip(evidence_ids, evidence_texts))\n",
        "\n",
        "with open(evidence_path, 'r') as f:\n",
        "    original_evidence_dict = json.load(f)\n",
        "\n",
        "def retrieve_evidence(claim_id, claim_data, retrieval=100, top_k=5):\n",
        "    claim_text = claim_data[\"claim_text\"]\n",
        "    cleaned_claim = clean_claim(claim_text)\n",
        "\n",
        "    # Step 1: Coarse retrieval (model + FAISS)\n",
        "    claim_embedding = model.encode([cleaned_claim], convert_to_numpy=True, normalize_embeddings=True)\n",
        "    scores, indices = index.search(claim_embedding, retrieval * 3)\n",
        "\n",
        "    # Step 2: Remove duplicate candidates\n",
        "    seen_original_ids = set()\n",
        "    candidates = []\n",
        "    for i in indices[0]:\n",
        "        eid = evidence_ids[i]\n",
        "        text = evidence_dict[eid]\n",
        "        original_id = original_evidence_ids[i]\n",
        "\n",
        "        if original_id not in seen_original_ids:\n",
        "            candidates.append((original_id, eid, text))\n",
        "            seen_original_ids.add(original_id)\n",
        "\n",
        "        if len(candidates) >= retrieval:\n",
        "            break\n",
        "\n",
        "    # Step 3: Reranking (CrossEncoder)\n",
        "    pairs = [(claim_text, original_evidence_dict[orig_id]) for (orig_id, _, _) in candidates]  \n",
        "    similarity_scores = reranker.predict(pairs)\n",
        "\n",
        "    reranked = sorted(zip(candidates, similarity_scores), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Only return the original document-level evidence ID\n",
        "    top_k_original_ids = [orig_id for (orig_id, _, _), _ in reranked[:top_k]]\n",
        "\n",
        "    # Construct the final result dict\n",
        "    result = {\n",
        "        \"claim_text\": claim_text,\n",
        "        \"evidences\": top_k_original_ids\n",
        "    }\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating R=100, K=3: 100%|██████████| 154/154 [00:21<00:00,  7.14it/s]\n",
            "Retrieval=100, Top-K=3\n",
            "   - Avg Recall   : 22.71%\n",
            "   - Avg Precision: 20.35%\n",
            "   - Avg F1       : 19.85%\n",
            "Evaluating R=100, K=4: 100%|██████████| 154/154 [00:21<00:00,  7.21it/s]\n",
            "Retrieval=100, Top-K=4\n",
            "   - Avg Recall   : 26.36%\n",
            "   - Avg Precision: 18.02%\n",
            "   - Avg F1       : 19.90%\n",
            "Evaluating R=100, K=5: 100%|██████████| 154/154 [00:23<00:00,  6.63it/s]\n",
            "Retrieval=100, Top-K=5\n",
            "   - Avg Recall   : 27.86%\n",
            "   - Avg Precision: 15.45%\n",
            "   - Avg F1       : 18.59%\n",
            "Evaluating R=200, K=3: 100%|██████████| 154/154 [00:38<00:00,  4.03it/s]\n",
            "Retrieval=200, Top-K=3\n",
            "   - Avg Recall   : 22.25%\n",
            "   - Avg Precision: 19.70%\n",
            "   - Avg F1       : 19.32%\n",
            "Evaluating R=200, K=4: 100%|██████████| 154/154 [00:41<00:00,  3.74it/s]\n",
            "Retrieval=200, Top-K=4\n",
            "   - Avg Recall   : 25.65%\n",
            "   - Avg Precision: 17.37%\n",
            "   - Avg F1       : 19.25%\n",
            "Evaluating R=200, K=5: 100%|██████████| 154/154 [00:43<00:00,  3.56it/s]\n",
            "Retrieval=200, Top-K=5\n",
            "   - Avg Recall   : 27.47%\n",
            "   - Avg Precision: 15.06%\n",
            "   - Avg F1       : 18.20%\n",
            "Evaluating R=500, K=3: 100%|██████████| 154/154 [01:45<00:00,  1.46it/s]\n",
            "Retrieval=500, Top-K=3\n",
            "   - Avg Recall   : 20.89%\n",
            "   - Avg Precision: 18.61%\n",
            "   - Avg F1       : 18.25%\n",
            "Evaluating R=500, K=4: 100%|██████████| 154/154 [01:49<00:00,  1.41it/s]\n",
            "Retrieval=500, Top-K=4\n",
            "   - Avg Recall   : 25.31%\n",
            "   - Avg Precision: 16.88%\n",
            "   - Avg F1       : 18.84%\n",
            "Evaluating R=500, K=5: 100%|██████████| 154/154 [01:50<00:00,  1.39it/s]\n",
            "Retrieval=500, Top-K=5\n",
            "   - Avg Recall   : 26.56%\n",
            "   - Avg Precision: 14.55%\n",
            "   - Avg F1       : 17.57%\n",
            "Evaluating R=1000, K=3: 100%|██████████| 154/154 [03:40<00:00,  1.43s/it]\n",
            "Retrieval=1000, Top-K=3\n",
            "   - Avg Recall   : 20.78%\n",
            "   - Avg Precision: 18.61%\n",
            "   - Avg F1       : 18.21%\n",
            "Evaluating R=1000, K=4: 100%|██████████| 154/154 [03:41<00:00,  1.44s/it]\n",
            "Retrieval=1000, Top-K=4\n",
            "   - Avg Recall   : 25.18%\n",
            "   - Avg Precision: 16.72%\n",
            "   - Avg F1       : 18.69%\n",
            "Evaluating R=1000, K=5: 100%|██████████| 154/154 [03:39<00:00,  1.42s/it]\n",
            "Retrieval=1000, Top-K=5\n",
            "   - Avg Recall   : 26.61%\n",
            "   - Avg Precision: 14.55%\n",
            "   - Avg F1       : 17.59%\n",
            "\n",
            "Best Setting: Retrieval=100, Top-K=4, F1=19.90%\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ✅\n",
        "# Load dev claims\n",
        "with open(dev_claims_path, 'r') as f:\n",
        "    dev_claims = json.load(f)\n",
        "\n",
        "claim_ids = list(dev_claims.keys())\n",
        "\n",
        "retrieval_values = [100,200,500,1000]\n",
        "top_k_values = [3, 4, 5]\n",
        "\n",
        "best_f1 = 0\n",
        "best_setting = {}\n",
        "\n",
        "for retrieval in retrieval_values:\n",
        "    for top_k in top_k_values:\n",
        "        recalls = []\n",
        "        precisions = []\n",
        "        f1s = []\n",
        "\n",
        "        for cid in tqdm(claim_ids, desc=f\"Evaluating R={retrieval}, K={top_k}\"):\n",
        "            truth = set(dev_claims[cid][\"evidences\"])\n",
        "            \n",
        "            retrieved_info = retrieve_evidence(cid, dev_claims[cid], retrieval=retrieval, top_k=top_k)\n",
        "            retrieved = set(retrieved_info[\"evidences\"])\n",
        "\n",
        "            hit = len(truth & retrieved)\n",
        "\n",
        "            recall = hit / len(truth) if len(truth) > 0 else 0\n",
        "            precision = hit / top_k if top_k > 0 else 0\n",
        "\n",
        "            if precision + recall > 0:\n",
        "                f1 = 2 * precision * recall / (precision + recall)\n",
        "            else:\n",
        "                f1 = 0\n",
        "\n",
        "            recalls.append(recall)\n",
        "            precisions.append(precision)\n",
        "            f1s.append(f1)\n",
        "\n",
        "        avg_recall = np.mean(recalls)\n",
        "        avg_precision = np.mean(precisions)\n",
        "        avg_f1 = np.mean(f1s)\n",
        "\n",
        "        print(f\"\\nRetrieval={retrieval}, Top-K={top_k}\")\n",
        "        print(f\"   - Avg Recall   : {avg_recall:.2%}\")\n",
        "        print(f\"   - Avg Precision: {avg_precision:.2%}\")\n",
        "        print(f\"   - Avg F1       : {avg_f1:.2%}\")\n",
        "\n",
        "        if avg_f1 > best_f1:\n",
        "            best_f1 = avg_f1\n",
        "            best_setting = {'retrieval': retrieval, 'top_k': top_k}\n",
        "\n",
        "print(f\"\\nBest Setting: Retrieval={best_setting['retrieval']}, Top-K={best_setting['top_k']}, F1={best_f1:.2%}\")\n",
        "\n",
        "print(\"\"\"\n",
        "Evaluating R=100, K=3: 100%|██████████| 154/154 [00:21<00:00,  7.14it/s]\n",
        "Retrieval=100, Top-K=3\n",
        "   - Avg Recall   : 22.71%\n",
        "   - Avg Precision: 20.35%\n",
        "   - Avg F1       : 19.85%\n",
        "Evaluating R=100, K=4: 100%|██████████| 154/154 [00:21<00:00,  7.21it/s]\n",
        "Retrieval=100, Top-K=4\n",
        "   - Avg Recall   : 26.36%\n",
        "   - Avg Precision: 18.02%\n",
        "   - Avg F1       : 19.90%\n",
        "Evaluating R=100, K=5: 100%|██████████| 154/154 [00:23<00:00,  6.63it/s]\n",
        "Retrieval=100, Top-K=5\n",
        "   - Avg Recall   : 27.86%\n",
        "   - Avg Precision: 15.45%\n",
        "   - Avg F1       : 18.59%\n",
        "Evaluating R=200, K=3: 100%|██████████| 154/154 [00:38<00:00,  4.03it/s]\n",
        "Retrieval=200, Top-K=3\n",
        "   - Avg Recall   : 22.25%\n",
        "   - Avg Precision: 19.70%\n",
        "   - Avg F1       : 19.32%\n",
        "Evaluating R=200, K=4: 100%|██████████| 154/154 [00:41<00:00,  3.74it/s]\n",
        "Retrieval=200, Top-K=4\n",
        "   - Avg Recall   : 25.65%\n",
        "   - Avg Precision: 17.37%\n",
        "   - Avg F1       : 19.25%\n",
        "Evaluating R=200, K=5: 100%|██████████| 154/154 [00:43<00:00,  3.56it/s]\n",
        "Retrieval=200, Top-K=5\n",
        "   - Avg Recall   : 27.47%\n",
        "   - Avg Precision: 15.06%\n",
        "   - Avg F1       : 18.20%\n",
        "Evaluating R=500, K=3: 100%|██████████| 154/154 [01:45<00:00,  1.46it/s]\n",
        "Retrieval=500, Top-K=3\n",
        "   - Avg Recall   : 20.89%\n",
        "   - Avg Precision: 18.61%\n",
        "   - Avg F1       : 18.25%\n",
        "Evaluating R=500, K=4: 100%|██████████| 154/154 [01:49<00:00,  1.41it/s]\n",
        "Retrieval=500, Top-K=4\n",
        "   - Avg Recall   : 25.31%\n",
        "   - Avg Precision: 16.88%\n",
        "   - Avg F1       : 18.84%\n",
        "Evaluating R=500, K=5: 100%|██████████| 154/154 [01:50<00:00,  1.39it/s]\n",
        "Retrieval=500, Top-K=5\n",
        "   - Avg Recall   : 26.56%\n",
        "   - Avg Precision: 14.55%\n",
        "   - Avg F1       : 17.57%\n",
        "Evaluating R=1000, K=3: 100%|██████████| 154/154 [03:40<00:00,  1.43s/it]\n",
        "Retrieval=1000, Top-K=3\n",
        "   - Avg Recall   : 20.78%\n",
        "   - Avg Precision: 18.61%\n",
        "   - Avg F1       : 18.21%\n",
        "Evaluating R=1000, K=4: 100%|██████████| 154/154 [03:41<00:00,  1.44s/it]\n",
        "Retrieval=1000, Top-K=4\n",
        "   - Avg Recall   : 25.18%\n",
        "   - Avg Precision: 16.72%\n",
        "   - Avg F1       : 18.69%\n",
        "Evaluating R=1000, K=5: 100%|██████████| 154/154 [03:39<00:00,  1.42s/it]\n",
        "Retrieval=1000, Top-K=5\n",
        "   - Avg Recall   : 26.61%\n",
        "   - Avg Precision: 14.55%\n",
        "   - Avg F1       : 17.59%\n",
        "\n",
        "Best Setting: Retrieval=100, Top-K=4, F1=19.90%\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Predict Task 2 on Dev Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mefSOe8eTmGP"
      },
      "source": [
        "## Object Oriented Programming codes here\n",
        "\n",
        "*You can use multiple code snippets. Just add more if needed*"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "nlp_final",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
