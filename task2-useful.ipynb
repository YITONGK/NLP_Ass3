{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0991e29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\Anaconda\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import all the packages\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import Counter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import re\n",
    "from datasets import Dataset\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06da7cac",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9a1641b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "label2id = {\n",
    "    \"SUPPORTS\": 0,\n",
    "    \"REFUTES\": 1,\n",
    "    \"NOT_ENOUGH_INFO\": 2,\n",
    "    \"DISPUTED\": 3\n",
    "}\n",
    "\n",
    "class ClaimEvidenceDataset(Dataset):\n",
    "    def __init__(self, claims, evidence_dict, tokenizer, max_length=512):\n",
    "        self.encodings = []\n",
    "        self.labels = []\n",
    "        for claim_data in claims.values():\n",
    "            claim_text = claim_data[\"claim_text\"]\n",
    "            label_str = claim_data[\"claim_label\"]\n",
    "            for eid in claim_data.get(\"evidences\", []):\n",
    "                if eid in evidence_dict:\n",
    "                    evidence_text = evidence_dict[eid]\n",
    "                    encoded = tokenizer(\n",
    "                        claim_text,\n",
    "                        evidence_text,\n",
    "                        padding=\"max_length\",\n",
    "                        truncation=True,\n",
    "                        max_length=max_length,\n",
    "                        return_tensors=\"pt\"\n",
    "                    )\n",
    "                    self.encodings.append({k: v.squeeze() for k, v in encoded.items()})\n",
    "                    self.labels.append(label2id[label_str])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.encodings[idx]\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7550fdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, random\n",
    "from collections import defaultdict, Counter\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "PAD, MASK = tokenizer.pad_token_id, tokenizer.mask_token_id\n",
    "\n",
    "label2id = {\"SUPPORTS\": 0, \"REFUTES\": 1, \"NOT_ENOUGH_INFO\": 2, \"DISPUTED\": 3}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "\n",
    "# Dataset class for claim and evidence\n",
    "class ClaimEvidenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    - balance=True  : Balance all labels to max_count * target_ratio\n",
    "    - augmenters    : Optional {'dropout','swap','pad','cutmix'}\n",
    "    - aug_params    : Hyperparameters for each method\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 claims: dict,\n",
    "                 evidence_dict: dict,\n",
    "                 tokenizer,\n",
    "                 max_length: int = 512,\n",
    "                 balance: bool = True,\n",
    "                 target_ratio: float = 1.0,\n",
    "                 augmenters=None,\n",
    "                 aug_params=None,\n",
    "                 seed: int = 42):\n",
    "\n",
    "        random.seed(seed)\n",
    "        self.tokenizer, self.max_length = tokenizer, max_length\n",
    "        self.encodings, self.labels = [], []\n",
    "        self.augmenters = set(augmenters or ['dropout', 'swap', 'pad', 'cutmix'])\n",
    "\n",
    "        # Default hyperparameters\n",
    "        _default = dict(dropout_prob=0.15,\n",
    "                        swap_prob=0.10,\n",
    "                        pad_prob=0.05,\n",
    "                        cutmix_min=0.3,\n",
    "                        cutmix_max=0.7)\n",
    "        self.aug_params = {**_default, **(aug_params or {})}\n",
    "\n",
    "        # --------- Original sample encoding --------- #\n",
    "        for cdict in claims.values():\n",
    "            claim_text = cdict[\"claim_text\"]\n",
    "            lab = label2id[cdict[\"claim_label\"]]\n",
    "            for eid in cdict.get(\"evidences\", []):\n",
    "                if eid in evidence_dict:\n",
    "                    evi = evidence_dict[eid]\n",
    "                    toks = tokenizer(claim_text, evi,\n",
    "                                     truncation=True,\n",
    "                                     padding=\"max_length\",\n",
    "                                     max_length=max_length,\n",
    "                                     return_tensors=\"pt\")\n",
    "                    self.encodings.append({k: v.squeeze(0) for k, v in toks.items()})\n",
    "                    self.labels.append(lab)\n",
    "\n",
    "        # --------- Oversample with online augmentation --------- #\n",
    "        if balance:\n",
    "            self._balance_dataset(target_ratio)\n",
    "\n",
    "    # ======= Dataset API ======= #\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: v.clone() for k, v in self.encodings[idx].items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    # ======= Balance samples by different labels ======= #\n",
    "    def _balance_dataset(self, target_ratio: float):\n",
    "        by_label = defaultdict(list)\n",
    "        for i, y in enumerate(self.labels):\n",
    "            by_label[y].append(i)\n",
    "\n",
    "        max_count = int(max(len(v) for v in by_label.values()) * target_ratio)\n",
    "\n",
    "        for lab, idx_list in by_label.items():\n",
    "            need = max_count - len(idx_list)\n",
    "            for _ in range(max(0, need)):\n",
    "                base_idx = random.choice(idx_list)\n",
    "                base_enc = self.encodings[base_idx]\n",
    "                aug_enc = self._augment_encoding(base_enc, lab)\n",
    "                self.encodings.append(aug_enc)\n",
    "                self.labels.append(lab)\n",
    "\n",
    "    # ======= Augment a single sample ======= #\n",
    "    def _augment_encoding(self, enc, lab):\n",
    "        enc = {k: v.clone() for k, v in enc.items()} \n",
    "        choice = random.choice(list(self.augmenters))\n",
    "        if choice == 'dropout':\n",
    "            self._token_dropout(enc)\n",
    "        elif choice == 'swap':\n",
    "            self._swap_neighbor(enc)\n",
    "        elif choice == 'pad':\n",
    "            self._random_pad(enc)\n",
    "        elif choice == 'cutmix':\n",
    "            self._cutmix(enc, lab)\n",
    "        return enc\n",
    "\n",
    "    # ----- 1. Random token dropout -----\n",
    "    def _token_dropout(self, enc):\n",
    "        ids = enc['input_ids']\n",
    "        mask = torch.rand_like(ids.float()) < self.aug_params['dropout_prob']\n",
    "        ids[mask & (ids != PAD)] = MASK\n",
    "        enc['input_ids'] = ids\n",
    "\n",
    "    # ----- 2. Swap neighbouring tokens -----\n",
    "    def _swap_neighbor(self, enc):\n",
    "        ids = enc['input_ids']\n",
    "        for i in range(1, len(ids) - 1):\n",
    "            if random.random() < self.aug_params['swap_prob'] and ids[i] not in (PAD, MASK):\n",
    "                ids[i], ids[i + 1] = ids[i + 1].clone(), ids[i].clone()\n",
    "        enc['input_ids'] = ids\n",
    "\n",
    "    # ----- 3. Random inner padding -----\n",
    "    def _random_pad(self, enc):\n",
    "        ids, mask = enc['input_ids'], enc['attention_mask']\n",
    "        pad_prob = self.aug_params['pad_prob']\n",
    "        new_ids, new_mask = [], []\n",
    "        for tok, m in zip(ids, mask):\n",
    "            if m.item() == 0:  \n",
    "                break\n",
    "            new_ids.append(tok.item())\n",
    "            new_mask.append(1)\n",
    "            if random.random() < pad_prob and len(new_ids) < self.max_length - 1:\n",
    "                new_ids.append(PAD)\n",
    "                new_mask.append(0)\n",
    "        # Truncate / pad with PAD at the end\n",
    "        new_ids = (new_ids + [PAD] * self.max_length)[:self.max_length]\n",
    "        new_mask = (new_mask + [0] * self.max_length)[:self.max_length]\n",
    "        enc['input_ids'] = torch.tensor(new_ids, dtype=torch.long)\n",
    "        enc['attention_mask'] = torch.tensor(new_mask, dtype=torch.long)\n",
    "\n",
    "    # ----- 4. CutMix (same label) -----\n",
    "    def _cutmix(self, enc, lab):\n",
    "        # Randomly select another sample with the same label\n",
    "        same_idxs = [i for i, y in enumerate(self.labels) if y == lab]\n",
    "        other = {k: v.clone() for k, v in self.encodings[random.choice(same_idxs)].items()}\n",
    "        lam = random.uniform(self.aug_params['cutmix_min'],\n",
    "                             self.aug_params['cutmix_max'])\n",
    "        cut_point = int(lam * self.max_length)\n",
    "        # Take the current half and the other half\n",
    "        enc['input_ids'][cut_point:] = other['input_ids'][cut_point:]\n",
    "        enc['attention_mask'][cut_point:] = other['attention_mask'][cut_point:]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc224366",
   "metadata": {},
   "source": [
    "## BERT finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ba91516",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./model/my_bert_classifier\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085ca35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# load data\n",
    "with open('./data/train-claims.json', 'r') as f:\n",
    "    train_claims = json.load(f)\n",
    "with open('./data/evidence.json', 'r') as f:\n",
    "    evidence_dict = json.load(f)\n",
    "\n",
    "\n",
    "# 3. Construct the dataset\n",
    "train_dataset = train_dataset = ClaimEvidenceDataset(\n",
    "    claims=train_claims,\n",
    "    evidence_dict=evidence_dict,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512,\n",
    "    balance=True,             \n",
    "    target_ratio=1.0,\n",
    ")\n",
    "# Load BERT model\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=4)\n",
    "\n",
    "# Set training parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    eval_strategy=\"no\",\n",
    "    save_strategy=\"no\",\n",
    "    logging_strategy=\"no\",   \n",
    "    logging_steps=50,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save model\n",
    "\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62cd6d0",
   "metadata": {},
   "source": [
    "## Predict and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f5c18e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying: 100%|██████████| 153/153 [00:14<00:00, 10.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: saved to test-claims-final-predictions.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 1. load model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BertForSequenceClassification.from_pretrained(model_path).to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "model.eval()\n",
    "\n",
    "# 2. load your retrieval file (contains claim_text)\n",
    "with open(\"test-claims-predictions.json\", \"r\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "# 3. load evidence content\n",
    "with open(\"./data/evidence.json\", \"r\") as f:\n",
    "    evidence_dict = json.load(f)\n",
    "\n",
    "id2label = {\n",
    "    0: \"SUPPORTS\",\n",
    "    1: \"REFUTES\",\n",
    "    2: \"NOT_ENOUGH_INFO\",\n",
    "    3: \"DISPUTED\"\n",
    "}\n",
    "\n",
    "# 4. define the function to predict the label with average probability\n",
    "def predict_label_average(claim_text, evidence_ids):\n",
    "    probs_list = []\n",
    "\n",
    "    for eid in evidence_ids:\n",
    "        if eid not in evidence_dict:\n",
    "            continue\n",
    "        ev_text = evidence_dict[eid]\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            claim_text,\n",
    "            ev_text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=512\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            probs = F.softmax(outputs.logits, dim=-1)\n",
    "            probs_list.append(probs.cpu().numpy()[0])\n",
    "\n",
    "    if not probs_list:\n",
    "        return \"NOT_ENOUGH_INFO\"  # fallback\n",
    "\n",
    "    avg_probs = np.mean(probs_list, axis=0)\n",
    "    pred_idx = int(np.argmax(avg_probs))\n",
    "    return id2label[pred_idx]\n",
    "\n",
    "# 5. predict all test claims\n",
    "final_predictions = {}\n",
    "\n",
    "for cid, entry in tqdm(test_data.items(), desc=\"Classifying\"):\n",
    "    claim_text = entry[\"claim_text\"]\n",
    "    evidence_ids = entry[\"evidences\"]\n",
    "\n",
    "    label = predict_label_average(claim_text, evidence_ids)\n",
    "\n",
    "    final_predictions[cid] = {\n",
    "        \"evidences\": evidence_ids,\n",
    "        \"claim_label\": label,\n",
    "        \"claim_text\": claim_text\n",
    "    }\n",
    "\n",
    "# 6. save\n",
    "with open(\"test-claims-final-predictions.json\", \"w\") as f:\n",
    "    json.dump(final_predictions, f, indent=2)\n",
    "\n",
    "print(\"Done: saved to test-claims-final-predictions.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
