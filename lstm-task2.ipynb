{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c5f1db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/felikskong/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/felikskong/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from lemminflect import getAllInflections\n",
    "from nltk.tokenize import sent_tokenize\n",
    "# If you haven’t already downloaded these:\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "934ddf85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step1: English keep 1207838/1208827\n",
      "Step2: Climate-related keep 385471/1207838\n",
      "✅ Saved 385471 passages to preprocessed/climate_evidence.json\n"
     ]
    }
   ],
   "source": [
    "# ── 0) Paths ───────────────────────────────────────────────────────────────\n",
    "train_claims_path = \"data/train-claims.json\"\n",
    "evidence_path     = \"data/evidence.json\"\n",
    "output_path       = Path(\"preprocessed\") / \"climate_evidence.json\"\n",
    "\n",
    "# ── 1) Load data ───────────────────────────────────────────────────────────\n",
    "with open(train_claims_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    train_claims = json.load(f)\n",
    "\n",
    "with open(evidence_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    evidence_dict = json.load(f)\n",
    "\n",
    "# ── 2) Extract top-100 nouns from train claims ───────────────────────────────\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "all_nouns = []\n",
    "for claim in train_claims.values():\n",
    "    doc = nlp(claim[\"claim_text\"])\n",
    "    all_nouns.extend([tok.lemma_.lower() for tok in doc if tok.pos_ == \"NOUN\"])\n",
    "\n",
    "top_keywords = set(w for w, _ in Counter(all_nouns).most_common(100))\n",
    "\n",
    "# ── 3) Expand each keyword to all its noun inflections ───────────────────────\n",
    "all_forms = set()\n",
    "for lemma in top_keywords:\n",
    "    all_forms.add(lemma)\n",
    "    infl_map = getAllInflections(lemma, upos=\"NOUN\") or {}\n",
    "    for forms in infl_map.values():\n",
    "        all_forms.update(forms)\n",
    "\n",
    "# ── 4) Define filtering functions ───────────────────────────────────────────\n",
    "def is_english(text: str, threshold: float = 0.5) -> bool:\n",
    "    \"\"\"Rough check: at least `threshold` fraction of chars must be A–Z/a–z.\"\"\"\n",
    "    cleaned = re.sub(r\"[^A-Za-z\\s]\", \"\", text)\n",
    "    if not cleaned:\n",
    "        return False\n",
    "    alpha_count = sum(ch.isalpha() for ch in cleaned)\n",
    "    return (alpha_count / len(cleaned)) >= threshold\n",
    "\n",
    "def contains_climate_keywords(text: str, forms: set) -> bool:\n",
    "    \"\"\"True if any token in text (ASCII, a–z only) is in our all_forms set.\"\"\"\n",
    "    words = re.findall(r\"\\b[a-z']+\\b\", text.lower())\n",
    "    return any(w in forms for w in words)\n",
    "\n",
    "# ── 5) Filter the evidence ─────────────────────────────────────────────────\n",
    "#  5.1 Keep only English passages\n",
    "english_pairs = [\n",
    "    (eid, txt)\n",
    "    for eid, txt in evidence_dict.items()\n",
    "    if is_english(txt)\n",
    "]\n",
    "\n",
    "#  5.2 Among those, keep only climate-related ones\n",
    "climate_pairs = [\n",
    "    (eid, txt)\n",
    "    for eid, txt in english_pairs\n",
    "    if contains_climate_keywords(txt, all_forms)\n",
    "]\n",
    "\n",
    "print(f\"Step1: English keep {len(english_pairs)}/{len(evidence_dict)}\")\n",
    "print(f\"Step2: Climate-related keep {len(climate_pairs)}/{len(english_pairs)}\")\n",
    "\n",
    "# ── 6) Write out climate-related evidence ───────────────────────────────────\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "climate_evidence = {eid: txt for eid, txt in climate_pairs}\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as outf:\n",
    "    json.dump(climate_evidence, outf, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✅ Saved {len(climate_evidence)} passages to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4787a59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/felikskong/anaconda3/envs/nlp/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/felikskong/anaconda3/envs/nlp/lib/python3.11/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <CFED5F8E-EC3F-36FD-AAA3-2C6C7F8D3DD9> /Users/felikskong/anaconda3/envs/nlp/lib/python3.11/site-packages/torchvision/image.so\n",
      "  Expected in:     <1FF6C703-3F50-3698-A578-F618DE160E0B> /Users/felikskong/anaconda3/envs/nlp/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# Full Training Script for BiLSTM+Frozen-BERT Classification in one Notebook Cell\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 0) Config & Paths\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "DATA_DIR       = Path(\"data\")\n",
    "# TRAIN_JSON     = DATA_DIR / \"combined-claims.json\"\n",
    "# DEV_JSON       = DATA_DIR / \"groundtruth_output.json\"\n",
    "TRAIN_JSON     = DATA_DIR / \"train-claims-augmented.json\"\n",
    "DEV_JSON       = DATA_DIR / \"dev-claims.json\"\n",
    "EVID_JSON      = DATA_DIR / \"evidence.json\"\n",
    "\n",
    "BERT_MODEL     = \"bert-base-uncased\"\n",
    "MAX_LEN        = 256\n",
    "LSTM_HID_DIM   = 512\n",
    "NUM_CLASSES    = 4\n",
    "DROPOUT_PROB   = 0.2\n",
    "NUM_LAYERS     = 3\n",
    "BATCH_SIZE     = 16\n",
    "EPOCHS         = 5\n",
    "LR             = 2e-4\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Label ↔ index map\n",
    "label2idx = {\n",
    "    \"SUPPORTS\":         0,\n",
    "    \"NOT_ENOUGH_INFO\":  1,\n",
    "    \"REFUTES\":          2,\n",
    "    \"DISPUTED\":         3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1ecc2a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTMWithBertEncoder(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout_bert): Dropout(p=0.2, inplace=False)\n",
       "  (lstm): LSTM(768, 512, num_layers=3, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "  (attn_fc): Linear(in_features=1024, out_features=1, bias=True)\n",
       "  (dropout_pool): Dropout(p=0.2, inplace=False)\n",
       "  (classifier): Linear(in_features=1024, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 1) Load JSON data\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "with open(TRAIN_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    train_claims = json.load(f)\n",
    "with open(DEV_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    dev_claims = json.load(f)\n",
    "with open(EVID_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    evidence_dict = json.load(f)\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 2) Dataset + DataLoader (num_workers=0 to avoid pickling errors)\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL)\n",
    "\n",
    "class ClaimEvidenceDataset(Dataset):\n",
    "    def __init__(self, claims, evidences, tokenizer, max_len):\n",
    "        self.items = []\n",
    "        for cid, obj in claims.items():\n",
    "            claim_text = obj[\"claim_text\"]\n",
    "            ev_ids     = obj.get(\"evidences\", [])\n",
    "            ev_texts   = [evidences[e] for e in ev_ids if e in evidences]\n",
    "            # full sequence: claim [SEP] evidence1 evidence2 ...\n",
    "            full_input = claim_text + \" [SEP] \" + \" \".join(ev_texts)\n",
    "            label = label2idx[obj[\"claim_label\"]]\n",
    "            self.items.append((full_input, label))\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len   = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text, label = self.items[idx]\n",
    "        enc = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return (\n",
    "            enc[\"input_ids\"].squeeze(0),\n",
    "            enc[\"attention_mask\"].squeeze(0),\n",
    "            torch.tensor(label, dtype=torch.long),\n",
    "        )\n",
    "\n",
    "def collate_batch(batch):\n",
    "    ids, masks, labs = zip(*batch)\n",
    "    return torch.stack(ids), torch.stack(masks), torch.stack(labs)\n",
    "\n",
    "# create datasets and loaders\n",
    "train_ds = ClaimEvidenceDataset(train_claims, evidence_dict, tokenizer, MAX_LEN)\n",
    "dev_ds   = ClaimEvidenceDataset(dev_claims,   evidence_dict, tokenizer, MAX_LEN)\n",
    "\n",
    "train_dl = DataLoader(\n",
    "    train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    collate_fn=collate_batch, num_workers=0, pin_memory=True\n",
    ")\n",
    "dev_dl   = DataLoader(\n",
    "    dev_ds,   batch_size=BATCH_SIZE, shuffle=False,\n",
    "    collate_fn=collate_batch, num_workers=0, pin_memory=True\n",
    ")\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 3) Model Definition\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "class BiLSTMWithBertEncoder(nn.Module):\n",
    "    def __init__(self, bert_name, lstm_hid, num_classes, \n",
    "                 dropout_prob, lstm_layers):\n",
    "        super().__init__()\n",
    "        # 1) Frozen BERT\n",
    "        self.bert = BertModel.from_pretrained(bert_name)\n",
    "        for p in self.bert.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        bert_dim = self.bert.config.hidden_size\n",
    "\n",
    "        # 2) Dropout on BERT outputs\n",
    "        self.dropout_bert = nn.Dropout(dropout_prob)\n",
    "\n",
    "        # 3) 2-layer BiLSTM with inter-layer dropout\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size    = bert_dim,\n",
    "            hidden_size   = lstm_hid,\n",
    "            num_layers    = lstm_layers,\n",
    "            batch_first   = True,\n",
    "            bidirectional = True,\n",
    "            dropout       = dropout_prob  # only applied between layers\n",
    "        )\n",
    "\n",
    "        # 4) Attention scoring layer\n",
    "        #    对每个 time-step 的 2*hid 维输出打分\n",
    "        self.attn_fc = nn.Linear(2 * lstm_hid, 1)\n",
    "\n",
    "        # 5) Dropout before classifier\n",
    "        self.dropout_pool = nn.Dropout(dropout_prob)\n",
    "\n",
    "        # 6) Final classification head\n",
    "        self.classifier = nn.Linear(2 * lstm_hid, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # a) BERT encoding (frozen)\n",
    "        bert_out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        seq_emb  = bert_out.last_hidden_state            # (B, L, D)\n",
    "        seq_emb  = self.dropout_bert(seq_emb)\n",
    "\n",
    "        # b) BiLSTM\n",
    "        lstm_out, _ = self.lstm(seq_emb)                 # (B, L, 2H)\n",
    "\n",
    "        # c) Self-attention pooling\n",
    "        #    1) 计算每个 time-step 的 attention score\n",
    "        scores = self.attn_fc(lstm_out).squeeze(-1)      # (B, L)\n",
    "        #    2) 对 pad 部分打 -inf\n",
    "        scores = scores.masked_fill(attention_mask == 0, -1e9)\n",
    "        #    3) 得到权重并做加权求和\n",
    "        alphas = torch.softmax(scores, dim=1)            # (B, L)\n",
    "        pooled = torch.sum(lstm_out * alphas.unsqueeze(-1), dim=1)  # (B, 2H)\n",
    "\n",
    "        # d) Dropout + classification\n",
    "        pooled = self.dropout_pool(pooled)\n",
    "        logits = self.classifier(pooled)                 # (B, num_classes)\n",
    "        return logits\n",
    "    \n",
    "model     = BiLSTMWithBertEncoder(BERT_MODEL, LSTM_HID_DIM, NUM_CLASSES, DROPOUT_PROB, NUM_LAYERS)\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bbb434a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ded429c0ade34a29b25a417d482d1e20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 1:   0%|          | 0/147 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 1 Avg Loss: 1.3853\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "776aeeba75d345d9abd42f631ed7d523",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " Eval:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Dev Accuracy: 53.9474%\n",
      "✅ New best model saved (epoch 1, acc 53.9474%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94e37ed4c8e248f4b50a1ca0f915868a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 2:   0%|          | 0/147 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 2 Avg Loss: 1.3816\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7f825f8be7347f38eec88d709fcb919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " Eval:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Dev Accuracy: 43.4211%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f4d2aae6a2f488588e91482cf4e4632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 3:   0%|          | 0/147 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 3 Avg Loss: 1.3777\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b7219b6d9064949b3807eb84f46b711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " Eval:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Dev Accuracy: 46.0526%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a8239b71b1447b3863e05e84df26421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 4:   0%|          | 0/147 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 4 Avg Loss: 1.3745\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97ee479e443b49a2adfb638c992a8df0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " Eval:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Dev Accuracy: 40.7895%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd95ab0ee557425da15cc08e8226a19b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 5:   0%|          | 0/147 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 5 Avg Loss: 1.3704\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6ca908bd6f44bfe9286126130141862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " Eval:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Dev Accuracy: 35.5263%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 4) Training Loop (with best-model saving)\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "optimizer = torch.optim.Adam(model.classifier.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "best_acc = 0.0\n",
    "BEST_MODEL_PATH = \"task2_best_model.pt\"\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # -- train --\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for input_ids, attn_mask, labels in tqdm(train_dl, desc=f\"Train Epoch {epoch}\"):\n",
    "        input_ids = input_ids.to(DEVICE)\n",
    "        attn_mask = attn_mask.to(DEVICE)\n",
    "        labels    = labels.to(DEVICE)\n",
    "\n",
    "        logits = model(input_ids, attn_mask)\n",
    "        loss   = criterion(logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_dl)\n",
    "    print(f\"→ Epoch {epoch} Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # -- eval on dev --\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total   = 0\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attn_mask, labels in tqdm(dev_dl, desc=\" Eval\"):\n",
    "            input_ids = input_ids.to(DEVICE)\n",
    "            attn_mask = attn_mask.to(DEVICE)\n",
    "            labels    = labels.to(DEVICE)\n",
    "\n",
    "            preds = model(input_ids, attn_mask).argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total   += labels.size(0)\n",
    "\n",
    "    acc = correct / total\n",
    "    print(f\"→ Dev Accuracy: {acc:.4%}\")\n",
    "\n",
    "    # -- save best model --\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
    "        print(f\"✅ New best model saved (epoch {epoch}, acc {acc:.4%})\\n\")\n",
    "    else:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "413280be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xv/d8fp_fgs1fx30rm4fs63qv1c0000gn/T/ipykernel_56082/2927943653.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\"task2_best_model_6.pt\", map_location=DEVICE)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dd7dcc3a7fb4d53bf1a611020e445eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting labels:   0%|          | 0/153 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Written predictions to /Users/felikskong/Desktop/NLP/NLP_Ass3/test-output.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# 假设 以下 变量/对象 已在当前 scope 中 定义好：\n",
    "#   model, tokenizer, MAX_LEN, DEVICE, label2idx\n",
    "\n",
    "# 1) 读入 test 集（带 top-k evidence IDs） & 全部 evidence 文本\n",
    "TEST_JSON     = Path(\"test-claims-predictions.json\")\n",
    "EVIDENCE_JSON = Path(\"data\") / \"evidence.json\"\n",
    "\n",
    "test_claims   = json.loads(TEST_JSON.read_text(encoding=\"utf-8\"))\n",
    "evidence_dict = json.loads(EVIDENCE_JSON.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "# 2) 构造 idx→label 映射\n",
    "idx2label = {v:k for k,v in label2idx.items()}\n",
    "\n",
    "# 3) 推断并写入结果\n",
    "model = BiLSTMWithBertEncoder(BERT_MODEL, LSTM_HID_DIM, NUM_CLASSES, DROPOUT_PROB, NUM_LAYERS)\n",
    "state_dict = torch.load(\"task2_best_model_6.pt\", map_location=DEVICE)\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "output = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for cid, obj in tqdm(test_claims.items(), desc=\"Predicting labels\"):\n",
    "        claim_text = obj[\"claim_text\"]\n",
    "        # 直接保留所有原始 evidence IDs（不做丢弃）\n",
    "        ev_ids = obj.get(\"evidences\", [])\n",
    "\n",
    "        # 如果还要拿文本去做前向编码，就用 .get()，不会抛 KeyError\n",
    "        ev_texts = [ evidence_dict.get(eid, \"\") for eid in ev_ids ]\n",
    "\n",
    "        # 用 tokenizer 的双句接口（自动在中间插 [SEP]）\n",
    "        enc = tokenizer(\n",
    "            claim_text,\n",
    "            \" \".join(ev_texts),\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=MAX_LEN,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids      = enc[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = enc[\"attention_mask\"].to(DEVICE)\n",
    "\n",
    "        # forward + argmax → label idx\n",
    "        logits = model(input_ids, attention_mask)  # (1, num_classes)\n",
    "        pred   = logits.argmax(dim=-1).item()\n",
    "        label  = idx2label[pred]\n",
    "\n",
    "        # 保存：claim_text, 预测 label, **原样保留** evidence IDs 列表\n",
    "        output[cid] = {\n",
    "            \"claim_text\":  claim_text,\n",
    "            \"claim_label\": label,\n",
    "            \"evidences\":   ev_ids\n",
    "        }\n",
    "\n",
    "# 4) 写入 test-output.json\n",
    "OUT_JSON = Path(\"test-output.json\")\n",
    "with OUT_JSON.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✅ Written predictions to {OUT_JSON.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc4faa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b 文件的 claim_label 已成功更新并保存为 test-output-0.21-0.51.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"test-output-0.51.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    a_data = json.load(f)\n",
    "with open(\"test-output.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    b_data = json.load(f)\n",
    "for claim_id, a_claim in a_data.items():\n",
    "    if claim_id in b_data:\n",
    "        b_data[claim_id][\"claim_label\"] = a_claim[\"claim_label\"]\n",
    "with open(\"test-output.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(b_data, f, ensure_ascii=False, indent=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
