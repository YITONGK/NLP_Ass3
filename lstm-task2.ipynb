{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c5f1db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/felikskong/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/felikskong/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from lemminflect import getAllInflections\n",
    "from nltk.tokenize import sent_tokenize\n",
    "# If you haven’t already downloaded these:\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "934ddf85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step1: English keep 1207838/1208827\n",
      "Step2: Climate-related keep 385471/1207838\n",
      "✅ Saved 385471 passages to preprocessed/climate_evidence.json\n"
     ]
    }
   ],
   "source": [
    "# ── 0) Paths ───────────────────────────────────────────────────────────────\n",
    "train_claims_path = \"data/train-claims.json\"\n",
    "evidence_path     = \"data/evidence.json\"\n",
    "output_path       = Path(\"preprocessed\") / \"climate_evidence.json\"\n",
    "\n",
    "# ── 1) Load data ───────────────────────────────────────────────────────────\n",
    "with open(train_claims_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    train_claims = json.load(f)\n",
    "\n",
    "with open(evidence_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    evidence_dict = json.load(f)\n",
    "\n",
    "# ── 2) Extract top-100 nouns from train claims ───────────────────────────────\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "all_nouns = []\n",
    "for claim in train_claims.values():\n",
    "    doc = nlp(claim[\"claim_text\"])\n",
    "    all_nouns.extend([tok.lemma_.lower() for tok in doc if tok.pos_ == \"NOUN\"])\n",
    "\n",
    "top_keywords = set(w for w, _ in Counter(all_nouns).most_common(100))\n",
    "\n",
    "# ── 3) Expand each keyword to all its noun inflections ───────────────────────\n",
    "all_forms = set()\n",
    "for lemma in top_keywords:\n",
    "    all_forms.add(lemma)\n",
    "    infl_map = getAllInflections(lemma, upos=\"NOUN\") or {}\n",
    "    for forms in infl_map.values():\n",
    "        all_forms.update(forms)\n",
    "\n",
    "# ── 4) Define filtering functions ───────────────────────────────────────────\n",
    "def is_english(text: str, threshold: float = 0.5) -> bool:\n",
    "    \"\"\"Rough check: at least `threshold` fraction of chars must be A–Z/a–z.\"\"\"\n",
    "    cleaned = re.sub(r\"[^A-Za-z\\s]\", \"\", text)\n",
    "    if not cleaned:\n",
    "        return False\n",
    "    alpha_count = sum(ch.isalpha() for ch in cleaned)\n",
    "    return (alpha_count / len(cleaned)) >= threshold\n",
    "\n",
    "def contains_climate_keywords(text: str, forms: set) -> bool:\n",
    "    \"\"\"True if any token in text (ASCII, a–z only) is in our all_forms set.\"\"\"\n",
    "    words = re.findall(r\"\\b[a-z']+\\b\", text.lower())\n",
    "    return any(w in forms for w in words)\n",
    "\n",
    "# ── 5) Filter the evidence ─────────────────────────────────────────────────\n",
    "#  5.1 Keep only English passages\n",
    "english_pairs = [\n",
    "    (eid, txt)\n",
    "    for eid, txt in evidence_dict.items()\n",
    "    if is_english(txt)\n",
    "]\n",
    "\n",
    "#  5.2 Among those, keep only climate-related ones\n",
    "climate_pairs = [\n",
    "    (eid, txt)\n",
    "    for eid, txt in english_pairs\n",
    "    if contains_climate_keywords(txt, all_forms)\n",
    "]\n",
    "\n",
    "print(f\"Step1: English keep {len(english_pairs)}/{len(evidence_dict)}\")\n",
    "print(f\"Step2: Climate-related keep {len(climate_pairs)}/{len(english_pairs)}\")\n",
    "\n",
    "# ── 6) Write out climate-related evidence ───────────────────────────────────\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "climate_evidence = {eid: txt for eid, txt in climate_pairs}\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as outf:\n",
    "    json.dump(climate_evidence, outf, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✅ Saved {len(climate_evidence)} passages to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1ecc2a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22cbede5346c4c28b95e8452f1d9dd00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch 1:   0%|          | 0/130 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Epoch 1 Avg Loss: 1.3850\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a877f55b4dd4453b712b17ac619c31d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " Eval:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Dev Accuracy: 37.6623%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# Full Training Script for BiLSTM+Frozen-BERT Classification in one Notebook Cell\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 0) Config & Paths\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "DATA_DIR       = Path(\"data\")\n",
    "TRAIN_AUG_JSON = DATA_DIR / \"train-claims-augmented.json\"\n",
    "DEV_JSON       = DATA_DIR / \"dev-claims.json\"\n",
    "EVID_JSON      = DATA_DIR / \"evidence.json\"\n",
    "\n",
    "BERT_MODEL     = \"bert-base-uncased\"\n",
    "MAX_LEN        = 256\n",
    "LSTM_HID_DIM   = 128\n",
    "NUM_CLASSES    = 4\n",
    "BATCH_SIZE     = 16\n",
    "EPOCHS         = 1\n",
    "LR             = 2e-4\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Label ↔ index map\n",
    "label2idx = {\n",
    "    \"SUPPORTS\":         0,\n",
    "    \"NOT_ENOUGH_INFO\":  1,\n",
    "    \"REFUTES\":          2,\n",
    "    \"DISPUTED\":         3,\n",
    "}\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 1) Load JSON data\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "with open(TRAIN_AUG_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    train_claims = json.load(f)\n",
    "with open(DEV_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    dev_claims = json.load(f)\n",
    "with open(EVID_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    evidence_dict = json.load(f)\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 2) Dataset + DataLoader (num_workers=0 to avoid pickling errors)\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL)\n",
    "\n",
    "class ClaimEvidenceDataset(Dataset):\n",
    "    def __init__(self, claims, evidences, tokenizer, max_len):\n",
    "        self.items = []\n",
    "        for cid, obj in claims.items():\n",
    "            claim_text = obj[\"claim_text\"]\n",
    "            ev_ids     = obj.get(\"evidences\", [])\n",
    "            ev_texts   = [evidences[e] for e in ev_ids if e in evidences]\n",
    "            # full sequence: claim [SEP] evidence1 evidence2 ...\n",
    "            full_input = claim_text + \" [SEP] \" + \" \".join(ev_texts)\n",
    "            label = label2idx[obj[\"claim_label\"]]\n",
    "            self.items.append((full_input, label))\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len   = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text, label = self.items[idx]\n",
    "        enc = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return (\n",
    "            enc[\"input_ids\"].squeeze(0),\n",
    "            enc[\"attention_mask\"].squeeze(0),\n",
    "            torch.tensor(label, dtype=torch.long),\n",
    "        )\n",
    "\n",
    "def collate_batch(batch):\n",
    "    ids, masks, labs = zip(*batch)\n",
    "    return torch.stack(ids), torch.stack(masks), torch.stack(labs)\n",
    "\n",
    "# create datasets and loaders\n",
    "train_ds = ClaimEvidenceDataset(train_claims, evidence_dict, tokenizer, MAX_LEN)\n",
    "dev_ds   = ClaimEvidenceDataset(dev_claims,   evidence_dict, tokenizer, MAX_LEN)\n",
    "\n",
    "train_dl = DataLoader(\n",
    "    train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    collate_fn=collate_batch, num_workers=0, pin_memory=True\n",
    ")\n",
    "dev_dl   = DataLoader(\n",
    "    dev_ds,   batch_size=BATCH_SIZE, shuffle=False,\n",
    "    collate_fn=collate_batch, num_workers=0, pin_memory=True\n",
    ")\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 3) Model Definition\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "class BiLSTMWithBertEncoder(nn.Module):\n",
    "    def __init__(self, bert_name, lstm_hid, num_classes):\n",
    "        super().__init__()\n",
    "        # 1) BERT (frozen)\n",
    "        self.bert = BertModel.from_pretrained(bert_name)\n",
    "        for p in self.bert.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        bert_dim = self.bert.config.hidden_size\n",
    "        # 2) BiLSTM on top\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size    = bert_dim,\n",
    "            hidden_size   = lstm_hid,\n",
    "            num_layers    = 1,\n",
    "            batch_first   = True,\n",
    "            bidirectional = True\n",
    "        )\n",
    "        # 3) classification head\n",
    "        self.classifier = nn.Linear(2 * lstm_hid, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # a) BERT encoding\n",
    "        out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        seq_emb = out.last_hidden_state        # (B, L, D)\n",
    "        # b) BiLSTM\n",
    "        lstm_out, _ = self.lstm(seq_emb)       # (B, L, 2H)\n",
    "        # c) masked mean-pooling\n",
    "        mask   = attention_mask.unsqueeze(-1).float()  # (B, L, 1)\n",
    "        summed = (lstm_out * mask).sum(1)              # (B, 2H)\n",
    "        lens   = mask.sum(1)                           # (B, 1)\n",
    "        pooled = summed / lens                         # (B, 2H)\n",
    "        # d) classifier\n",
    "        return self.classifier(pooled)                 # (B, num_classes)\n",
    "\n",
    "model     = BiLSTMWithBertEncoder(BERT_MODEL, LSTM_HID_DIM, NUM_CLASSES)\n",
    "model.to(DEVICE)\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 4) Training Loop\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "optimizer = torch.optim.Adam(model.classifier.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for input_ids, attn_mask, labels in tqdm(train_dl, desc=f\"Train Epoch {epoch}\"):\n",
    "        input_ids = input_ids.to(DEVICE)\n",
    "        attn_mask = attn_mask.to(DEVICE)\n",
    "        labels    = labels.to(DEVICE)\n",
    "\n",
    "        logits = model(input_ids, attn_mask)\n",
    "        loss   = criterion(logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"→ Epoch {epoch} Avg Loss: {total_loss/len(train_dl):.4f}\")\n",
    "\n",
    "    # eval on dev\n",
    "    model.eval()\n",
    "    correct = 0; total = 0\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attn_mask, labels in tqdm(dev_dl, desc=\" Eval\"):\n",
    "            input_ids = input_ids.to(DEVICE)\n",
    "            attn_mask = attn_mask.to(DEVICE)\n",
    "            labels    = labels.to(DEVICE)\n",
    "\n",
    "            preds = model(input_ids, attn_mask).argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total   += labels.size(0)\n",
    "\n",
    "    print(f\"→ Dev Accuracy: {correct/total:.4%}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "413280be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1f26e6d64e84e139ea85fbd9dcaa2e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting labels:   0%|          | 0/153 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Written predictions to /Users/felikskong/Desktop/NLP/NLP_Ass3/test-output.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# 假设 以下 变量/对象 已在当前 scope 中 定义好：\n",
    "#   model, tokenizer, MAX_LEN, DEVICE, label2idx\n",
    "\n",
    "# 1) 读入 test 集（带 top-k evidence IDs） & 全部 evidence 文本\n",
    "TEST_JSON     = Path(\"test-claims-predictions_top3.json\")\n",
    "EVIDENCE_JSON = Path(\"data\") / \"evidence.json\"\n",
    "\n",
    "test_claims   = json.loads(TEST_JSON.read_text(encoding=\"utf-8\"))\n",
    "evidence_dict = json.loads(EVIDENCE_JSON.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "# 2) 构造 idx→label 映射\n",
    "idx2label = {v:k for k,v in label2idx.items()}\n",
    "\n",
    "# 3) 推断并写入结果\n",
    "model.eval()\n",
    "output = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for cid, obj in tqdm(test_claims.items(), desc=\"Predicting labels\"):\n",
    "        claim_text = obj[\"claim_text\"]\n",
    "        # 直接保留所有原始 evidence IDs（不做丢弃）\n",
    "        ev_ids = obj.get(\"evidence\", [])\n",
    "\n",
    "        # 如果还要拿文本去做前向编码，就用 .get()，不会抛 KeyError\n",
    "        ev_texts = [ evidence_dict.get(eid, \"\") for eid in ev_ids ]\n",
    "\n",
    "        # 用 tokenizer 的双句接口（自动在中间插 [SEP]）\n",
    "        enc = tokenizer(\n",
    "            claim_text,\n",
    "            \" \".join(ev_texts),\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=MAX_LEN,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids      = enc[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = enc[\"attention_mask\"].to(DEVICE)\n",
    "\n",
    "        # forward + argmax → label idx\n",
    "        logits = model(input_ids, attention_mask)  # (1, num_classes)\n",
    "        pred   = logits.argmax(dim=-1).item()\n",
    "        label  = idx2label[pred]\n",
    "\n",
    "        # 保存：claim_text, 预测 label, **原样保留** evidence IDs 列表\n",
    "        output[cid] = {\n",
    "            \"claim_text\":  claim_text,\n",
    "            \"claim_label\": label,\n",
    "            \"evidences\":   ev_ids\n",
    "        }\n",
    "\n",
    "# 4) 写入 test-output.json\n",
    "OUT_JSON = Path(\"test-output.json\")\n",
    "with OUT_JSON.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✅ Written predictions to {OUT_JSON.resolve()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
