{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "189afb4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\Anaconda\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import all the packages\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import Counter\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder, InputExample\n",
    "import pickle\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import re\n",
    "from datasets import Dataset\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9dd1770",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_claims_path = './data/train-claims.json'\n",
    "dev_claims_path = './data/dev-claims.json'\n",
    "evidence_path = './data/evidence.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1eb6c34",
   "metadata": {},
   "source": [
    "## 训minilm的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "279bf514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training pairs: 4122\n",
      "Missing evidence ids: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "735152cf145e4c43950a9cbfd4e26c4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='645' max='645' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [645/645 01:12, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.707800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetuned model saved.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Load the data\n",
    "with open(train_claims_path, 'r') as f:\n",
    "    train_claims = json.load(f)\n",
    "with open(evidence_path, 'r') as f:\n",
    "    evidence_dict = json.load(f)\n",
    "\n",
    "\n",
    "# Construct the training sample list (claim, evidence_text) -> label defaults to 1.0\n",
    "train_samples = []\n",
    "missed = 0\n",
    "\n",
    "for claim in train_claims.values():\n",
    "    claim_text = claim['claim_text']\n",
    "    evidence_ids = claim.get('evidences', [])\n",
    "    for eid in evidence_ids:\n",
    "        if eid in evidence_dict:\n",
    "            ev_text = evidence_dict[eid]\n",
    "            train_samples.append(InputExample(texts=[claim_text, ev_text], label=1.0))\n",
    "        else:\n",
    "            missed += 1\n",
    "\n",
    "\n",
    "print(f\"Total training pairs: {len(train_samples)}\")\n",
    "print(f\"Missing evidence ids: {missed}\")\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Build the DataLoader\n",
    "train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=32)\n",
    "\n",
    "# Define the loss function\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model)\n",
    "\n",
    "# Start training\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=5,  \n",
    "    warmup_steps=100,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save('./model/my_finetuned_minilm_retriever')\n",
    "print(\"Finetuned model saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2864fce1",
   "metadata": {},
   "source": [
    "# 训msmarco reranker的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "694dcd0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 8244\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2580' max='2580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2580/2580 02:10, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.187200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.023200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.009600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.007200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.001900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetuned model saved.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder, InputExample\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "with open(train_claims_path, 'r') as f:\n",
    "    train_claims = json.load(f)\n",
    "with open(evidence_path, 'r') as f:\n",
    "    evidence_dict = json.load(f)\n",
    "\n",
    "\n",
    "# Construct positive and negative samples\n",
    "train_samples = []\n",
    "\n",
    "def generate_samples(claims_data):\n",
    "    samples = []\n",
    "    for claim in claims_data.values():\n",
    "        claim_text = claim[\"claim_text\"]\n",
    "        evidence_ids = claim.get(\"evidences\", [])\n",
    "        pos_evidence_texts = [evidence_dict[eid] for eid in evidence_ids if eid in evidence_dict]\n",
    "\n",
    "        # Positive samples\n",
    "        for ev in pos_evidence_texts:\n",
    "            samples.append(InputExample(texts=[claim_text, ev], label=1.0))\n",
    "\n",
    "        # Negative samples\n",
    "        neg_pool = [e for eid, e in evidence_dict.items() if eid not in evidence_ids]\n",
    "        for _ in range(len(pos_evidence_texts)):\n",
    "            neg_ev = random.choice(neg_pool)\n",
    "            samples.append(InputExample(texts=[claim_text, neg_ev], label=0.0))\n",
    "\n",
    "    return samples\n",
    "\n",
    "train_samples.extend(generate_samples(train_claims))\n",
    "\n",
    "print(f\"Total training samples: {len(train_samples)}\")\n",
    "\n",
    "# Build the DataLoader (InputExample is a valid format)\n",
    "train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=16)\n",
    "\n",
    "# Load MS MARCO CrossEncoder\n",
    "model = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\", num_labels=1)\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    train_dataloader=train_dataloader,\n",
    "    epochs=5,\n",
    "    warmup_steps=100,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save('./model/my_finetuned_msmarco_reranker')\n",
    "print(\"Finetuned model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b08dc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('./model/my_finetuned_minilm_retriever')\n",
    "reranker =  CrossEncoder('./model/my_finetuned_msmarco_reranker')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a31334",
   "metadata": {},
   "source": [
    "## encode evidence dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1e2f859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "from lemminflect import getAllInflections\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# load data\n",
    "with open(train_claims_path, 'r') as f:\n",
    "    train_claims = json.load(f)\n",
    "\n",
    "with open(evidence_path, 'r') as f:\n",
    "    evidence_dict = json.load(f)\n",
    "\n",
    "# Extract all nouns from claim_text and count their frequencies \n",
    "all_nouns = []\n",
    "for claim_obj in train_claims.values():\n",
    "    doc = nlp(claim_obj[\"claim_text\"])\n",
    "    nouns = [token.lemma_.lower() for token in doc if token.pos_ == \"NOUN\"]\n",
    "    all_nouns.extend(nouns)\n",
    "\n",
    "# Select the top 100 most frequent nouns as keywords\n",
    "top_keywords = set(word for word, _ in Counter(all_nouns).most_common(100))\n",
    "\n",
    "all_forms = set()\n",
    "for lemma in top_keywords:\n",
    "  all_forms.add(lemma)\n",
    "  # Get all possible noun forms\n",
    "  infl_map = getAllInflections(lemma, upos=\"NOUN\")\n",
    "  # infl_map is a dict: { 'NNS': ['cats'], 'NNPS': ['children'], ... }\n",
    "  for forms in infl_map.values():\n",
    "      all_forms.update(forms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20fe2442",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_climate_keywords(text: str, all_forms: set) -> bool:\n",
    "    # Lowercase the text and split into words then check the set\n",
    "    words = re.findall(r\"\\b[a-z']+\\b\", text.lower())\n",
    "    return any(word in all_forms for word in words)\n",
    "\n",
    "\n",
    "def is_english(text: str, threshold: float = 0.5) -> bool:\n",
    "    # Clean the text, only keep letters and spaces\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    if len(text) == 0:  # If the text is empty after cleaning, return False\n",
    "        return False\n",
    "    # Calculate the proportion of English characters\n",
    "    english_char_count = sum(1 for char in text if char.isalpha())\n",
    "    return (english_char_count / len(text)) >= threshold\n",
    "\n",
    "def clean_and_split(eid, text):\n",
    "    result_ids = []\n",
    "    result_texts = []\n",
    "    sentences = sent_tokenize(text)\n",
    "    for i, sent in enumerate(sentences):\n",
    "        sent = sent.lower()\n",
    "        sent = re.sub(r'[^a-z0-9\\s.,!?]', '', sent)  # Remove punctuation\n",
    "        sent = re.sub(r'\\s+', ' ', sent).strip()\n",
    "        if len(sent.split()) >= 5:  # Optional: Filter out too short texts\n",
    "          result_ids.append(f\"{eid}_s{i}\")  # Use the original eID plus sentence index\n",
    "          result_texts.append(sent)\n",
    "    return result_ids, result_texts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7821493a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step1: English keep 1207838/1208827\n",
      "Step2: Climate-related keep 385471/1207838\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e99143a1db894c3ca90e938f8ea784a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12133 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the evidence embeddings\n",
    "word_embedding_path = './word_embedding/evidence_embeddings.npy'\n",
    "word_embedding_meta_path = \"./word_embedding/evidence_meta.pkl\"\n",
    "\n",
    "\n",
    "with open(evidence_path, 'r') as f:\n",
    "    evidence_dict = json.load(f)\n",
    "# 1. Remove non-English\n",
    "eids  = list(evidence_dict.keys())\n",
    "texts = list(evidence_dict.values())\n",
    "english_pairs = [\n",
    "  (eid, txt)\n",
    "  for eid, txt in zip(eids, texts)\n",
    "  if is_english(txt)\n",
    "]\n",
    "print(f\"Step1: English keep {len(english_pairs)}/{len(texts)}\")\n",
    "\n",
    "# 2. Remove non-climate-related\n",
    "climate_pairs = [\n",
    "    (eid, txt)\n",
    "    for eid, txt in english_pairs\n",
    "    if contains_climate_keywords(txt, all_forms)\n",
    "]\n",
    "print(f\"Step2: Climate-related keep {len(climate_pairs)}/{len(english_pairs)}\")\n",
    "\n",
    "\n",
    "# The results of cleaning and splitting\n",
    "cleaned_evidence_ids = []\n",
    "cleaned_evidence_texts = []\n",
    "original_evidence_ids = []  # Record the original evidence_id of each sentence\n",
    "\n",
    "# Iterate through the evidence data and clean and split\n",
    "for eid, text in climate_pairs:\n",
    "    cleaned_ids, cleaned_texts = clean_and_split(eid, text)\n",
    "    cleaned_evidence_ids.extend(cleaned_ids)\n",
    "    cleaned_evidence_texts.extend(cleaned_texts)\n",
    "    original_evidence_ids.extend([eid] * len(cleaned_ids))  # Each sentence records the original eID\n",
    "\n",
    "# Encode the cleaned sentences using Sentence-BERT\n",
    "evidence_embeddings = model.encode(\n",
    "    cleaned_evidence_texts,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "# Save the encoded embeddings (embeddings)\n",
    "np.save(word_embedding_path, evidence_embeddings)\n",
    "\n",
    "# Save the evidence_ids and texts after splitting and the corresponding original evidence_id mapping\n",
    "with open(word_embedding_meta_path, \"wb\") as f:\n",
    "    pickle.dump((cleaned_evidence_ids, cleaned_evidence_texts, original_evidence_ids), f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bef490",
   "metadata": {},
   "source": [
    "## test and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daca0134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the evidence embeddings\n",
    "word_embedding_path = './word_embedding/evidence_embeddings.npy'\n",
    "word_embedding_meta_path = \"./word_embedding/evidence_meta.pkl\"\n",
    "\n",
    "# Load numpy embeddings\n",
    "evidence_embeddings = np.load(word_embedding_path)\n",
    "\n",
    "# Load evidence_ids, evidence_texts, and original_evidence_ids\n",
    "with open(word_embedding_meta_path, \"rb\") as f:\n",
    "    evidence_ids, evidence_texts, original_evidence_ids = pickle.load(f)\n",
    "\n",
    "\n",
    "dimension = evidence_embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)  # Inner Product = Cosine similarity if normalized\n",
    "index.add(evidence_embeddings)\n",
    "\n",
    "\n",
    "def clean_claim(claim: str) -> str:\n",
    "    # Lowercase\n",
    "    claim = claim.lower()\n",
    "    # Remove punctuation\n",
    "    claim = re.sub(r'[^a-z0-9\\s]', '', claim)\n",
    "    # Remove extra spaces\n",
    "    claim = re.sub(r'\\s+', ' ', claim).strip()\n",
    "    return claim\n",
    "\n",
    "# Create mapping\n",
    "evidence_dict = dict(zip(evidence_ids, evidence_texts))\n",
    "\n",
    "with open(evidence_path, 'r') as f:\n",
    "    original_evidence_dict = json.load(f)\n",
    "\n",
    "def retrieve_evidence(claim_id, claim_data, retrieval=100, top_k=5):\n",
    "    claim_text = claim_data[\"claim_text\"]\n",
    "    cleaned_claim = clean_claim(claim_text)\n",
    "\n",
    "    # Step 1: Coarse retrieval (model + FAISS)\n",
    "    claim_embedding = model.encode([cleaned_claim], convert_to_numpy=True, normalize_embeddings=True)\n",
    "    scores, indices = index.search(claim_embedding, retrieval * 3)\n",
    "\n",
    "    # Step 2: Remove duplicate candidates\n",
    "    seen_original_ids = set()\n",
    "    candidates = []\n",
    "    for i in indices[0]:\n",
    "        eid = evidence_ids[i]\n",
    "        text = evidence_dict[eid]\n",
    "        original_id = original_evidence_ids[i]\n",
    "\n",
    "        if original_id not in seen_original_ids:\n",
    "            candidates.append((original_id, eid, text))\n",
    "            seen_original_ids.add(original_id)\n",
    "\n",
    "        if len(candidates) >= retrieval:\n",
    "            break\n",
    "\n",
    "    # Step 3: Reranking (CrossEncoder)\n",
    "    pairs = [(claim_text, original_evidence_dict[orig_id]) for (orig_id, _, _) in candidates]  \n",
    "    similarity_scores = reranker.predict(pairs)\n",
    "\n",
    "    reranked = sorted(zip(candidates, similarity_scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Only return the original document-level evidence ID\n",
    "    top_k_original_ids = [orig_id for (orig_id, _, _), _ in reranked[:top_k]]\n",
    "\n",
    "    # Construct the final result dict\n",
    "    result = {\n",
    "        \"claim_text\": claim_text,\n",
    "        \"evidences\": top_k_original_ids\n",
    "    }\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9843a67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating R=100, K=3: 100%|██████████| 154/154 [00:21<00:00,  7.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieval=100, Top-K=3\n",
      "   - Avg Recall   : 22.71%\n",
      "   - Avg Precision: 20.35%\n",
      "   - Avg F1       : 19.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating R=100, K=4: 100%|██████████| 154/154 [00:21<00:00,  7.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieval=100, Top-K=4\n",
      "   - Avg Recall   : 26.36%\n",
      "   - Avg Precision: 18.02%\n",
      "   - Avg F1       : 19.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating R=100, K=5: 100%|██████████| 154/154 [00:23<00:00,  6.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieval=100, Top-K=5\n",
      "   - Avg Recall   : 27.86%\n",
      "   - Avg Precision: 15.45%\n",
      "   - Avg F1       : 18.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating R=200, K=3: 100%|██████████| 154/154 [00:38<00:00,  4.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieval=200, Top-K=3\n",
      "   - Avg Recall   : 22.25%\n",
      "   - Avg Precision: 19.70%\n",
      "   - Avg F1       : 19.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating R=200, K=4: 100%|██████████| 154/154 [00:41<00:00,  3.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieval=200, Top-K=4\n",
      "   - Avg Recall   : 25.65%\n",
      "   - Avg Precision: 17.37%\n",
      "   - Avg F1       : 19.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating R=200, K=5: 100%|██████████| 154/154 [00:43<00:00,  3.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieval=200, Top-K=5\n",
      "   - Avg Recall   : 27.47%\n",
      "   - Avg Precision: 15.06%\n",
      "   - Avg F1       : 18.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating R=500, K=3: 100%|██████████| 154/154 [01:45<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieval=500, Top-K=3\n",
      "   - Avg Recall   : 20.89%\n",
      "   - Avg Precision: 18.61%\n",
      "   - Avg F1       : 18.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating R=500, K=4: 100%|██████████| 154/154 [01:49<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieval=500, Top-K=4\n",
      "   - Avg Recall   : 25.31%\n",
      "   - Avg Precision: 16.88%\n",
      "   - Avg F1       : 18.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating R=500, K=5: 100%|██████████| 154/154 [01:50<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieval=500, Top-K=5\n",
      "   - Avg Recall   : 26.56%\n",
      "   - Avg Precision: 14.55%\n",
      "   - Avg F1       : 17.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating R=1000, K=3: 100%|██████████| 154/154 [03:40<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieval=1000, Top-K=3\n",
      "   - Avg Recall   : 20.78%\n",
      "   - Avg Precision: 18.61%\n",
      "   - Avg F1       : 18.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating R=1000, K=4: 100%|██████████| 154/154 [03:41<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieval=1000, Top-K=4\n",
      "   - Avg Recall   : 25.18%\n",
      "   - Avg Precision: 16.72%\n",
      "   - Avg F1       : 18.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating R=1000, K=5: 100%|██████████| 154/154 [03:39<00:00,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieval=1000, Top-K=5\n",
      "   - Avg Recall   : 26.61%\n",
      "   - Avg Precision: 14.55%\n",
      "   - Avg F1       : 17.59%\n",
      "\n",
      "Best Setting: Retrieval=100, Top-K=4, F1=19.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Load dev claims\n",
    "with open(dev_claims_path, 'r') as f:\n",
    "    dev_claims = json.load(f)\n",
    "\n",
    "claim_ids = list(dev_claims.keys())\n",
    "\n",
    "retrieval_values = [100,200,500,1000]\n",
    "top_k_values = [3, 4, 5]\n",
    "\n",
    "best_f1 = 0\n",
    "best_setting = {}\n",
    "\n",
    "for retrieval in retrieval_values:\n",
    "    for top_k in top_k_values:\n",
    "        recalls = []\n",
    "        precisions = []\n",
    "        f1s = []\n",
    "\n",
    "        for cid in tqdm(claim_ids, desc=f\"Evaluating R={retrieval}, K={top_k}\"):\n",
    "            truth = set(dev_claims[cid][\"evidences\"])\n",
    "            \n",
    "            retrieved_info = retrieve_evidence(cid, dev_claims[cid], retrieval=retrieval, top_k=top_k)\n",
    "            retrieved = set(retrieved_info[\"evidences\"])\n",
    "\n",
    "            hit = len(truth & retrieved)\n",
    "\n",
    "            recall = hit / len(truth) if len(truth) > 0 else 0\n",
    "            precision = hit / top_k if top_k > 0 else 0\n",
    "\n",
    "            if precision + recall > 0:\n",
    "                f1 = 2 * precision * recall / (precision + recall)\n",
    "            else:\n",
    "                f1 = 0\n",
    "\n",
    "            recalls.append(recall)\n",
    "            precisions.append(precision)\n",
    "            f1s.append(f1)\n",
    "\n",
    "        avg_recall = np.mean(recalls)\n",
    "        avg_precision = np.mean(precisions)\n",
    "        avg_f1 = np.mean(f1s)\n",
    "\n",
    "        print(f\"\\nRetrieval={retrieval}, Top-K={top_k}\")\n",
    "        print(f\"   - Avg Recall   : {avg_recall:.2%}\")\n",
    "        print(f\"   - Avg Precision: {avg_precision:.2%}\")\n",
    "        print(f\"   - Avg F1       : {avg_f1:.2%}\")\n",
    "\n",
    "        if avg_f1 > best_f1:\n",
    "            best_f1 = avg_f1\n",
    "            best_setting = {'retrieval': retrieval, 'top_k': top_k}\n",
    "\n",
    "print(f\"\\nBest Setting: Retrieval={best_setting['retrieval']}, Top-K={best_setting['top_k']}, F1={best_f1:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0537e765",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0116b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Load dev claims\n",
    "with open(dev_claims_path, 'r') as f:\n",
    "    dev_claims = json.load(f)\n",
    "\n",
    "claim_ids = list(dev_claims.keys())\n",
    "\n",
    "retrieval = 100\n",
    "top_k = 4\n",
    "\n",
    "recalls = []\n",
    "precisions = []\n",
    "f1s = []\n",
    "\n",
    "for cid in tqdm(claim_ids, desc=f\"Evaluating R={retrieval}, K={top_k}\"):\n",
    "    truth = set(dev_claims[cid][\"evidences\"])\n",
    "    \n",
    "    retrieved_info = retrieve_evidence(cid, dev_claims[cid], retrieval=retrieval, top_k=top_k)\n",
    "    retrieved = set(retrieved_info[\"evidences\"])\n",
    "\n",
    "    hit = len(truth & retrieved)\n",
    "\n",
    "    recall = hit / len(truth) if len(truth) > 0 else 0\n",
    "    precision = hit / top_k if top_k > 0 else 0\n",
    "\n",
    "    if precision + recall > 0:\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "    else:\n",
    "        f1 = 0\n",
    "\n",
    "    recalls.append(recall)\n",
    "    precisions.append(precision)\n",
    "    f1s.append(f1)\n",
    "\n",
    "avg_recall = np.mean(recalls)\n",
    "avg_precision = np.mean(precisions)\n",
    "avg_f1 = np.mean(f1s)\n",
    "\n",
    "print(f\"\\nEvaluation Results (Retrieval={retrieval}, Top-K={top_k})\")\n",
    "print(f\"   - Avg Recall   : {avg_recall:.2%}\")\n",
    "print(f\"   - Avg Precision: {avg_precision:.2%}\")\n",
    "print(f\"   - Avg F1       : {avg_f1:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5c5688f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving Evidence: 100%|██████████| 153/153 [00:16<00:00,  9.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval with reranker completed and saved to test-claims-predictions.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load test claims data\n",
    "with open('./data/test-claims-unlabelled.json', 'r') as f:\n",
    "    test_claims = json.load(f)\n",
    "\n",
    "# Clean function\n",
    "def clean_claim(claim: str) -> str:\n",
    "    claim = claim.lower()\n",
    "    claim = re.sub(r'[^a-z0-9\\s]', '', claim)\n",
    "    claim = re.sub(r'\\s+', ' ', claim).strip()\n",
    "    return claim\n",
    "\n",
    "# Execute retrieval and save\n",
    "test_predictions = {}\n",
    "\n",
    "for claim_id, claim_data in tqdm(test_claims.items(), desc=\"Retrieving Evidence\"):\n",
    "    result = retrieve_evidence(claim_id, claim_data, retrieval=100, top_k=4)\n",
    "\n",
    "    # Remove claim_label (because test has no label)\n",
    "    if \"claim_label\" in result:\n",
    "        del result[\"claim_label\"]\n",
    "\n",
    "    test_predictions[claim_id] = result\n",
    "\n",
    "# Save the predictions\n",
    "with open(\"test-claims-predictions.json\", \"w\") as f:\n",
    "    json.dump(test_predictions, f, indent=2)\n",
    "\n",
    "print(\"Retrieval with reranker completed and saved to test-claims-predictions.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
