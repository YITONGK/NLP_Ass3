{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189afb4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import all the packages\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import Counter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import re\n",
    "from datasets import Dataset\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dd1770",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_claims_path = './data/train-claims.json'\n",
    "dev_claims_path = './data/dev-claims.json'\n",
    "evidence_path = './data/evidence.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1eb6c34",
   "metadata": {},
   "source": [
    "## è®­minilmçš„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279bf514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training pairs: 4613\n",
      "Missing evidence ids: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69b0b4bcebeb49ffacb859f38a7940a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='725' max='725' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [725/725 01:26, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.694300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Finetuned model saved.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "import random\n",
    "\n",
    "train = False\n",
    "dev_train_included = False\n",
    "if (train):\n",
    "    # åŠ è½½æ•°æ®\n",
    "    with open(train_claims_path, 'r') as f:\n",
    "        train_claims = json.load(f)\n",
    "    with open(evidence_path, 'r') as f:\n",
    "        evidence_dict = json.load(f)\n",
    "    if (dev_train_included):\n",
    "        with open(dev_claims_path,'r') as f:\n",
    "            dev_claims = json.load(f)\n",
    "\n",
    "    # æ„å»ºè®­ç»ƒæ ·æœ¬åˆ—è¡¨ï¼ˆclaim, evidence_textï¼‰ -> labelé»˜è®¤ä¸º1.0\n",
    "    train_samples = []\n",
    "    missed = 0\n",
    "\n",
    "    for claim in train_claims.values():\n",
    "        claim_text = claim['claim_text']\n",
    "        evidence_ids = claim.get('evidences', [])\n",
    "        for eid in evidence_ids:\n",
    "            if eid in evidence_dict:\n",
    "                ev_text = evidence_dict[eid]\n",
    "                train_samples.append(InputExample(texts=[claim_text, ev_text], label=1.0))\n",
    "            else:\n",
    "                missed += 1\n",
    "\n",
    "    if (dev_train_included):\n",
    "        for claim in dev_claims.values():\n",
    "            claim_text = claim['claim_text']\n",
    "            evidence_ids = claim.get('evidences', [])\n",
    "            for eid in evidence_ids:\n",
    "                if eid in evidence_dict:\n",
    "                    ev_text = evidence_dict[eid]\n",
    "                    train_samples.append(InputExample(texts=[claim_text, ev_text], label=1.0))\n",
    "                else:\n",
    "                    missed += 1\n",
    "    \n",
    "    print(f\"Total training pairs: {len(train_samples)}\")\n",
    "    print(f\"Missing evidence ids: {missed}\")\n",
    "\n",
    "    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹\n",
    "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "    # æ„å»º Dataloader\n",
    "    train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=32)\n",
    "\n",
    "    # å®šä¹‰æŸå¤±å‡½æ•°\n",
    "    train_loss = losses.MultipleNegativesRankingLoss(model)\n",
    "\n",
    "    # å¼€å§‹è®­ç»ƒ\n",
    "    model.fit(\n",
    "        train_objectives=[(train_dataloader, train_loss)],\n",
    "        epochs=5,  # å¯è®¾ä¸º 2-3ï¼Œçœ‹ä½ èµ„æº\n",
    "        warmup_steps=100,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "\n",
    "    # ä¿å­˜æ¨¡å‹\n",
    "    if (dev_train_included):\n",
    "        model.save('./model/final_finetuned_minilm_retriever')\n",
    "        print(\"âœ… Finetuned model saved.\")\n",
    "    else:\n",
    "        model.save('./model/my_finetuned_minilm_retriever')\n",
    "        print(\"âœ… Finetuned model saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2864fce1",
   "metadata": {},
   "source": [
    "# è®­msmarco rerankerçš„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694dcd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder, InputExample\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "import random\n",
    "\n",
    "train = False\n",
    "dev_train_included = False\n",
    "\n",
    "if train:\n",
    "    with open(train_claims_path, 'r') as f:\n",
    "        train_claims = json.load(f)\n",
    "    with open(evidence_path, 'r') as f:\n",
    "        evidence_dict = json.load(f)\n",
    "    if dev_train_included:\n",
    "        with open(dev_claims_path, 'r') as f:\n",
    "            dev_claims = json.load(f)\n",
    "\n",
    "    # âœ… æ„é€ æ­£è´Ÿæ ·æœ¬\n",
    "    train_samples = []\n",
    "\n",
    "    def generate_samples(claims_data):\n",
    "        samples = []\n",
    "        for claim in claims_data.values():\n",
    "            claim_text = claim[\"claim_text\"]\n",
    "            evidence_ids = claim.get(\"evidences\", [])\n",
    "            pos_evidence_texts = [evidence_dict[eid] for eid in evidence_ids if eid in evidence_dict]\n",
    "\n",
    "            # æ­£æ ·æœ¬\n",
    "            for ev in pos_evidence_texts:\n",
    "                samples.append(InputExample(texts=[claim_text, ev], label=1.0))\n",
    "\n",
    "            # è´Ÿæ ·æœ¬\n",
    "            neg_pool = [e for eid, e in evidence_dict.items() if eid not in evidence_ids]\n",
    "            for _ in range(len(pos_evidence_texts)):\n",
    "                neg_ev = random.choice(neg_pool)\n",
    "                samples.append(InputExample(texts=[claim_text, neg_ev], label=0.0))\n",
    "\n",
    "        return samples\n",
    "\n",
    "    train_samples.extend(generate_samples(train_claims))\n",
    "    if dev_train_included:\n",
    "        train_samples.extend(generate_samples(dev_claims))\n",
    "\n",
    "    print(f\"âœ… Total training samples: {len(train_samples)}\")\n",
    "\n",
    "    # âœ… æ„å»º DataLoaderï¼ˆInputExample æ˜¯åˆæ³•æ ¼å¼ï¼‰\n",
    "    train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=16)\n",
    "\n",
    "    # âœ… åŠ è½½ MS MARCO CrossEncoder\n",
    "    model = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\", num_labels=1)\n",
    "\n",
    "    # âœ… è®­ç»ƒæ¨¡å‹\n",
    "    model.fit(\n",
    "        train_dataloader=train_dataloader,\n",
    "        epochs=5,\n",
    "        warmup_steps=100,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "\n",
    "    # âœ… ä¿å­˜æ¨¡å‹\n",
    "    if (dev_train_included):\n",
    "        model.save('./model/final_finetuned_msmarco_reranker')\n",
    "        print(\"âœ… Finetuned model saved.\")\n",
    "    else:\n",
    "        model.save('./model/my_finetuned_msmarco_reranker')\n",
    "        print(\"âœ… Finetuned model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b08dc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (dev_train_included):\n",
    "    model = SentenceTransformer('./model/final_finetuned_minilm_retriever')\n",
    "    reranker =  CrossEncoder('./model/final_finetuned_msmarco_reranker')\n",
    "else:\n",
    "    model = SentenceTransformer('./model/my_finetuned_minilm_retriever')\n",
    "    reranker =  CrossEncoder('./model/my_finetuned_msmarco_reranker')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a31334",
   "metadata": {},
   "source": [
    "## encode evidence dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e2f859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "from lemminflect import getAllInflections\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "with open(train_claims_path, 'r') as f:\n",
    "    train_claims = json.load(f)\n",
    "\n",
    "with open(evidence_path, 'r') as f:\n",
    "    evidence_dict = json.load(f)\n",
    "\n",
    "# === ä» claim_text ä¸­æå–æ‰€æœ‰åè¯å¹¶ç»Ÿè®¡è¯é¢‘ ===\n",
    "all_nouns = []\n",
    "for claim_obj in train_claims.values():\n",
    "    doc = nlp(claim_obj[\"claim_text\"])\n",
    "    nouns = [token.lemma_.lower() for token in doc if token.pos_ == \"NOUN\"]\n",
    "    all_nouns.extend(nouns)\n",
    "\n",
    "# === é€‰å‡º top 100 é«˜é¢‘åè¯ä½œä¸ºå…³é”®è¯ ===\n",
    "top_keywords = set(word for word, _ in Counter(all_nouns).most_common(100))\n",
    "\n",
    "all_forms = set()\n",
    "for lemma in top_keywords:\n",
    "  all_forms.add(lemma)\n",
    "  # æ‹¿åˆ°æ‰€æœ‰å¯èƒ½çš„åè¯å½¢å¼\n",
    "  infl_map = getAllInflections(lemma, upos=\"NOUN\")\n",
    "  # infl_map æ˜¯ä¸ª dictï¼š{ 'NNS': ['cats'], 'NNPS': ['children'], ... }\n",
    "  for forms in infl_map.values():\n",
    "      all_forms.update(forms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fe2442",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_climate_keywords(text: str, all_forms: set) -> bool:\n",
    "    # ç›´æ¥æŠŠæ–‡æœ¬å°å†™åŒ–ã€æ‹†æˆâ€œè¯â€åæŸ¥é›†åˆ\n",
    "    words = re.findall(r\"\\b[a-z']+\\b\", text.lower())\n",
    "    return any(word in all_forms for word in words)\n",
    "\n",
    "\n",
    "def is_english(text: str, threshold: float = 0.5) -> bool:\n",
    "  # æ¸…ç†æ–‡æœ¬ï¼Œä»…ä¿ç•™å­—æ¯å’Œç©ºæ ¼\n",
    "  text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "  if len(text) == 0:  # å¦‚æœæ–‡æœ¬æ¸…ç†åä¸ºç©ºï¼Œè¿”å›False\n",
    "    return False\n",
    "  # è®¡ç®—è‹±æ–‡å­—ç¬¦å æ¯”\n",
    "  english_char_count = sum(1 for char in text if char.isalpha())\n",
    "  return (english_char_count / len(text)) >= threshold\n",
    "\n",
    "def clean_and_split(eid, text):\n",
    "  result_ids = []\n",
    "  result_texts = []\n",
    "  sentences = sent_tokenize(text)\n",
    "  for i, sent in enumerate(sentences):\n",
    "    sent = sent.lower()\n",
    "    sent = re.sub(r'[^a-z0-9\\s.,!?]', '', sent)  # å»é™¤æ ‡ç‚¹\n",
    "    sent = re.sub(r'\\s+', ' ', sent).strip()\n",
    "    if len(sent.split()) >= 5:  # å¯é€‰ï¼šè¿‡æ»¤å¤ªçŸ­çš„æ–‡æœ¬\n",
    "      result_ids.append(f\"{eid}_s{i}\")  # ç”¨åŸå§‹çš„ eID åŠ ä¸Šå¥å­ç´¢å¼•\n",
    "      result_texts.append(sent)\n",
    "  return result_ids, result_texts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7821493a",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = False # Variable to control whether encode the evidence\n",
    "# Load the sentence-BERT (all-MiniLM-L6-v2)\n",
    "if (dev_train_included):\n",
    "  word_embedding_path = './word_embedding/final_evidence_embeddings.npy'\n",
    "  word_embedding_meta_path = \"./word_embedding/final_evidence_meta.pkl\"\n",
    "else: \n",
    "  word_embedding_path = './word_embedding/evidence_embeddings.npy'\n",
    "  word_embedding_meta_path = \"./word_embedding/evidence_meta.pkl\"\n",
    "\n",
    "if (encode):\n",
    "  with open(evidence_path, 'r') as f:\n",
    "    evidence_dict = json.load(f)\n",
    "  # â€”â€”â€” 1. æ‰¹é‡å‰”é™¤éè‹±æ–‡ â€”â€” #\n",
    "  eids  = list(evidence_dict.keys())\n",
    "  texts = list(evidence_dict.values())\n",
    "  english_pairs = [\n",
    "    (eid, txt)\n",
    "    for eid, txt in zip(eids, texts)\n",
    "    if is_english(txt)\n",
    "  ]\n",
    "  print(f\"Step1: English keep {len(english_pairs)}/{len(texts)}\")\n",
    "\n",
    "  # â€”â€”â€” 2. æ‰¹é‡å‰”é™¤é climate-related â€”â€” #\n",
    "  climate_pairs = [\n",
    "    (eid, txt)\n",
    "    for eid, txt in english_pairs\n",
    "    if contains_climate_keywords(txt, all_forms)\n",
    "  ]\n",
    "  print(f\"Step2: Climate-related keep {len(climate_pairs)}/{len(english_pairs)}\")\n",
    "\n",
    "\n",
    "  # æ¸…æ´—å’Œåˆ†å¥åçš„ç»“æœ\n",
    "  cleaned_evidence_ids = []\n",
    "  cleaned_evidence_texts = []\n",
    "  original_evidence_ids = []  # è®°å½•æ¯ä¸ªåˆ†å¥æ‰€å±çš„åŸå§‹ evidence_id\n",
    "\n",
    "  # éå† evidence æ•°æ®å¹¶è¿›è¡Œæ¸…æ´—å’Œåˆ†å¥\n",
    "  for eid, text in climate_pairs:\n",
    "    cleaned_ids, cleaned_texts = clean_and_split(eid, text)\n",
    "    cleaned_evidence_ids.extend(cleaned_ids)\n",
    "    cleaned_evidence_texts.extend(cleaned_texts)\n",
    "    original_evidence_ids.extend([eid] * len(cleaned_ids))  # æ¯ä¸ªåˆ†å¥éƒ½è®°å½•åŸå§‹çš„ eID\n",
    "\n",
    "  # ä½¿ç”¨ Sentence-BERT å¯¹æ¸…æ´—åçš„åˆ†å¥è¿›è¡Œç¼–ç \n",
    "  evidence_embeddings = model.encode(\n",
    "    cleaned_evidence_texts,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True,\n",
    "    show_progress_bar=True\n",
    "  )\n",
    "\n",
    "  # ä¿å­˜ç¼–ç åçš„åµŒå…¥ï¼ˆembeddingsï¼‰\n",
    "  np.save(word_embedding_path, evidence_embeddings)\n",
    "\n",
    "  # ä¿å­˜åˆ†å¥åçš„ evidence_ids å’Œ texts ä»¥åŠå¯¹åº”çš„åŸå§‹ evidence_id æ˜ å°„\n",
    "  with open(word_embedding_meta_path, \"wb\") as f:\n",
    "      pickle.dump((cleaned_evidence_ids, cleaned_evidence_texts, original_evidence_ids), f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bef490",
   "metadata": {},
   "source": [
    "## test and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daca0134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load numpy embeddings\n",
    "evidence_embeddings = np.load(word_embedding_path)\n",
    "\n",
    "# Load evidence_ids, evidence_texts, and original_evidence_ids\n",
    "with open(word_embedding_meta_path, \"rb\") as f:\n",
    "  evidence_ids, evidence_texts, original_evidence_ids = pickle.load(f)\n",
    "\n",
    "\n",
    "dimension = evidence_embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)  # Inner Product = Cosine similarity if normalized\n",
    "index.add(evidence_embeddings)\n",
    "\n",
    "\n",
    "def clean_claim(claim: str) -> str:\n",
    "  # å°å†™åŒ–\n",
    "  claim = claim.lower()\n",
    "  # å»é™¤æ ‡ç‚¹ç¬¦å·\n",
    "  claim = re.sub(r'[^a-z0-9\\s]', '', claim)\n",
    "  # å»é™¤å¤šä½™ç©ºæ ¼\n",
    "  claim = re.sub(r'\\s+', ' ', claim).strip()\n",
    "  # å¦‚æœéœ€è¦ï¼Œä½ ä¹Ÿå¯ä»¥æ·»åŠ å»é™¤åœç”¨è¯çš„æ­¥éª¤\n",
    "  # claim = \" \".join([word for word in claim.split() if word not in stopwords])\n",
    "  return claim\n",
    "\n",
    "# Create mapping\n",
    "evidence_dict = dict(zip(evidence_ids, evidence_texts))\n",
    "\n",
    "with open(evidence_path, 'r') as f:\n",
    "    original_evidence_dict = json.load(f)\n",
    "\n",
    "def retrieve_evidence(claim_id, claim_data, retrieval=100, top_k=5):\n",
    "    claim_text = claim_data[\"claim_text\"]\n",
    "    cleaned_claim = clean_claim(claim_text)\n",
    "\n",
    "    # Step 1: ç²—æ£€ç´¢ï¼ˆmodel + FAISSï¼‰\n",
    "    claim_embedding = model.encode([cleaned_claim], convert_to_numpy=True, normalize_embeddings=True)\n",
    "    scores, indices = index.search(claim_embedding, retrieval * 3)\n",
    "\n",
    "    # Step 2: å»é‡å€™é€‰\n",
    "    seen_original_ids = set()\n",
    "    candidates = []\n",
    "    for i in indices[0]:\n",
    "        eid = evidence_ids[i]\n",
    "        text = evidence_dict[eid]\n",
    "        original_id = original_evidence_ids[i]\n",
    "\n",
    "        if original_id not in seen_original_ids:\n",
    "            candidates.append((original_id, eid, text))\n",
    "            seen_original_ids.add(original_id)\n",
    "\n",
    "        if len(candidates) >= retrieval:\n",
    "            break\n",
    "\n",
    "    # Step 3: ç²¾æ’åºï¼ˆCrossEncoderï¼‰\n",
    "    pairs = [(claim_text, original_evidence_dict[orig_id]) for (orig_id, _, _) in candidates]  # âœ…\n",
    "    similarity_scores = reranker.predict(pairs)\n",
    "\n",
    "    reranked = sorted(zip(candidates, similarity_scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # âœ… åªè¿”å›åŸå§‹ document-level evidence ID\n",
    "    top_k_original_ids = [orig_id for (orig_id, _, _), _ in reranked[:top_k]]\n",
    "\n",
    "    # âœ… æ„é€ æœ€ç»ˆç»“æœ dict\n",
    "    result = {\n",
    "        \"claim_text\": claim_text,\n",
    "        \"evidences\": top_k_original_ids\n",
    "    }\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9843a67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 154/154 [00:17<00:00,  9.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Average Recall@4:    26.36%\n",
      "ğŸ“Š Average Precision@4: 18.02%\n",
      "ğŸ“Š Average F1@4:        19.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# åŠ è½½ dev claims\n",
    "with open(dev_claims_path, 'r') as f:\n",
    "    dev_claims = json.load(f)\n",
    "\n",
    "claim_ids = list(dev_claims.keys())\n",
    "top_k = 4\n",
    "\n",
    "recalls = []\n",
    "precisions = []\n",
    "f1s = []\n",
    "\n",
    "for cid in tqdm(claim_ids, desc=\"Evaluating\"):\n",
    "    truth = set(dev_claims[cid][\"evidences\"])\n",
    "    \n",
    "    # âœ… æ–°ç‰ˆ retrieve_evidence è¿”å› dict\n",
    "    retrieved_info = retrieve_evidence(cid, dev_claims[cid], retrieval=100, top_k=top_k)\n",
    "    retrieved = set(retrieved_info[\"evidences\"])\n",
    "\n",
    "    hit = len(truth & retrieved)\n",
    "\n",
    "    # Recall\n",
    "    recall = hit / len(truth) if len(truth) > 0 else 0\n",
    "    recalls.append(recall)\n",
    "\n",
    "    # Precision\n",
    "    precision = hit / top_k if top_k > 0 else 0\n",
    "    precisions.append(precision)\n",
    "\n",
    "    # F1\n",
    "    if precision + recall > 0:\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "    else:\n",
    "        f1 = 0\n",
    "    f1s.append(f1)\n",
    "\n",
    "# è¾“å‡ºå¹³å‡æŒ‡æ ‡\n",
    "print(f\"\\nğŸ“Š Average Recall@{top_k}:    {np.mean(recalls):.2%}\")\n",
    "print(f\"ğŸ“Š Average Precision@{top_k}: {np.mean(precisions):.2%}\")\n",
    "print(f\"ğŸ“Š Average F1@{top_k}:        {np.mean(f1s):.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c5688f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving Evidence: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 153/153 [00:16<00:00,  9.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Retrieval with reranker completed and saved to test-claims-predictions.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# åŠ è½½ test claims æ•°æ®\n",
    "with open('./data/test-claims-unlabelled.json', 'r') as f:\n",
    "    test_claims = json.load(f)\n",
    "\n",
    "# æ¸…æ´—å‡½æ•°\n",
    "def clean_claim(claim: str) -> str:\n",
    "    claim = claim.lower()\n",
    "    claim = re.sub(r'[^a-z0-9\\s]', '', claim)\n",
    "    claim = re.sub(r'\\s+', ' ', claim).strip()\n",
    "    return claim\n",
    "\n",
    "# âœ… æ‰§è¡Œæ£€ç´¢å¹¶ä¿å­˜\n",
    "test_predictions = {}\n",
    "\n",
    "for claim_id, claim_data in tqdm(test_claims.items(), desc=\"Retrieving Evidence\"):\n",
    "    result = retrieve_evidence(claim_id, claim_data, retrieval=100, top_k=4)\n",
    "\n",
    "    # å»æ‰ claim_labelï¼ˆå› ä¸º test ä¸­æ—  labelï¼‰\n",
    "    if \"claim_label\" in result:\n",
    "        del result[\"claim_label\"]\n",
    "\n",
    "    test_predictions[claim_id] = result\n",
    "\n",
    "# âœ… ä¿å­˜é¢„æµ‹ç»“æœ\n",
    "with open(\"test-claims-predictions.json\", \"w\") as f:\n",
    "    json.dump(test_predictions, f, indent=2)\n",
    "\n",
    "print(\"âœ… Retrieval with reranker completed and saved to test-claims-predictions.json\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
