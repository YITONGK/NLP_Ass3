{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189afb4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import all the packages\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import Counter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import re\n",
    "from datasets import Dataset\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dd1770",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_claims_path = './data/train-claims.json'\n",
    "dev_claims_path = './data/dev-claims.json'\n",
    "evidence_path = './data/evidence.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1eb6c34",
   "metadata": {},
   "source": [
    "## 训minilm的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279bf514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training pairs: 4613\n",
      "Missing evidence ids: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69b0b4bcebeb49ffacb859f38a7940a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='725' max='725' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [725/725 01:26, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.694300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finetuned model saved.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "import random\n",
    "\n",
    "train = False\n",
    "dev_train_included = False\n",
    "if (train):\n",
    "    # 加载数据\n",
    "    with open(train_claims_path, 'r') as f:\n",
    "        train_claims = json.load(f)\n",
    "    with open(evidence_path, 'r') as f:\n",
    "        evidence_dict = json.load(f)\n",
    "    if (dev_train_included):\n",
    "        with open(dev_claims_path,'r') as f:\n",
    "            dev_claims = json.load(f)\n",
    "\n",
    "    # 构建训练样本列表（claim, evidence_text） -> label默认为1.0\n",
    "    train_samples = []\n",
    "    missed = 0\n",
    "\n",
    "    for claim in train_claims.values():\n",
    "        claim_text = claim['claim_text']\n",
    "        evidence_ids = claim.get('evidences', [])\n",
    "        for eid in evidence_ids:\n",
    "            if eid in evidence_dict:\n",
    "                ev_text = evidence_dict[eid]\n",
    "                train_samples.append(InputExample(texts=[claim_text, ev_text], label=1.0))\n",
    "            else:\n",
    "                missed += 1\n",
    "\n",
    "    if (dev_train_included):\n",
    "        for claim in dev_claims.values():\n",
    "            claim_text = claim['claim_text']\n",
    "            evidence_ids = claim.get('evidences', [])\n",
    "            for eid in evidence_ids:\n",
    "                if eid in evidence_dict:\n",
    "                    ev_text = evidence_dict[eid]\n",
    "                    train_samples.append(InputExample(texts=[claim_text, ev_text], label=1.0))\n",
    "                else:\n",
    "                    missed += 1\n",
    "    \n",
    "    print(f\"Total training pairs: {len(train_samples)}\")\n",
    "    print(f\"Missing evidence ids: {missed}\")\n",
    "\n",
    "    # 加载预训练模型\n",
    "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "    # 构建 Dataloader\n",
    "    train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=32)\n",
    "\n",
    "    # 定义损失函数\n",
    "    train_loss = losses.MultipleNegativesRankingLoss(model)\n",
    "\n",
    "    # 开始训练\n",
    "    model.fit(\n",
    "        train_objectives=[(train_dataloader, train_loss)],\n",
    "        epochs=5,  # 可设为 2-3，看你资源\n",
    "        warmup_steps=100,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "\n",
    "    # 保存模型\n",
    "    if (dev_train_included):\n",
    "        model.save('./model/final_finetuned_minilm_retriever')\n",
    "        print(\"✅ Finetuned model saved.\")\n",
    "    else:\n",
    "        model.save('./model/my_finetuned_minilm_retriever')\n",
    "        print(\"✅ Finetuned model saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2864fce1",
   "metadata": {},
   "source": [
    "# 训msmarco reranker的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694dcd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder, InputExample\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "import random\n",
    "\n",
    "train = False\n",
    "dev_train_included = False\n",
    "\n",
    "if train:\n",
    "    with open(train_claims_path, 'r') as f:\n",
    "        train_claims = json.load(f)\n",
    "    with open(evidence_path, 'r') as f:\n",
    "        evidence_dict = json.load(f)\n",
    "    if dev_train_included:\n",
    "        with open(dev_claims_path, 'r') as f:\n",
    "            dev_claims = json.load(f)\n",
    "\n",
    "    # ✅ 构造正负样本\n",
    "    train_samples = []\n",
    "\n",
    "    def generate_samples(claims_data):\n",
    "        samples = []\n",
    "        for claim in claims_data.values():\n",
    "            claim_text = claim[\"claim_text\"]\n",
    "            evidence_ids = claim.get(\"evidences\", [])\n",
    "            pos_evidence_texts = [evidence_dict[eid] for eid in evidence_ids if eid in evidence_dict]\n",
    "\n",
    "            # 正样本\n",
    "            for ev in pos_evidence_texts:\n",
    "                samples.append(InputExample(texts=[claim_text, ev], label=1.0))\n",
    "\n",
    "            # 负样本\n",
    "            neg_pool = [e for eid, e in evidence_dict.items() if eid not in evidence_ids]\n",
    "            for _ in range(len(pos_evidence_texts)):\n",
    "                neg_ev = random.choice(neg_pool)\n",
    "                samples.append(InputExample(texts=[claim_text, neg_ev], label=0.0))\n",
    "\n",
    "        return samples\n",
    "\n",
    "    train_samples.extend(generate_samples(train_claims))\n",
    "    if dev_train_included:\n",
    "        train_samples.extend(generate_samples(dev_claims))\n",
    "\n",
    "    print(f\"✅ Total training samples: {len(train_samples)}\")\n",
    "\n",
    "    # ✅ 构建 DataLoader（InputExample 是合法格式）\n",
    "    train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=16)\n",
    "\n",
    "    # ✅ 加载 MS MARCO CrossEncoder\n",
    "    model = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\", num_labels=1)\n",
    "\n",
    "    # ✅ 训练模型\n",
    "    model.fit(\n",
    "        train_dataloader=train_dataloader,\n",
    "        epochs=5,\n",
    "        warmup_steps=100,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "\n",
    "    # ✅ 保存模型\n",
    "    if (dev_train_included):\n",
    "        model.save('./model/final_finetuned_msmarco_reranker')\n",
    "        print(\"✅ Finetuned model saved.\")\n",
    "    else:\n",
    "        model.save('./model/my_finetuned_msmarco_reranker')\n",
    "        print(\"✅ Finetuned model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b08dc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (dev_train_included):\n",
    "    model = SentenceTransformer('./model/final_finetuned_minilm_retriever')\n",
    "    reranker =  CrossEncoder('./model/final_finetuned_msmarco_reranker')\n",
    "else:\n",
    "    model = SentenceTransformer('./model/my_finetuned_minilm_retriever')\n",
    "    reranker =  CrossEncoder('./model/my_finetuned_msmarco_reranker')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a31334",
   "metadata": {},
   "source": [
    "## encode evidence dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e2f859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "from lemminflect import getAllInflections\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "with open(train_claims_path, 'r') as f:\n",
    "    train_claims = json.load(f)\n",
    "\n",
    "with open(evidence_path, 'r') as f:\n",
    "    evidence_dict = json.load(f)\n",
    "\n",
    "# === 从 claim_text 中提取所有名词并统计词频 ===\n",
    "all_nouns = []\n",
    "for claim_obj in train_claims.values():\n",
    "    doc = nlp(claim_obj[\"claim_text\"])\n",
    "    nouns = [token.lemma_.lower() for token in doc if token.pos_ == \"NOUN\"]\n",
    "    all_nouns.extend(nouns)\n",
    "\n",
    "# === 选出 top 100 高频名词作为关键词 ===\n",
    "top_keywords = set(word for word, _ in Counter(all_nouns).most_common(100))\n",
    "\n",
    "all_forms = set()\n",
    "for lemma in top_keywords:\n",
    "  all_forms.add(lemma)\n",
    "  # 拿到所有可能的名词形式\n",
    "  infl_map = getAllInflections(lemma, upos=\"NOUN\")\n",
    "  # infl_map 是个 dict：{ 'NNS': ['cats'], 'NNPS': ['children'], ... }\n",
    "  for forms in infl_map.values():\n",
    "      all_forms.update(forms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fe2442",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_climate_keywords(text: str, all_forms: set) -> bool:\n",
    "    # 直接把文本小写化、拆成“词”后查集合\n",
    "    words = re.findall(r\"\\b[a-z']+\\b\", text.lower())\n",
    "    return any(word in all_forms for word in words)\n",
    "\n",
    "\n",
    "def is_english(text: str, threshold: float = 0.5) -> bool:\n",
    "  # 清理文本，仅保留字母和空格\n",
    "  text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "  if len(text) == 0:  # 如果文本清理后为空，返回False\n",
    "    return False\n",
    "  # 计算英文字符占比\n",
    "  english_char_count = sum(1 for char in text if char.isalpha())\n",
    "  return (english_char_count / len(text)) >= threshold\n",
    "\n",
    "def clean_and_split(eid, text):\n",
    "  result_ids = []\n",
    "  result_texts = []\n",
    "  sentences = sent_tokenize(text)\n",
    "  for i, sent in enumerate(sentences):\n",
    "    sent = sent.lower()\n",
    "    sent = re.sub(r'[^a-z0-9\\s.,!?]', '', sent)  # 去除标点\n",
    "    sent = re.sub(r'\\s+', ' ', sent).strip()\n",
    "    if len(sent.split()) >= 5:  # 可选：过滤太短的文本\n",
    "      result_ids.append(f\"{eid}_s{i}\")  # 用原始的 eID 加上句子索引\n",
    "      result_texts.append(sent)\n",
    "  return result_ids, result_texts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7821493a",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = False # Variable to control whether encode the evidence\n",
    "# Load the sentence-BERT (all-MiniLM-L6-v2)\n",
    "if (dev_train_included):\n",
    "  word_embedding_path = './word_embedding/final_evidence_embeddings.npy'\n",
    "  word_embedding_meta_path = \"./word_embedding/final_evidence_meta.pkl\"\n",
    "else: \n",
    "  word_embedding_path = './word_embedding/evidence_embeddings.npy'\n",
    "  word_embedding_meta_path = \"./word_embedding/evidence_meta.pkl\"\n",
    "\n",
    "if (encode):\n",
    "  with open(evidence_path, 'r') as f:\n",
    "    evidence_dict = json.load(f)\n",
    "  # ——— 1. 批量剔除非英文 —— #\n",
    "  eids  = list(evidence_dict.keys())\n",
    "  texts = list(evidence_dict.values())\n",
    "  english_pairs = [\n",
    "    (eid, txt)\n",
    "    for eid, txt in zip(eids, texts)\n",
    "    if is_english(txt)\n",
    "  ]\n",
    "  print(f\"Step1: English keep {len(english_pairs)}/{len(texts)}\")\n",
    "\n",
    "  # ——— 2. 批量剔除非 climate-related —— #\n",
    "  climate_pairs = [\n",
    "    (eid, txt)\n",
    "    for eid, txt in english_pairs\n",
    "    if contains_climate_keywords(txt, all_forms)\n",
    "  ]\n",
    "  print(f\"Step2: Climate-related keep {len(climate_pairs)}/{len(english_pairs)}\")\n",
    "\n",
    "\n",
    "  # 清洗和分句后的结果\n",
    "  cleaned_evidence_ids = []\n",
    "  cleaned_evidence_texts = []\n",
    "  original_evidence_ids = []  # 记录每个分句所属的原始 evidence_id\n",
    "\n",
    "  # 遍历 evidence 数据并进行清洗和分句\n",
    "  for eid, text in climate_pairs:\n",
    "    cleaned_ids, cleaned_texts = clean_and_split(eid, text)\n",
    "    cleaned_evidence_ids.extend(cleaned_ids)\n",
    "    cleaned_evidence_texts.extend(cleaned_texts)\n",
    "    original_evidence_ids.extend([eid] * len(cleaned_ids))  # 每个分句都记录原始的 eID\n",
    "\n",
    "  # 使用 Sentence-BERT 对清洗后的分句进行编码\n",
    "  evidence_embeddings = model.encode(\n",
    "    cleaned_evidence_texts,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True,\n",
    "    show_progress_bar=True\n",
    "  )\n",
    "\n",
    "  # 保存编码后的嵌入（embeddings）\n",
    "  np.save(word_embedding_path, evidence_embeddings)\n",
    "\n",
    "  # 保存分句后的 evidence_ids 和 texts 以及对应的原始 evidence_id 映射\n",
    "  with open(word_embedding_meta_path, \"wb\") as f:\n",
    "      pickle.dump((cleaned_evidence_ids, cleaned_evidence_texts, original_evidence_ids), f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bef490",
   "metadata": {},
   "source": [
    "## test and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daca0134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load numpy embeddings\n",
    "evidence_embeddings = np.load(word_embedding_path)\n",
    "\n",
    "# Load evidence_ids, evidence_texts, and original_evidence_ids\n",
    "with open(word_embedding_meta_path, \"rb\") as f:\n",
    "  evidence_ids, evidence_texts, original_evidence_ids = pickle.load(f)\n",
    "\n",
    "\n",
    "dimension = evidence_embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)  # Inner Product = Cosine similarity if normalized\n",
    "index.add(evidence_embeddings)\n",
    "\n",
    "\n",
    "def clean_claim(claim: str) -> str:\n",
    "  # 小写化\n",
    "  claim = claim.lower()\n",
    "  # 去除标点符号\n",
    "  claim = re.sub(r'[^a-z0-9\\s]', '', claim)\n",
    "  # 去除多余空格\n",
    "  claim = re.sub(r'\\s+', ' ', claim).strip()\n",
    "  # 如果需要，你也可以添加去除停用词的步骤\n",
    "  # claim = \" \".join([word for word in claim.split() if word not in stopwords])\n",
    "  return claim\n",
    "\n",
    "# Create mapping\n",
    "evidence_dict = dict(zip(evidence_ids, evidence_texts))\n",
    "\n",
    "with open(evidence_path, 'r') as f:\n",
    "    original_evidence_dict = json.load(f)\n",
    "\n",
    "def retrieve_evidence(claim_id, claim_data, retrieval=100, top_k=5):\n",
    "    claim_text = claim_data[\"claim_text\"]\n",
    "    cleaned_claim = clean_claim(claim_text)\n",
    "\n",
    "    # Step 1: 粗检索（model + FAISS）\n",
    "    claim_embedding = model.encode([cleaned_claim], convert_to_numpy=True, normalize_embeddings=True)\n",
    "    scores, indices = index.search(claim_embedding, retrieval * 3)\n",
    "\n",
    "    # Step 2: 去重候选\n",
    "    seen_original_ids = set()\n",
    "    candidates = []\n",
    "    for i in indices[0]:\n",
    "        eid = evidence_ids[i]\n",
    "        text = evidence_dict[eid]\n",
    "        original_id = original_evidence_ids[i]\n",
    "\n",
    "        if original_id not in seen_original_ids:\n",
    "            candidates.append((original_id, eid, text))\n",
    "            seen_original_ids.add(original_id)\n",
    "\n",
    "        if len(candidates) >= retrieval:\n",
    "            break\n",
    "\n",
    "    # Step 3: 精排序（CrossEncoder）\n",
    "    pairs = [(claim_text, original_evidence_dict[orig_id]) for (orig_id, _, _) in candidates]  # ✅\n",
    "    similarity_scores = reranker.predict(pairs)\n",
    "\n",
    "    reranked = sorted(zip(candidates, similarity_scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # ✅ 只返回原始 document-level evidence ID\n",
    "    top_k_original_ids = [orig_id for (orig_id, _, _), _ in reranked[:top_k]]\n",
    "\n",
    "    # ✅ 构造最终结果 dict\n",
    "    result = {\n",
    "        \"claim_text\": claim_text,\n",
    "        \"evidences\": top_k_original_ids\n",
    "    }\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9843a67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 154/154 [00:17<00:00,  9.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Average Recall@4:    26.36%\n",
      "📊 Average Precision@4: 18.02%\n",
      "📊 Average F1@4:        19.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# 加载 dev claims\n",
    "with open(dev_claims_path, 'r') as f:\n",
    "    dev_claims = json.load(f)\n",
    "\n",
    "claim_ids = list(dev_claims.keys())\n",
    "top_k = 4\n",
    "\n",
    "recalls = []\n",
    "precisions = []\n",
    "f1s = []\n",
    "\n",
    "for cid in tqdm(claim_ids, desc=\"Evaluating\"):\n",
    "    truth = set(dev_claims[cid][\"evidences\"])\n",
    "    \n",
    "    # ✅ 新版 retrieve_evidence 返回 dict\n",
    "    retrieved_info = retrieve_evidence(cid, dev_claims[cid], retrieval=100, top_k=top_k)\n",
    "    retrieved = set(retrieved_info[\"evidences\"])\n",
    "\n",
    "    hit = len(truth & retrieved)\n",
    "\n",
    "    # Recall\n",
    "    recall = hit / len(truth) if len(truth) > 0 else 0\n",
    "    recalls.append(recall)\n",
    "\n",
    "    # Precision\n",
    "    precision = hit / top_k if top_k > 0 else 0\n",
    "    precisions.append(precision)\n",
    "\n",
    "    # F1\n",
    "    if precision + recall > 0:\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "    else:\n",
    "        f1 = 0\n",
    "    f1s.append(f1)\n",
    "\n",
    "# 输出平均指标\n",
    "print(f\"\\n📊 Average Recall@{top_k}:    {np.mean(recalls):.2%}\")\n",
    "print(f\"📊 Average Precision@{top_k}: {np.mean(precisions):.2%}\")\n",
    "print(f\"📊 Average F1@{top_k}:        {np.mean(f1s):.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c5688f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving Evidence: 100%|██████████| 153/153 [00:16<00:00,  9.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Retrieval with reranker completed and saved to test-claims-predictions.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 加载 test claims 数据\n",
    "with open('./data/test-claims-unlabelled.json', 'r') as f:\n",
    "    test_claims = json.load(f)\n",
    "\n",
    "# 清洗函数\n",
    "def clean_claim(claim: str) -> str:\n",
    "    claim = claim.lower()\n",
    "    claim = re.sub(r'[^a-z0-9\\s]', '', claim)\n",
    "    claim = re.sub(r'\\s+', ' ', claim).strip()\n",
    "    return claim\n",
    "\n",
    "# ✅ 执行检索并保存\n",
    "test_predictions = {}\n",
    "\n",
    "for claim_id, claim_data in tqdm(test_claims.items(), desc=\"Retrieving Evidence\"):\n",
    "    result = retrieve_evidence(claim_id, claim_data, retrieval=100, top_k=4)\n",
    "\n",
    "    # 去掉 claim_label（因为 test 中无 label）\n",
    "    if \"claim_label\" in result:\n",
    "        del result[\"claim_label\"]\n",
    "\n",
    "    test_predictions[claim_id] = result\n",
    "\n",
    "# ✅ 保存预测结果\n",
    "with open(\"test-claims-predictions.json\", \"w\") as f:\n",
    "    json.dump(test_predictions, f, indent=2)\n",
    "\n",
    "print(\"✅ Retrieval with reranker completed and saved to test-claims-predictions.json\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
