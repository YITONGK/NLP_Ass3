{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3687cea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import random\n",
    "import statistics\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e535a110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          split  # claims avg #evidence                                                         label distribution\n",
      "          train      1228          3.36 {'SUPPORTS': 519, 'NOT_ENOUGH_INFO': 386, 'REFUTES': 199, 'DISPUTED': 124}\n",
      "            dev       154          3.19     {'SUPPORTS': 68, 'NOT_ENOUGH_INFO': 41, 'REFUTES': 27, 'DISPUTED': 18}\n",
      "           test       153             0                                                                         {}\n",
      "evidence-corpus   1208827   19.7 tokens                                                                          -\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"data\"\n",
    "\n",
    "def load_json(fname):\n",
    "    path = os.path.join(DATA_DIR, fname)\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"[WARN] {path} not found, skip.\")\n",
    "        return None\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "train     = load_json(\"train-claims.json\")\n",
    "dev       = load_json(\"dev-claims.json\")\n",
    "test      = load_json(\"test-claims-unlabelled.json\")\n",
    "evidence  = load_json(\"evidence.json\")\n",
    "\n",
    "def claim_stats(claim_dict, split_name):\n",
    "    if claim_dict is None:\n",
    "        return {\"split\": split_name, \"n_claims\": 0}\n",
    "\n",
    "    n_claims   = len(claim_dict)\n",
    "    labels     = [v.get(\"claim_label\") for v in claim_dict.values() if \"claim_label\" in v]\n",
    "    ev_per_c   = [len(v.get(\"evidences\", [])) for v in claim_dict.values()]\n",
    "    return {\n",
    "        \"split\": split_name,\n",
    "        \"# claims\": n_claims,\n",
    "        \"avg #evidence\": round(statistics.mean(ev_per_c), 2) if ev_per_c else 0,\n",
    "        \"label distribution\": pd.Series(labels).value_counts().to_dict() if labels else {},\n",
    "    }\n",
    "\n",
    "summary = [\n",
    "    claim_stats(train, \"train\"),\n",
    "    claim_stats(dev,   \"dev\"),\n",
    "    claim_stats(test,  \"test\")\n",
    "]\n",
    "\n",
    "if evidence is not None:\n",
    "    token_lens = [len(passage.split()) for passage in evidence.values()]\n",
    "    summary.append({\n",
    "        \"split\": \"evidence-corpus\",\n",
    "        \"# claims\": len(evidence),     \n",
    "        \"avg #evidence\": f\"{statistics.mean(token_lens):.1f} tokens\",                  \n",
    "        \"label distribution\": \"-\",                               \n",
    "    })\n",
    "\n",
    "print(pd.DataFrame(summary).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce442b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached data loaded in 4.00 s – ready to use.\n",
      "No valid cache – preprocessing will start …\n",
      "Tokenising 1,208,827 evidence passages with 9 CPU process(es)…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stemming evidence: 100%|██████████| 1208827/1208827 [07:15<00:00, 2775.40doc/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidence saved → /Users/felikskong/Desktop/NLP/NLP_Ass3/preprocessed/evidence_stemmed.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stemming train-claims.json: 100%|██████████| 1228/1228 [00:43<00:00, 28.17doc/s]\n",
      "Stemming dev-claims.json: 100%|██████████| 154/154 [00:42<00:00,  3.64doc/s]\n",
      "Stemming test-claims-unlabelled.json: 100%|██████████| 153/153 [00:42<00:00,  3.59doc/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claims saved → /Users/felikskong/Desktop/NLP/NLP_Ass3/preprocessed/claims_stemmed.json\n",
      "\n",
      "=== Evidence after stemming ===\n",
      "Total passages        : 1,208,067\n",
      "Stem length (min/max) : 1 / 314\n",
      "Stem length (mean)    : 11.9\n",
      "Vocabulary size       : 510,481\n",
      "Top-20 stems          : [('the', 267774), ('it', 91702), ('he', 75901), ('also', 66963), ('in', 64095), ('state', 58250), ('born', 54299), ('first', 53537), ('one', 49589), ('new', 45425), ('year', 42117), ('play', 39752), ('american', 39704), ('includ', 39608), ('use', 39337), ('unit', 38930), ('nation', 37995), ('name', 37335), ('known', 36300), ('district', 34882)]\n",
      "\n",
      "Finished in 573.0 s – results cached for future runs.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ------------------------------------------------------------------\n",
    "DATA_DIR      = Path(\"data\")\n",
    "OUT_EVID      = Path(\"preprocessed/evidence_stemmed.json\")\n",
    "OUT_CLAIM     = Path(\"preprocessed/claims_stemmed.json\")\n",
    "FORCE_REBUILD = False                 # True → ignore cache, rebuild\n",
    "BATCH_SIZE    = 1_000                 # spaCy batch size\n",
    "NUM_PROC      = max(mp.cpu_count() - 1, 1)   # use all but 1 core\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# INITIALISE SPACY & STEMMER\n",
    "# ------------------------------------------------------------------\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
    "stemmer = PorterStemmer()\n",
    "stop_set = set(stopwords.words(\"english\")) - {\"not\", \"no\"}  # keep negations\n",
    "\n",
    "def stem_doc(doc):\n",
    "    \"\"\"spaCy Doc → list of stems (alpha tokens, no stop-words).\"\"\"\n",
    "    return [\n",
    "        stemmer.stem(tok.text)\n",
    "        for tok in doc\n",
    "        if tok.text.isalpha() and tok.text not in stop_set\n",
    "    ]\n",
    "\n",
    "def jload(path: Path):\n",
    "    with path.open(encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def jdump(obj, path: Path):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 0) LOAD CACHE (IF PRESENT)\n",
    "# ------------------------------------------------------------------\n",
    "if OUT_EVID.exists() and OUT_CLAIM.exists() and not FORCE_REBUILD:\n",
    "    t0 = time.time()\n",
    "    evidence_proc  = jload(OUT_EVID)\n",
    "    claim_proc_all = jload(OUT_CLAIM)\n",
    "    print(f\"Cached data loaded in {time.time() - t0:.2f} s – ready to use.\")\n",
    "    exit(0)\n",
    "\n",
    "print(\"No valid cache – preprocessing will start …\")\n",
    "t_start = time.time()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1) PRE-PROCESS EVIDENCE (PARALLEL)\n",
    "# ------------------------------------------------------------------\n",
    "evidence_raw = jload(DATA_DIR / \"evidence.json\")\n",
    "evid_ids     = list(evidence_raw.keys())\n",
    "evid_texts   = list(evidence_raw.values())\n",
    "\n",
    "evidence_proc = {}\n",
    "lengths = []\n",
    "\n",
    "print(f\"Tokenising {len(evid_ids):,} evidence passages \"\n",
    "      f\"with {NUM_PROC} CPU process(es)…\")\n",
    "\n",
    "for evid_id, doc in tqdm(\n",
    "        zip(evid_ids,\n",
    "            nlp.pipe(evid_texts,\n",
    "                     batch_size=BATCH_SIZE,\n",
    "                     n_process=NUM_PROC)),\n",
    "        total=len(evid_ids),\n",
    "        desc=\"Stemming evidence\",\n",
    "        unit=\"doc\"\n",
    "):\n",
    "    stems = stem_doc(doc)\n",
    "    if stems:\n",
    "        evidence_proc[evid_id] = stems\n",
    "        lengths.append(len(stems))\n",
    "\n",
    "jdump(evidence_proc, OUT_EVID)\n",
    "print(f\"Evidence saved → {OUT_EVID.resolve()}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2) PRE-PROCESS CLAIMS (PARALLEL, PER SPLIT)\n",
    "# ------------------------------------------------------------------\n",
    "claim_files = [\n",
    "    \"train-claims.json\",\n",
    "    \"dev-claims.json\",\n",
    "    \"test-claims-unlabelled.json\",\n",
    "]\n",
    "claim_proc_all = {}\n",
    "\n",
    "for fname in claim_files:\n",
    "    raw_claims = jload(DATA_DIR / fname)\n",
    "    cids  = list(raw_claims.keys())\n",
    "    texts = [raw_claims[cid][\"claim_text\"] for cid in cids]\n",
    "\n",
    "    for cid, doc in tqdm(\n",
    "            zip(cids,\n",
    "                nlp.pipe(texts,\n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         n_process=NUM_PROC)),\n",
    "            total=len(cids),\n",
    "            desc=f\"Stemming {fname}\",\n",
    "            unit=\"doc\"\n",
    "    ):\n",
    "        claim_proc_all[cid] = stem_doc(doc)\n",
    "\n",
    "jdump(claim_proc_all, OUT_CLAIM)\n",
    "print(f\"Claims saved → {OUT_CLAIM.resolve()}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3) QUICK CORPUS STATISTICS\n",
    "# ------------------------------------------------------------------\n",
    "print(\"\\n=== Evidence after stemming ===\")\n",
    "print(f\"Total passages        : {len(evidence_proc):,}\")\n",
    "print(f\"Stem length (min/max) : {min(lengths)} / {max(lengths)}\")\n",
    "print(f\"Stem length (mean)    : {statistics.mean(lengths):.1f}\")\n",
    "\n",
    "vocab = {s for toks in evidence_proc.values() for s in toks}\n",
    "print(f\"Vocabulary size       : {len(vocab):,}\")\n",
    "\n",
    "counter = collections.Counter(s for toks in evidence_proc.values() for s in toks)\n",
    "print(\"Top-20 stems          :\", counter.most_common(20))\n",
    "\n",
    "print(f\"\\nFinished in {time.time() - t_start:.1f} s – \"\n",
    "      f\"results cached for future runs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40c1f5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 33/33 [01:46<00:00,  3.22s/it, loss=0.103]\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR   = Path(\"data\")\n",
    "STEM_EVID  = Path(\"preprocessed/evidence_stemmed.json\")\n",
    "STEM_CLAIM = Path(\"preprocessed/claims_stemmed.json\")\n",
    "\n",
    "evidence = json.loads(STEM_EVID.read_text())\n",
    "claims   = json.loads(STEM_CLAIM.read_text())\n",
    "\n",
    "train_lbl = json.loads((DATA_DIR / \"train-claims.json\").read_text())\n",
    "dev_lbl   = json.loads((DATA_DIR / \"dev-claims.json\").read_text())\n",
    "MIN_FREQ = 3\n",
    "PAD, UNK = \"<PAD>\", \"<UNK>\"\n",
    "\n",
    "freq = collections.Counter(t for toks in\n",
    "                           itertools.chain(evidence.values(), claims.values())\n",
    "                           for t in toks)\n",
    "itos = [PAD, UNK] + [t for t, c in freq.items() if c >= MIN_FREQ]\n",
    "stoi = {t:i for i,t in enumerate(itos)}\n",
    "\n",
    "def numerise(tokens: List[str]) -> List[int]:\n",
    "    return [stoi.get(t, stoi[UNK]) for t in tokens]\n",
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, labelled: Dict, evid_dict: Dict):\n",
    "        evid_ids = list(evid_dict.keys())\n",
    "        self.items = []                                # (cid, pos_id, neg_id)\n",
    "\n",
    "        for cid, obj in labelled.items():\n",
    "            # keep only gold evidences that survived preprocessing\n",
    "            pos_ids = [eid for eid in obj[\"evidences\"] if eid in evid_dict]\n",
    "            if not pos_ids:           # drop the claim if none survive\n",
    "                continue\n",
    "\n",
    "            for pos_id in pos_ids:\n",
    "                neg_id = random.choice(evid_ids)\n",
    "                # guarantee positive ≠ negative\n",
    "                while neg_id == pos_id:\n",
    "                    neg_id = random.choice(evid_ids)\n",
    "                self.items.append((cid, pos_id, neg_id))\n",
    "\n",
    "        self.evid_dict = evid_dict\n",
    "\n",
    "    def __len__(self): return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cid, pos, neg = self.items[idx]\n",
    "        return (numerise(claims[cid]),\n",
    "                numerise(self.evid_dict[pos]),\n",
    "                numerise(self.evid_dict[neg]))\n",
    "\n",
    "def collate(batch):\n",
    "    def pad(seqs):\n",
    "        m = max(len(s) for s in seqs)\n",
    "        return torch.tensor([s + [0]*(m-len(s)) for s in seqs])\n",
    "    c, p, n = zip(*batch)\n",
    "    return pad(c), pad(p), pad(n)\n",
    "\n",
    "train_ds = TripletDataset(train_lbl, evidence)\n",
    "train_dl = DataLoader(train_ds, batch_size=128,\n",
    "                      shuffle=True, collate_fn=collate)\n",
    "class BiLSTMSentenceEncoder(nn.Module):\n",
    "    def __init__(self, vocab_sz, emb_dim=100, hid_dim=128):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_sz, emb_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, batch_first=True,\n",
    "                            bidirectional=True)\n",
    "    def forward(self, x):                 # x: (B, L)\n",
    "        mask = (x != 0).float()\n",
    "        out, _ = self.lstm(self.emb(x))\n",
    "        out = (out * mask.unsqueeze(-1)).sum(1) / mask.sum(1, keepdim=True)\n",
    "        out = nn.functional.normalize(out, p=2, dim=-1)   # (B, 2*hid)\n",
    "        return out\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model  = BiLSTMSentenceEncoder(len(itos)).to(device)\n",
    "optim  = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "margin = 0.3\n",
    "loss_fn = nn.MarginRankingLoss(margin=margin)\n",
    "\n",
    "for epoch in range(1):\n",
    "    model.train(); total = 0\n",
    "    pbar = tqdm(train_dl, desc=f\"Epoch {epoch+1}\")\n",
    "    for c, p, n in pbar:\n",
    "        c, p, n = (t.to(device) for t in (c, p, n))\n",
    "        vc, vp, vn = model(c), model(p), model(n)\n",
    "        pos_sim = (vc * vp).sum(1)\n",
    "        neg_sim = (vc * vn).sum(1)\n",
    "        loss = loss_fn(pos_sim, neg_sim,\n",
    "                       torch.ones_like(pos_sim, device=device))\n",
    "        optim.zero_grad(); loss.backward(); optim.step()\n",
    "        total += loss.item()\n",
    "        pbar.set_postfix(loss=total/(pbar.n+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc80ba80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding evidence: 100%|██████████| 1209/1209 [01:02<00:00, 19.39batch/s]\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "BATCH_SIZE    = 1_000\n",
    "EVID_JSON = Path(\"preprocessed\") / \"evidence_stemmed.json\"\n",
    "with open(EVID_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    evidence_proc = json.load(f)\n",
    "\n",
    "class EvidenceDataset(Dataset):\n",
    "    \"\"\"Returns (evidence_id, token_id_tensor).\"\"\"\n",
    "    def __init__(self, evid_dict, numerise_fn):\n",
    "        self.ids       = list(evid_dict.keys())\n",
    "        self.evid_dict = evid_dict\n",
    "        self.numerise  = numerise_fn\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        eid       = self.ids[idx]\n",
    "        token_ids = torch.tensor(self.numerise(self.evid_dict[eid]), dtype=torch.long)\n",
    "        return eid, token_ids\n",
    "\n",
    "def pad_collate(batch):\n",
    "    \"\"\"Pads a batch of (eid, seq) into (list_of_eids, padded_tensor).\"\"\"\n",
    "    eids, seqs = zip(*batch)\n",
    "    padded = pad_sequence(seqs, batch_first=True, padding_value=0)\n",
    "    return list(eids), padded\n",
    "\n",
    "# build the DataLoader with multiple workers\n",
    "loader = DataLoader(\n",
    "    EvidenceDataset(evidence_proc, numerise),\n",
    "    batch_size   = BATCH_SIZE,\n",
    "    shuffle      = False,\n",
    "    collate_fn   = pad_collate,\n",
    "    num_workers  = 0,        # ← single-process mode for notebooks\n",
    "    pin_memory   = torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "# encode all evidence in parallel batches\n",
    "model.to(DEVICE).eval()\n",
    "evidence_vecs = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for eids, batch in tqdm(loader, desc=\"Encoding evidence\", unit=\"batch\"):\n",
    "        # batch: (B, L)\n",
    "        batch_vec = model(batch.to(DEVICE)).cpu()  # (B, hidden*2)\n",
    "        for eid, vec in zip(eids, batch_vec):\n",
    "            evidence_vecs[eid] = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08697378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ——— Demo on one dev claim ———\n",
    "# demo_id = next(iter(dev_lbl))\n",
    "# claim_stems = claims[demo_id]             # List[str]\n",
    "# top5 = rank_evidence(claim_stems, top_k=5)\n",
    "# print(\"TOP-5 evidence IDs:\", top5)\n",
    "# print(\"Gold               :\", dev_lbl[demo_id][\"evidences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1382d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xv/d8fp_fgs1fx30rm4fs63qv1c0000gn/T/ipykernel_84049/1594526304.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  evid_vecs     = torch.load(EVID_VECS_PTH, map_location=\"cpu\")\n",
      "Evaluating: 100%|██████████| 154/154 [04:09<00:00,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recall@3:    0.00%\n",
      "Precision@3: 0.00%\n",
      "F1@3:        0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DEVICE       = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "TOP_K        = 3\n",
    "\n",
    "# 1) Load your pre-stemmed claims\n",
    "CLAIMS_JSON  = Path(\"preprocessed\") / \"claims_stemmed.json\"\n",
    "with open(CLAIMS_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    claim_proc_all = json.load(f)\n",
    "\n",
    "# 2) Load your evidence vectors\n",
    "EVID_VECS_PTH = Path(\"preprocessed\") / \"evidence_vecs.pt\"\n",
    "evid_vecs     = torch.load(EVID_VECS_PTH, map_location=\"cpu\")\n",
    "\n",
    "# 3) Make sure your model is on the same device\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# 4) rank_evidence uses that model & evid_vecs\n",
    "# def rank_evidence(claim_tokens, top_k=TOP_K):\n",
    "#     idxs = numerise(claim_tokens)\n",
    "#     x    = torch.tensor([idxs], dtype=torch.long, device=DEVICE)\n",
    "#     with torch.no_grad():\n",
    "#         v_c = model(x).cpu().squeeze(0)\n",
    "#     sims = {eid: float(torch.dot(v_c, v_e)) for eid, v_e in evid_vecs.items()}\n",
    "#     return sorted(sims, key=sims.get, reverse=True)[:top_k]\n",
    "\n",
    "def rank_evidence(claim_tokens: List[str], top_k: int = 5) -> List[str]:\n",
    "    # 1) Numericise + to DEVICE\n",
    "    idxs = numerise(claim_tokens)\n",
    "    x = torch.tensor([idxs], dtype=torch.long, device=DEVICE)\n",
    "    \n",
    "    # 2) Encode and bring back to CPU\n",
    "    with torch.no_grad():\n",
    "        v_c = model(x)         # shape (1, D) on DEVICE\n",
    "    v_c = v_c.cpu().squeeze(0)  # shape (D,) on CPU\n",
    "    \n",
    "    # 3) Compute dot-product (cosine since vectors are l2-normalised)\n",
    "    sims = {}\n",
    "    for eid, v_e in evid_vecs.items():\n",
    "        # v_e is already a CPU tensor of shape (D,)\n",
    "        sims[eid] = float(torch.dot(v_c, v_e))\n",
    "    \n",
    "    # 4) Sort by descending similarity and return top_k IDs\n",
    "    ranked = sorted(sims, key=sims.get, reverse=True)[:top_k]\n",
    "    return ranked\n",
    "\n",
    "# 5) Run evaluation\n",
    "with open(\"data/dev-claims.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dev_claims = json.load(f)\n",
    "\n",
    "recalls, precisions, f1s = [], [], []\n",
    "for cid, obj in tqdm(dev_claims.items(), desc=\"Evaluating\"):\n",
    "    gold      = set(obj[\"evidences\"])\n",
    "    stems     = claim_proc_all[cid]\n",
    "    retrieved = rank_evidence(stems)\n",
    "\n",
    "    hits      = len(gold & set(retrieved))\n",
    "    recall    = hits / len(gold) if gold else 0.0\n",
    "    precision = hits / TOP_K       # always divide by K\n",
    "    f1        = (2 * recall * precision / (recall + precision)\n",
    "                 if (recall + precision) > 0 else 0.0)\n",
    "\n",
    "    recalls.append(recall)\n",
    "    precisions.append(precision)\n",
    "    f1s.append(f1)\n",
    "\n",
    "print(f\"\\nRecall@{TOP_K}:    {np.mean(recalls):.2%}\")\n",
    "print(f\"Precision@{TOP_K}: {np.mean(precisions):.2%}\")\n",
    "print(f\"F1@{TOP_K}:        {np.mean(f1s):.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
