{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36de6a14",
   "metadata": {},
   "source": [
    "# 2025 COMP90042 Project\n",
    "*Make sure you change the file name with your group id.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8057bc",
   "metadata": {},
   "source": [
    "Harmonic Mean of F and A: 0.25\n",
    "F1: 0.17\n",
    "Claim Acc:0.43"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f342758",
   "metadata": {},
   "source": [
    "# Readme\n",
    "*If there is something to be noted for the marker, please mention here.*\n",
    "\n",
    "*If you are planning to implement a program with Object Oriented Programming style, please put those the bottom of this ipynb file*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb6085a",
   "metadata": {},
   "source": [
    "# 1.DataSet Processing\n",
    "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa043103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Monte the files from MyDrive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19caca7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-4.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.11.0-cp312-cp312-win_amd64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: tqdm in e:\\anaconda\\lib\\site-packages (4.66.5)\n",
      "Requirement already satisfied: nltk in e:\\anaconda\\lib\\site-packages (3.9.1)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.5.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting lemminflect\n",
      "  Downloading lemminflect-0.2.3-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Downloading torch-2.7.0-cp312-cp312-win_amd64.whl.metadata (29 kB)\n",
      "Requirement already satisfied: scikit-learn in e:\\anaconda\\lib\\site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in e:\\anaconda\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: Pillow in e:\\anaconda\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in e:\\anaconda\\lib\\site-packages (from sentence-transformers) (4.11.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in e:\\anaconda\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in e:\\anaconda\\lib\\site-packages (from faiss-cpu) (24.1)\n",
      "Requirement already satisfied: colorama in e:\\anaconda\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: click in e:\\anaconda\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in e:\\anaconda\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in e:\\anaconda\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: filelock in e:\\anaconda\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in e:\\anaconda\\lib\\site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in e:\\anaconda\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in e:\\anaconda\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in e:\\anaconda\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in e:\\anaconda\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in e:\\anaconda\\lib\\site-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in e:\\anaconda\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in e:\\anaconda\\lib\\site-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in e:\\anaconda\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in e:\\anaconda\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in e:\\anaconda\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in e:\\anaconda\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in e:\\anaconda\\lib\\site-packages (from aiohttp->datasets) (1.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\anaconda\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\anaconda\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\anaconda\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\anaconda\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Collecting sympy>=1.13.3 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in e:\\anaconda\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in e:\\anaconda\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in e:\\anaconda\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (75.1.0)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in e:\\anaconda\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in e:\\anaconda\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in e:\\anaconda\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in e:\\anaconda\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in e:\\anaconda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in e:\\anaconda\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in e:\\anaconda\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Downloading sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\n",
      "Downloading faiss_cpu-1.11.0-cp312-cp312-win_amd64.whl (15.0 MB)\n",
      "   ---------------------------------------- 0.0/15.0 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.8/15.0 MB 9.2 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 5.0/15.0 MB 13.1 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 8.1/15.0 MB 14.0 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 10.5/15.0 MB 13.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 12.3/15.0 MB 12.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 14.2/15.0 MB 11.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.0/15.0 MB 11.5 MB/s eta 0:00:00\n",
      "Downloading datasets-3.5.1-py3-none-any.whl (491 kB)\n",
      "Downloading lemminflect-0.2.3-py3-none-any.whl (769 kB)\n",
      "   ---------------------------------------- 0.0/769.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 769.7/769.7 kB 5.4 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading torch-2.7.0-cp312-cp312-win_amd64.whl (212.5 MB)\n",
      "   ---------------------------------------- 0.0/212.5 MB ? eta -:--:--\n",
      "    --------------------------------------- 3.1/212.5 MB 16.8 MB/s eta 0:00:13\n",
      "   - -------------------------------------- 6.6/212.5 MB 16.8 MB/s eta 0:00:13\n",
      "   - -------------------------------------- 10.2/212.5 MB 16.8 MB/s eta 0:00:13\n",
      "   -- ------------------------------------- 14.7/212.5 MB 18.1 MB/s eta 0:00:11\n",
      "   --- ------------------------------------ 18.4/212.5 MB 18.1 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 22.0/212.5 MB 18.1 MB/s eta 0:00:11\n",
      "   ---- ----------------------------------- 25.7/212.5 MB 18.1 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 29.9/212.5 MB 18.4 MB/s eta 0:00:10\n",
      "   ------ --------------------------------- 33.3/212.5 MB 18.1 MB/s eta 0:00:10\n",
      "   ------ --------------------------------- 36.7/212.5 MB 17.8 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 39.8/212.5 MB 17.7 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 43.5/212.5 MB 17.6 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 47.4/212.5 MB 17.8 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 51.1/212.5 MB 17.8 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 55.1/212.5 MB 17.9 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 58.5/212.5 MB 17.8 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 62.4/212.5 MB 17.9 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 66.1/212.5 MB 17.8 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 70.0/212.5 MB 18.0 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 74.2/212.5 MB 18.1 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 77.9/212.5 MB 18.1 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 81.5/212.5 MB 18.1 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 84.9/212.5 MB 18.0 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 88.6/212.5 MB 18.0 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 92.0/212.5 MB 18.0 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 95.4/212.5 MB 17.9 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 99.1/212.5 MB 17.9 MB/s eta 0:00:07\n",
      "   ------------------ -------------------- 103.3/212.5 MB 18.0 MB/s eta 0:00:07\n",
      "   ------------------- ------------------- 107.7/212.5 MB 18.1 MB/s eta 0:00:06\n",
      "   -------------------- ------------------ 111.4/212.5 MB 18.1 MB/s eta 0:00:06\n",
      "   --------------------- ----------------- 115.1/212.5 MB 18.1 MB/s eta 0:00:06\n",
      "   --------------------- ----------------- 119.3/212.5 MB 18.1 MB/s eta 0:00:06\n",
      "   ---------------------- ---------------- 124.0/212.5 MB 18.3 MB/s eta 0:00:05\n",
      "   ----------------------- --------------- 128.5/212.5 MB 18.4 MB/s eta 0:00:05\n",
      "   ------------------------ -------------- 132.4/212.5 MB 18.4 MB/s eta 0:00:05\n",
      "   ------------------------- ------------- 136.6/212.5 MB 18.5 MB/s eta 0:00:05\n",
      "   ------------------------- ------------- 140.8/212.5 MB 18.5 MB/s eta 0:00:04\n",
      "   -------------------------- ------------ 144.7/212.5 MB 18.6 MB/s eta 0:00:04\n",
      "   --------------------------- ----------- 147.8/212.5 MB 18.5 MB/s eta 0:00:04\n",
      "   --------------------------- ----------- 151.3/212.5 MB 18.4 MB/s eta 0:00:04\n",
      "   ---------------------------- ---------- 154.9/212.5 MB 18.4 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 158.6/212.5 MB 18.4 MB/s eta 0:00:03\n",
      "   ----------------------------- --------- 162.5/212.5 MB 18.4 MB/s eta 0:00:03\n",
      "   ------------------------------ -------- 166.7/212.5 MB 18.4 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 170.1/212.5 MB 18.4 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 174.1/212.5 MB 18.4 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 178.3/212.5 MB 18.5 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 182.2/212.5 MB 18.5 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 186.1/212.5 MB 18.5 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 190.3/212.5 MB 18.5 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 194.2/212.5 MB 18.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 197.7/212.5 MB 18.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 201.3/212.5 MB 18.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 205.3/212.5 MB 18.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  208.7/212.5 MB 18.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  212.3/212.5 MB 18.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  212.3/212.5 MB 18.5 MB/s eta 0:00:01\n",
      "   --------------------------------------- 212.5/212.5 MB 18.0 MB/s eta 0:00:00\n",
      "Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "   ---------------------------------------- 0.0/10.4 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 2.6/10.4 MB 12.5 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 6.0/10.4 MB 14.7 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 6.0/10.4 MB 14.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.4/10.4 MB 15.1 MB/s eta 0:00:00\n",
      "Downloading xxhash-3.5.0-cp312-cp312-win_amd64.whl (30 kB)\n",
      "Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 3.1/6.3 MB 15.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 16.1 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   -------------------------------------- - 2.4/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.4/2.4 MB 144.9 MB/s eta 0:00:00\n",
      "Installing collected packages: xxhash, sympy, safetensors, multiprocess, lemminflect, faiss-cpu, torch, huggingface-hub, tokenizers, transformers, datasets, sentence-transformers\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.2\n",
      "    Uninstalling sympy-1.13.2:\n",
      "      Successfully uninstalled sympy-1.13.2\n",
      "Successfully installed datasets-3.5.1 faiss-cpu-1.11.0 huggingface-hub-0.30.2 lemminflect-0.2.3 multiprocess-0.70.16 safetensors-0.5.3 sentence-transformers-4.1.0 sympy-1.14.0 tokenizers-0.21.1 torch-2.7.0 transformers-4.51.3 xxhash-3.5.0\n"
     ]
    }
   ],
   "source": [
    "# install packages for hugging face\n",
    "!pip install sentence-transformers faiss-cpu tqdm nltk datasets lemminflect spacy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7c1dc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\Anaconda\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import all the packages\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import Counter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import re\n",
    "from datasets import Dataset\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43a3ad9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "label2id = {\n",
    "    \"SUPPORTS\": 0,\n",
    "    \"REFUTES\": 1,\n",
    "    \"NOT_ENOUGH_INFO\": 2,\n",
    "    \"DISPUTED\": 3\n",
    "}\n",
    "\n",
    "class ClaimEvidenceDataset(Dataset):\n",
    "    def __init__(self, claims, evidence_dict, tokenizer, max_length=512):\n",
    "        self.encodings = []\n",
    "        self.labels = []\n",
    "        for claim_data in claims.values():\n",
    "            claim_text = claim_data[\"claim_text\"]\n",
    "            label_str = claim_data[\"claim_label\"]\n",
    "            for eid in claim_data.get(\"evidences\", []):\n",
    "                if eid in evidence_dict:\n",
    "                    evidence_text = evidence_dict[eid]\n",
    "                    encoded = tokenizer(\n",
    "                        claim_text,\n",
    "                        evidence_text,\n",
    "                        padding=\"max_length\",\n",
    "                        truncation=True,\n",
    "                        max_length=max_length,\n",
    "                        return_tensors=\"pt\"\n",
    "                    )\n",
    "                    self.encodings.append({k: v.squeeze() for k, v in encoded.items()})\n",
    "                    self.labels.append(label2id[label_str])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.encodings[idx]\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51f39bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最终样本量: 7720\n",
      "类别计数: Counter({3: 1930, 1: 1930, 0: 1930, 2: 1930})\n"
     ]
    }
   ],
   "source": [
    "import json, random\n",
    "from collections import defaultdict, Counter\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "PAD, MASK = tokenizer.pad_token_id, tokenizer.mask_token_id\n",
    "\n",
    "label2id = {\"SUPPORTS\": 0, \"REFUTES\": 1, \"NOT_ENOUGH_INFO\": 2, \"DISPUTED\": 3}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "\n",
    "# =========  数据集 ========= #\n",
    "class ClaimEvidenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    - balance=True  : 对所有标签补到 max_count * target_ratio\n",
    "    - augmenters    : 可选 {'dropout','swap','pad','cutmix'}\n",
    "    - aug_params    : 各方法的细粒度超参\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 claims: dict,\n",
    "                 evidence_dict: dict,\n",
    "                 tokenizer,\n",
    "                 max_length: int = 512,\n",
    "                 balance: bool = True,\n",
    "                 target_ratio: float = 1.0,\n",
    "                 augmenters=None,\n",
    "                 aug_params=None,\n",
    "                 seed: int = 42):\n",
    "\n",
    "        random.seed(seed)\n",
    "        self.tokenizer, self.max_length = tokenizer, max_length\n",
    "        self.encodings, self.labels = [], []\n",
    "        self.augmenters = set(augmenters or ['dropout', 'swap', 'pad', 'cutmix'])\n",
    "\n",
    "        # 默认超参\n",
    "        _default = dict(dropout_prob=0.15,\n",
    "                        swap_prob=0.10,\n",
    "                        pad_prob=0.05,\n",
    "                        cutmix_min=0.3,\n",
    "                        cutmix_max=0.7)\n",
    "        self.aug_params = {**_default, **(aug_params or {})}\n",
    "\n",
    "        # --------- 原始样本编码 --------- #\n",
    "        for cdict in claims.values():\n",
    "            claim_text = cdict[\"claim_text\"]\n",
    "            lab = label2id[cdict[\"claim_label\"]]\n",
    "            for eid in cdict.get(\"evidences\", []):\n",
    "                if eid in evidence_dict:\n",
    "                    evi = evidence_dict[eid]\n",
    "                    toks = tokenizer(claim_text, evi,\n",
    "                                     truncation=True,\n",
    "                                     padding=\"max_length\",\n",
    "                                     max_length=max_length,\n",
    "                                     return_tensors=\"pt\")\n",
    "                    self.encodings.append({k: v.squeeze(0) for k, v in toks.items()})\n",
    "                    self.labels.append(lab)\n",
    "\n",
    "        # --------- Oversample with online augmentation --------- #\n",
    "        if balance:\n",
    "            self._balance_dataset(target_ratio)\n",
    "\n",
    "    # ======= Dataset API ======= #\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: v.clone() for k, v in self.encodings[idx].items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    # ======= 不同标签补样本 ======= #\n",
    "    def _balance_dataset(self, target_ratio: float):\n",
    "        by_label = defaultdict(list)\n",
    "        for i, y in enumerate(self.labels):\n",
    "            by_label[y].append(i)\n",
    "\n",
    "        max_count = int(max(len(v) for v in by_label.values()) * target_ratio)\n",
    "\n",
    "        for lab, idx_list in by_label.items():\n",
    "            need = max_count - len(idx_list)\n",
    "            for _ in range(max(0, need)):\n",
    "                base_idx = random.choice(idx_list)\n",
    "                base_enc = self.encodings[base_idx]\n",
    "                aug_enc = self._augment_encoding(base_enc, lab)\n",
    "                self.encodings.append(aug_enc)\n",
    "                self.labels.append(lab)\n",
    "\n",
    "    # ======= 单条样本增广 ======= #\n",
    "    def _augment_encoding(self, enc, lab):\n",
    "        enc = {k: v.clone() for k, v in enc.items()}  # 不要改原 tensor\n",
    "        choice = random.choice(list(self.augmenters))\n",
    "        if choice == 'dropout':\n",
    "            self._token_dropout(enc)\n",
    "        elif choice == 'swap':\n",
    "            self._swap_neighbor(enc)\n",
    "        elif choice == 'pad':\n",
    "            self._random_pad(enc)\n",
    "        elif choice == 'cutmix':\n",
    "            self._cutmix(enc, lab)\n",
    "        return enc\n",
    "\n",
    "    # ----- 1. Random token dropout -----\n",
    "    def _token_dropout(self, enc):\n",
    "        ids = enc['input_ids']\n",
    "        mask = torch.rand_like(ids.float()) < self.aug_params['dropout_prob']\n",
    "        ids[mask & (ids != PAD)] = MASK\n",
    "        enc['input_ids'] = ids\n",
    "\n",
    "    # ----- 2. Swap neighbouring tokens -----\n",
    "    def _swap_neighbor(self, enc):\n",
    "        ids = enc['input_ids']\n",
    "        for i in range(1, len(ids) - 1):\n",
    "            if random.random() < self.aug_params['swap_prob'] and ids[i] not in (PAD, MASK):\n",
    "                ids[i], ids[i + 1] = ids[i + 1].clone(), ids[i].clone()\n",
    "        enc['input_ids'] = ids\n",
    "\n",
    "    # ----- 3. Random inner padding -----\n",
    "    def _random_pad(self, enc):\n",
    "        ids, mask = enc['input_ids'], enc['attention_mask']\n",
    "        pad_prob = self.aug_params['pad_prob']\n",
    "        new_ids, new_mask = [], []\n",
    "        for tok, m in zip(ids, mask):\n",
    "            if m.item() == 0:  # 已到 PAD 区\n",
    "                break\n",
    "            new_ids.append(tok.item())\n",
    "            new_mask.append(1)\n",
    "            if random.random() < pad_prob and len(new_ids) < self.max_length - 1:\n",
    "                new_ids.append(PAD)\n",
    "                new_mask.append(0)\n",
    "        # 截断 / 末尾补 PAD\n",
    "        new_ids = (new_ids + [PAD] * self.max_length)[:self.max_length]\n",
    "        new_mask = (new_mask + [0] * self.max_length)[:self.max_length]\n",
    "        enc['input_ids'] = torch.tensor(new_ids, dtype=torch.long)\n",
    "        enc['attention_mask'] = torch.tensor(new_mask, dtype=torch.long)\n",
    "\n",
    "    # ----- 4. CutMix (同标签) -----\n",
    "    def _cutmix(self, enc, lab):\n",
    "        # 随机再挑一条同标签样本\n",
    "        same_idxs = [i for i, y in enumerate(self.labels) if y == lab]\n",
    "        other = {k: v.clone() for k, v in self.encodings[random.choice(same_idxs)].items()}\n",
    "        lam = random.uniform(self.aug_params['cutmix_min'],\n",
    "                             self.aug_params['cutmix_max'])\n",
    "        cut_point = int(lam * self.max_length)\n",
    "        # 前半取当前，后半取 other\n",
    "        enc['input_ids'][cut_point:] = other['input_ids'][cut_point:]\n",
    "        enc['attention_mask'][cut_point:] = other['attention_mask'][cut_point:]\n",
    "\n",
    "\n",
    "# ========== 用 法 ========== #\n",
    "with open('./data/train-claims.json') as f:\n",
    "    train_claims = json.load(f)\n",
    "with open('./data/evidence.json') as f:\n",
    "    evidence_dict = json.load(f)\n",
    "\n",
    "train_dataset = ClaimEvidenceDataset(\n",
    "    train_claims,\n",
    "    evidence_dict,\n",
    "    tokenizer,\n",
    "    max_length=512,\n",
    "    balance=True,                # ✨ 打开类别均衡\n",
    "    target_ratio=1.0,            # 把所有类补到与最大类一样多\n",
    "    augmenters=['dropout', 'swap', 'pad', 'cutmix'],\n",
    "    aug_params=dict(dropout_prob=0.15, swap_prob=0.1,\n",
    "                    pad_prob=0.05, cutmix_min=0.4, cutmix_max=0.6)\n",
    ")\n",
    "\n",
    "print(\"最终样本量:\", len(train_dataset))\n",
    "print(\"类别计数:\", Counter(train_dataset.labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4110ee",
   "metadata": {},
   "source": [
    "# 2. Model Implementation\n",
    "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6d21e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_14776\\4187663085.py:26: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='774' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  3/774 00:27 < 5:59:18, 0.04 it/s, Epoch 0.01/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 34\u001b[0m\n\u001b[0;32m     26\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     27\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     28\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m     29\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[0;32m     30\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer\n\u001b[0;32m     31\u001b[0m )\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# 开始训练\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# 保存模型\u001b[39;00m\n\u001b[0;32m     37\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./model/my_bert_classifier\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32me:\\Anaconda\\Lib\\site-packages\\transformers\\trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   2246\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   2247\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[0;32m   2248\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[0;32m   2249\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[0;32m   2250\u001b[0m     )\n",
      "File \u001b[1;32me:\\Anaconda\\Lib\\site-packages\\transformers\\trainer.py:2565\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2559\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m   2560\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[0;32m   2562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2563\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2564\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m-> 2565\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2566\u001b[0m ):\n\u001b[0;32m   2567\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2568\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[0;32m   2569\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "train = True\n",
    "dev_train_included = True\n",
    "if (dev_train_included):\n",
    "    model_path = \"./model/final_bert_classifier\"\n",
    "else:\n",
    "    model_path = \"./model/my_bert_classifier\"\n",
    "if (train):\n",
    "    # 加载数据\n",
    "    with open('./data/train-claims.json', 'r') as f:\n",
    "        train_claims = json.load(f)\n",
    "    with open('./data/evidence.json', 'r') as f:\n",
    "        evidence_dict = json.load(f)\n",
    "    if (dev_train_included):\n",
    "        with open('./data/dev-claims.json','r') as f:\n",
    "            dev_claims = json.load(f)\n",
    "\n",
    "        all_claims = {**train_claims, **dev_claims}   # dev 覆盖 train 的同名 key；反之亦可\n",
    "\n",
    "        # ---------- 3. 构造数据集 ----------\n",
    "        train_dataset = ClaimEvidenceDataset(\n",
    "            claims=all_claims,\n",
    "            evidence_dict=evidence_dict,\n",
    "            tokenizer=tokenizer,\n",
    "            max_length=512,\n",
    "            balance=True,             # 若要做类别均衡 / 增广就保持 True\n",
    "            target_ratio=1.0,\n",
    "        )\n",
    "    else:\n",
    "        train_dataset = train_dataset = ClaimEvidenceDataset(\n",
    "            claims=train_claims,\n",
    "            evidence_dict=evidence_dict,\n",
    "            tokenizer=tokenizer,\n",
    "            max_length=512,\n",
    "            balance=True,             # 若要做类别均衡 / 增广就保持 True\n",
    "            target_ratio=1.0,\n",
    "        )\n",
    "    # 加载 BERT 模型\n",
    "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=4)\n",
    "\n",
    "    # 设置训练参数\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        per_device_train_batch_size=16,\n",
    "        num_train_epochs=3,\n",
    "        eval_strategy=\"no\",\n",
    "        save_strategy=\"no\",\n",
    "        logging_strategy=\"no\",   \n",
    "        logging_steps=50,\n",
    "    )\n",
    "\n",
    "    # 初始化 Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    # 开始训练\n",
    "    trainer.train()\n",
    "\n",
    "    # 保存模型\n",
    "\n",
    "    model.save_pretrained(model_path)\n",
    "    tokenizer.save_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13391336",
   "metadata": {},
   "source": [
    "# 3.Testing and Evaluation\n",
    "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c7270b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying: 100%|██████████| 153/153 [00:15<00:00, 10.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done: saved to test-claims-final-predictions.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 1. 加载模型\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BertForSequenceClassification.from_pretrained(model_path).to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "model.eval()\n",
    "\n",
    "# 2. 加载你的 retrieval 文件（包含 claim_text）\n",
    "with open(\"test-claims-predictions.json\", \"r\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "# 3. 加载 evidence 内容\n",
    "with open(\"./data/evidence.json\", \"r\") as f:\n",
    "    evidence_dict = json.load(f)\n",
    "\n",
    "id2label = {\n",
    "    0: \"SUPPORTS\",\n",
    "    1: \"REFUTES\",\n",
    "    2: \"NOT_ENOUGH_INFO\",\n",
    "    3: \"DISPUTED\"\n",
    "}\n",
    "\n",
    "# 4. 定义平均概率融合预测函数\n",
    "def predict_label_average(claim_text, evidence_ids):\n",
    "    probs_list = []\n",
    "\n",
    "    for eid in evidence_ids:\n",
    "        if eid not in evidence_dict:\n",
    "            continue\n",
    "        ev_text = evidence_dict[eid]\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            claim_text,\n",
    "            ev_text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=512\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            probs = F.softmax(outputs.logits, dim=-1)\n",
    "            probs_list.append(probs.cpu().numpy()[0])\n",
    "\n",
    "    if not probs_list:\n",
    "        return \"NOT_ENOUGH_INFO\"  # fallback\n",
    "\n",
    "    avg_probs = np.mean(probs_list, axis=0)\n",
    "    pred_idx = int(np.argmax(avg_probs))\n",
    "    return id2label[pred_idx]\n",
    "\n",
    "# 5. 预测所有 test claim\n",
    "final_predictions = {}\n",
    "\n",
    "for cid, entry in tqdm(test_data.items(), desc=\"Classifying\"):\n",
    "    claim_text = entry[\"claim_text\"]\n",
    "    evidence_ids = entry[\"evidence\"]\n",
    "\n",
    "    label = predict_label_average(claim_text, evidence_ids)\n",
    "\n",
    "    final_predictions[cid] = {\n",
    "        \"evidences\": evidence_ids,\n",
    "        \"claim_label\": label,\n",
    "        \"claim_text\": claim_text\n",
    "    }\n",
    "\n",
    "# 6. 保存\n",
    "with open(\"test-claims-final-predictions.json\", \"w\") as f:\n",
    "    json.dump(final_predictions, f, indent=2)\n",
    "\n",
    "print(\"✅ Done: saved to test-claims-final-predictions.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b208a663",
   "metadata": {},
   "source": [
    "## Object Oriented Programming codes here\n",
    "\n",
    "*You can use multiple code snippets. Just add more if needed*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
